# Folder Structure

[What to keep in mind](https://www.notion.so/What-to-keep-in-mind-2d01bb84f4a480398c47c90d95063012?pvs=21)

- [ ] Published docs to github pages mkdocs and also dbt docs

## Name: Olist â€“ Modern E-commerce Analytics Platform

**Subtitle (Highly Recommended)**

End-to-End Analytics Platform using Azure Blob, Snowflake, dbt, Power BI, Git & CI

**Folder/Repo: olist-modern-analytics-platform**

| Power BI workspace | Olist Analytics - PROD |
| ------------------ | ---------------------- |

# **ğŸ¯Â Phase 1: Business Understanding & Requirements**

### 1) Business Problem

| **Field**                | **Details**                                                                                                                                                                                                                                                                                                                                             |
| ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Business Domain          | E-Commerce Marketplace & Logistics                                                                                                                                                                                                                                                                                                                      |
| Business Problem         | Olist Lack a centralized, trusted analytics layer to track sales performance, customer behavior, seller contribution, regional demand, and delivery efficiency                                                                                                                                                                                          |
| Desired Business Outcome | Establish aÂ **single source of truth analytics platform**Â that enables stakeholders to reliably track revenue and growth trends, identify top-performing categories, sellers, and regions, monitor delivery SLAs across states, analyze repeat customer behavior, and support fast, self-service, data-driven decisions through interactive dashboards. |

Business Context:

**Olist operates a multi-seller e-commerce marketplace in Brazil.**

**As the platform scaled, analytics evolved in a fragmented mannerâ€”data was accessed directly from raw transactional tables, metrics were redefined across teams, and reporting relied heavily on manual SQL queries and spreadsheets.**

This legacy analytics approach createdÂ **inconsistent KPIs, low data trust, and slow insight delivery**, limiting leadershipâ€™s ability to understand sales trends, customer retention, seller performance, and logistics efficiency.

To support growth and move toward a data-driven operating model, Olist requires aÂ **modern analytics platform**Â that centralizes data, enforces trusted metrics, and enables fast, self-service analytics for business and operations teams.

| **Architecture Domain** | **Current State (Legacy Pain)**                                                      | **Target State (Your Solution)**                                                                |
| ----------------------- | ------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------- |
| **Data Storage**        | SiloedÂ **OLTP Transactional Tables**Â (Postgres); heavy read-load slows down the app. | CentralizedÂ **OLAP Data Warehouse**Â (Snowflake) optimized for analytical queries.               |
| **Ingestion Pipeline**  | Manual CSV extracts and ad-hoc scripts; frequent failures & stale data.              | AutomatedÂ **ELT Pipeline**Â via Azure Blob Storage & Python; distinct "Raw" vs. "Curated" zones. |
| **Data Modeling**       | No defined schema; massiveÂ **"Spaghetti SQL"**queries joined at runtime.             | **Kimball Star Schema**Â modeled inÂ **dbt**Â with version control, testing, and documentation.    |
| **Governance & Trust**  | **"Metric Drift"**: Every department calculates "Revenue" differently.               | **Single Source of Truth**: Metrics defined once in dbt/Power BI and reused everywhere.         |
| **Consumption Layer**   | Static, emailed Excel spreadsheets; 100% dependency on IT for changes.               | **Self-Service Power BI Dashboard**; Interactive, drill-down capable, and user-friendly.        |
| **Scalability**         | **Low**: System crashes with high data volume; manual fixes required.                | **High**: Cloud-native architecture scales compute instantly to handle millions of rows.        |

### 2) Key Business Questions

| **ID** | **Business Question**                                                                            | **Analytics Type** |
| ------ | ------------------------------------------------------------------------------------------------ | ------------------ |
| Q1     | How are total revenue and order volume trending over time?                                       | Descriptive        |
| Q2     | Which product categories generate the most revenue and orders?                                   | Descriptive        |
| Q3     | Which regions and states contribute most to revenue and orders?                                  | Descriptive        |
| Q4     | How efficient is order delivery performance across regions in terms of delivery time and delays? | Diagnostic         |
| Q5     | Which sellers contribute the most to revenue and order volume?                                   | Descriptive        |
| Q6     | What payment methods are most commonly used, and how do they impact order value?                 | Diagnostic         |
| Q7     | How many customers are repeat buyers versus new customers?                                       | Descriptive        |

### 3) KPI/Metrics

| ID     | Business Question                                                      | KPI Type   | KPI Name                        | KPI Definition                                    | Fact / Dim Tables Used    |
| ------ | ---------------------------------------------------------------------- | ---------- | ------------------------------- | ------------------------------------------------- | ------------------------- |
| **Q1** | How are total revenue and order volume trending over time?             | Primary    | Total Revenue                   | Sum of order item prices for delivered orders     | fct_orders, dim_date      |
| **Q1** |                                                                        | Primary    | Total Orders                    | Count of distinct orders delivered                | fct_orders                |
| **Q1** |                                                                        | Supporting | Avg Order Value (AOV)           | Total Revenue Ã· Total Orders                      | fct_orders                |
| **Q1** |                                                                        | Supporting | MoM Revenue Growth %            | (Current Month âˆ’ Previous Month) Ã· Previous Month | fct_orders, dim_date      |
| **Q2** | Which product categories generate the most revenue and orders?         | Primary    | Revenue by Category             | Total Revenue grouped by product category         | fct_orders, dim_products  |
| **Q2** |                                                                        | Primary    | Orders by Category              | Count of orders per product category              | fct_orders, dim_products  |
| **Q2** |                                                                        | Supporting | Avg Price per Category          | Avg item price per category                       | fct_orders, dim_products  |
| **Q3** | Which regions and states contribute most to revenue and orders?        | Primary    | Revenue by State                | Total Revenue grouped by customer state           | fct_orders, dim_customers |
| **Q3** |                                                                        | Primary    | Orders by State                 | Count of orders per state                         | fct_orders, dim_customers |
| **Q3** |                                                                        | Supporting | Revenue % Contribution          | % share of total revenue by state                 | fct_orders                |
| **Q4** | How efficient is order delivery performance across regions?            | Primary    | Avg Delivery Time (Days)        | Avg days between order purchase and delivery      | fct_orders                |
| **Q4** |                                                                        | Primary    | Delivery Delay Rate %           | % of orders delivered after estimated date        | fct_orders                |
| **Q4** |                                                                        | Supporting | On-Time Delivery %              | % of orders delivered on or before estimate       | fct_orders                |
| **Q5** | Which sellers contribute the most to revenue and orders?               | Primary    | Revenue by Seller               | Total Revenue per seller                          | fct_orders, dim_sellers   |
| **Q5** |                                                                        | Primary    | Orders by Seller                | Count of orders per seller                        | fct_orders                |
| **Q5** |                                                                        | Supporting | Avg Revenue per Seller          | Revenue Ã· Orders per seller                       | fct_orders                |
| **Q6** | What payment methods are most used and how do they impact order value? | Primary    | Orders by Payment Type          | Count of orders per payment method                | fct_payments              |
| **Q6** |                                                                        | Primary    | Avg Order Value by Payment Type | Avg payment value per payment method              | fct_payments              |
| **Q6** |                                                                        | Supporting | Payment Share %                 | % distribution of payment methods                 | fct_payments              |
| **Q7** | How many customers are repeat buyers vs new customers?                 | Primary    | Repeat Customers                | Customers with â‰¥ 2 orders                         | fct_orders, dim_customers |
| **Q7** |                                                                        | Primary    | New Customers                   | Customers with only 1 order                       | fct_orders, dim_customers |
| **Q7** |                                                                        | Supporting | Repeat Purchase Rate            | Repeat Customers Ã· Total Customers                | fct_orders                |

### 4) Data Scope and Sources

## **1. Project Scope**

| Data Element          | Details                                     |
| --------------------- | ------------------------------------------- |
| Date Range            | 2016â€“2018 (full historical dataset)         |
| Geographic Scope      | All Brazilian states                        |
| Data Refresh Schedule | Daily at 06:00 UTC (simulated batch design) |
| Data Latency Target   | â‰¤ 24 hours                                  |
| Historical Retention  | Full history retained                       |
| Source Type           | Batch files (CSV â†’ Snowflake)               |

### **âœ… In-Scope**

- **Sales & Order Performance:**Â Tracking revenue, order volume, and growth trends.
- **Customer Behavior:**Â Analysis of New vs. Repeat purchase patterns.
- **Seller Contribution:**Â Ranking and performance analysis of sellers.
- **Regional Demand:**Â Geospatial analysis at the State and City levels.
- **Delivery Logistics:**Â Measuring efficiency, shipping times, and delay rates.
- **Payment Analysis:**Â Usage breakdown and impact on Average Order Value (AOV).

### **âŒ Out-of-Scope**

- **Customer Reviews & Sentiment:**Â Text mining of review comments.
- **Fraud Detection:**Â Anomaly detection for credit card transactions.
- **Real-Time Streaming:**Â Sub-second latency data ingestion.
- **Marketing Attribution:**Â Tracking ad spend or campaign ROI.

---

## **2. Source Data Inventory (Raw Layer)**

| **Source File / Table**                     | **Description**                               | **Purpose in Analysis**                                         |
| ------------------------------------------- | --------------------------------------------- | --------------------------------------------------------------- |
| **`olist_orders_dataset.csv`**              | Order lifecycle details (status, timestamps). | Core fact table for sales and delivery timelines.               |
| **`olist_order_items_dataset.csv`**         | Products within each order (price, freight).  | Calculation of Revenue, Quantity, and Freight costs.            |
| **`olist_customers_dataset.csv`**           | Customer identifiers and location keys.       | Customer demographics and repeat buyer analysis.                |
| **`olist_sellers_dataset.csv`**             | Seller details and locations.                 | Seller performance and geographic contribution.                 |
| **`olist_products_dataset.csv`**            | Product master data (dimensions, weight).     | Product category and shipping cost analysis.                    |
| **`olist_order_payments_dataset.csv`**      | Payment methods, installments, and value.     | Analysis of payment preferences and AOV.                        |
| **`olist_geolocation_dataset.csv`**         | Zip Code prefix to City/State mapping.        | Enabling geographic visualization and Row Level Security (RLS). |
| **`product_category_name_translation.csv`** | Portuguese to English category translations.  | Ensuring reporting is business-friendly and readable.           |

---

## **3. Dashboard Focus Areas**

- **ğŸ’° Revenue & Orders:**Â Track overall sales performance, Month-over-Month (MoM) growth, and total volume.
- **ğŸ‘¥ Customers:**Â Distinguish between one-time buyers and loyal (repeat) customers to calculate retention rates.
- **ğŸ“¦ Sellers:**Â Identify top-performing sellers and their contribution to total platform revenue.
- **ğŸŒ Geography:**Â Analyze high-demand regions using State and City level heatmaps.
- **ğŸšš Delivery:**Â Monitor logistics KPIs, including Average Delivery Days and "% of Orders Delayed".
- **ğŸ’³ Payments:**Â Understand consumer financing behavior (Installments) and preferred payment types (Credit Card vs. Boleto).

---

## **4. Data Strategy & Architecture**

| **Aspect**              | **Decision / Strategy**                                                                             |
| ----------------------- | --------------------------------------------------------------------------------------------------- |
| **Ingestion Method**    | Python Script â†’Â **Azure Blob Storage**Â â†’ Snowflake External Stage                                   |
| **Load Frequency**      | **Full Load**Â (Initial History) + Incremental Simulation capability                                 |
| **Transformation Tool** | **dbt (Data Build Tool)**Â coveringÂ `Staging`Â â†’Â `Intermediate`Â â†’Â `Marts`Â layers                      |
| **Schema Evolution**    | Controlled viaÂ **dbt models**; changes are version-controlled in Git                                |
| **Data Quality**        | Automated testing forÂ **Row Counts**,Â **Null Values**, andÂ **Referential Integrity**Â (Foreign Keys) |

### 5) Business Rules

### Purpose

Business rules define how Olistâ€™s data should be interpreted, filtered, and calculated so that all dashboards, KPIs, and analyses remain consistent, trusted, and business-aligned.

These rules act as a contract between business stakeholders and the data team.

---

## A) Core Business Rules (Notion Table)

| Rule Name                  | Description                                             | Example (Olist Context)                           |
| -------------------------- | ------------------------------------------------------- | ------------------------------------------------- |
| Delivered Orders Only      | Only completed deliveries count toward revenue and KPIs | Include orders where order_status = 'delivered'   |
| Revenue Recognition        | Revenue is recognized only after delivery               | Order placed â‰  revenue; delivered order = revenue |
| Repeat Customer Definition | Defines what qualifies as a repeat buyer                | Customer with â‰¥ 2 delivered orders                |
| Delivery Delay Logic       | Defines what counts as a late delivery                  | actual_delivery_date > estimated_delivery_date    |
| Geographic Mapping         | How customers are mapped to regions/states              | Use customer_zip_code_prefix â†’ state mapping      |
| Payment Attribution        | How payments are linked to orders                       | Multiple payments summed per order                |
| Time Zone Standard         | Consistent date interpretation                          | All dates interpreted in Brazil local time        |
| Outlier Handling           | How extreme values are treated                          | Orders with very high value flagged, not removed  |

---

## B) KPI-Focused Business Rules

| Rule Type            | Rule                                           | Purpose                                   |
| -------------------- | ---------------------------------------------- | ----------------------------------------- |
| Sales Calculation    | Include only delivered orders in Total Revenue | Avoid inflated sales from canceled orders |
| Order Count          | Count distinct delivered orders                | Standardize order metrics                 |
| Average Order Value  | AOV = Revenue Ã· Delivered Orders               | Prevent skew from canceled orders         |
| Repeat Customer      | â‰¥ 2 delivered orders per customer              | Consistent customer segmentation          |
| Delivery Performance | Delay = Actual âˆ’ Estimated delivery date       | Measure logistics efficiency              |
| Payment Analysis     | Sum all payment values per order               | Accurate revenue attribution              |

---

## C) Implementation Mapping (Very Important for ADLC)

| #   | Rule Name             | Description                           | Implemented In              |
| --- | --------------------- | ------------------------------------- | --------------------------- |
| 1   | Delivered Orders Only | Filter orders with status = delivered | dbt: stg_orders.sql         |
| 2   | Revenue Recognition   | Revenue counted post-delivery         | dbt: fct_orders.sql         |
| 3   | Repeat Customer Logic | â‰¥ 2 delivered orders                  | dbt: dim_customers.sql      |
| 4   | Delivery Delay Flag   | Late vs on-time delivery              | dbt: fct_orders.sql         |
| 5   | Payment Aggregation   | Sum multiple payments per order       | dbt: int_order_payments.sql |
| 6   | Profit Calculation    | Profit = Revenue âˆ’ Cost               | Power BI Measure            |
| 7   | Outlier Flagging      | Flag extreme order values             | dbt: intermediate model     |

### 6) Time Windows & Reporting Frequency

## 6.1 Defined Time Windows (Olist Project)

| **Type**                 | **Definition**                           | **Purpose**                             | **Implementation**            |
| ------------------------ | ---------------------------------------- | --------------------------------------- | ----------------------------- |
| Static Historical Window | Sep 2016 â€“ Oct 2018 (full Olist dataset) | Analyze complete historical performance | dbt model filtering           |
| Rolling Window           | Last 12 Months (dynamic)                 | Monitor recent business trends          | Power BI relative date filter |
| Reporting Frequency      | Monthly                                  | Standard business reporting cadence     | Power BI Service refresh      |

---

## 6.2 Analysis Usage by Time Window

| **Analysis Type**       | **Time Window**        | **Purpose**                           | **Notes**               |
| ----------------------- | ---------------------- | ------------------------------------- | ----------------------- |
| Revenue & Orders Trend  | Monthly (Full History) | Identify growth and decline patterns  | Primary executive view  |
| Delivery Performance    | Monthly (Full History) | Compare delivery efficiency over time | Diagnostic analysis     |
| Customer Retention      | Rolling 12 Months      | Identify repeat vs new customers      | Dynamic business metric |
| Payment Method Analysis | Monthly                | Compare AOV by payment type           | Stable reporting        |

### 5) Expected Deliverables

## Data Platform Deliverables (Snowflake + dbt)

| Deliverable                   | Description                                                | Why It Matters                               |
| ----------------------------- | ---------------------------------------------------------- | -------------------------------------------- |
| RAW Tables                    | Original Olist CSV/Parquet/JSON data loaded into Snowflake | Preserves source-of-truth and auditability   |
| Staging Models (stg\_\*)      | Cleaned, standardized, type-casted tables                  | Ensures consistent column names & data types |
| Intermediate Models (int\_\*) | Business logic joins and transformations                   | Centralizes reusable logic                   |
| Marts Layer (fct*\*, dim*\*)  | Star schema analytics tables                               | Enables fast BI and analytics                |
| Surrogate Keys                | Generated using dbt for dimensions                         | Improves joins and performance               |
| Data Quality Tests            | dbt tests for freshness, uniqueness, relationships         | Prevents silent data issues                  |
| Data Contracts                | Enforced schema expectations                               | Prevents breaking downstream reports         |

---

## Analytics & BI Deliverables (Power BI)

| Deliverable              | Description                                    | Why It Matters                 |
| ------------------------ | ---------------------------------------------- | ------------------------------ |
| Power BI Semantic Model  | Star schema model with relationships           | Enables self-service analytics |
| Measures Table           | Centralized DAX measures (\_Measures)          | Consistent KPI logic           |
| Power BI Dashboards      | 2â€“3 report pages aligned to business questions | Communicates insights visually |
| KPI Cards                | Primary KPIs (Revenue, Orders, Customers)      | Executive summary view         |
| Diagnostic Visuals       | Delivery delay, payment analysis, churn views  | Supports â€œwhyâ€ questions       |
| Time Intelligence        | YoY, MoM, YTD calculations                     | Trend analysis                 |
| RLS (Row Level Security) | Region/Seller based access control             | Enterprise-ready security      |
| Data Dictionary Page     | Auto-generated metadata (INFO.VIEW)            | Improves model understanding   |

## Data Quality & Governance Deliverables

| Deliverable             | Description                                      |
| ----------------------- | ------------------------------------------------ |
| Data Quality Checklist  | Validation rules for raw, staging, marts         |
| Outlier Strategy        | Flag-based outlier handling (not blind deletion) |
| Business Rules Document | Revenue logic, customer definitions, filters     |
| Schema Drift Protection | Contracts + Power Query column selection         |

---

## Automation, CI & DevOps Deliverables

| Deliverable       | Description                        |
| ----------------- | ---------------------------------- |
| Git Repository    | Full project under version control |
| GitHub Actions CI | Automated dbt tests on PR          |
| Branch Strategy   | Feature branches + main branch     |
| SQLFluff Linting  | SQL quality enforcement            |
| Deployment Docs   | Manual promotion steps documented  |

---

## Documentation & Framework Deliverables

| Deliverable               | Description                             |
| ------------------------- | --------------------------------------- |
| ADLC Framework (Notion)   | End-to-end documented lifecycle         |
| Architecture Diagram      | Azure Blob â†’ Snowflake â†’ dbt â†’ Power BI |
| Business Requirement Docs | Problems, questions, KPIs               |
| KPI Definitions           | SQL + DAX formulas                      |
| Reusability Notes         | What can be reused in future projects   |

---

## Business Impact Deliverables

| Deliverable           | Description                            |
| --------------------- | -------------------------------------- |
| Before vs After Table | Manual reporting â†’ automated analytics |
| Time Saved Metrics    | Faster reporting & ad-hoc analysis     |
| Trust Improvement     | Single Source of Truth                 |
| Decision Enablement   | Faster insights for stakeholders       |

---

## Final Project Output Summary (One-Line)

> â€œBuilt an end-to-end modern analytics platform using Azure Blob, Snowflake, dbt, and Power BI that delivers trusted KPIs, diagnostic insights, and reusable data models with enterprise-grade documentation and CI.â€

---

##

### 6) Stakeholders and Roles

| **Stakeholder Role**                      | **Responsibilities**                        | **Data Needs**                                | **Primary Outputs** |
| ----------------------------------------- | ------------------------------------------- | --------------------------------------------- | ------------------- |
| Business Stakeholder (E-commerce Manager) | Owns revenue, orders, and growth            | Revenue trends, category & region performance | Power BI Dashboard  |
| Operations Manager                        | Oversees delivery and logistics performance | Delivery time, delays, regional efficiency    | Power BI Dashboard  |
| Sales / Seller Manager                    | Manages seller performance                  | Seller contribution, order volume             | Power BI Dashboard  |
| Finance / Payments Analyst                | Tracks revenue and payment behavior         | AOV, payment methods, order value             | Power BI Dashboard  |
| Data Analyst (You)                        | Builds insights and dashboards              | Clean data, business rules                    | All deliverables    |
| Analytics Engineer (You â€“ Project Scope)  | Models data and enforces logic              | dbt models, tests                             | Snowflake tables    |

### 7) Success Criteria

### Business & Analytics Success

| Area                   | Success Criteria                                                                       |
| ---------------------- | -------------------------------------------------------------------------------------- |
| Business Adoption      | Dashboard answers all defined business questions                                       |
| Decision Making        | Stakeholders can identify trends, top contributors, and issues without analyst support |
| KPI Accuracy           | All KPIs match documented business definitions                                         |
| Single Source of Truth | Metrics are consistent across Power BI, SQL, and documentation                         |

---

### Data Engineering & Modeling Success

![ğŸ§±](https://fonts.gstatic.com/s/e/notoemoji/16.0/1f9f1/72.png)

| Area             | Success Criteria                                        |
| ---------------- | ------------------------------------------------------- |
| Data Pipeline    | Data flows from Azure Blob â†’ Snowflake without failures |
| Transformations  | Raw â†’ Staging â†’ Marts models built using dbt            |
| Data Quality     | dbt tests pass (uniqueness, not null, relationships)    |
| Schema Stability | Schema changes do not break downstream reports          |
| Freshness        | Data is updated within defined latency SLA              |

---

### Automation & CI Success

![âš™ï¸](https://fonts.gstatic.com/s/e/notoemoji/16.0/2699_fe0f/72.png)

| Area             | Success Criteria                                  |
| ---------------- | ------------------------------------------------- |
| CI Pipeline      | dbt tests run automatically via GitHub Actions    |
| Failure Handling | Pipeline fails clearly on test or schema errors   |
| Version Control  | All SQL, dbt, and Power BI (.pbip) tracked in Git |
| Reproducibility  | Project can be rebuilt end-to-end from repository |

---

### Power BI & Performance Success

![ğŸ¨](https://fonts.gstatic.com/s/e/notoemoji/16.0/1f3a8/72.png)

| Area             | Success Criteria                                       |
| ---------------- | ------------------------------------------------------ |
| Load Performance | Report pages load in under 5 seconds                   |
| Model Design     | Star Schema enforced (Facts central, Dims surrounding) |
| Semantic Layer   | Measures centralized and reusable                      |
| Security         | RLS works correctly for different users                |
| UX               | Users can self-serve without confusion                 |

---

### Documentation & Maintainability

![ğŸ§ ](https://fonts.gstatic.com/s/e/notoemoji/16.0/1f9e0/72.png)

| Area              | Success Criteria                                         |
| ----------------- | -------------------------------------------------------- |
| Documentation     | ADLC, KPIs, data models, and rules fully documented      |
| Onboarding        | New analyst can understand pipeline within 1 day         |
| Change Tracking   | All changes traceable via Git commits                    |
| Knowledge Sharing | Business logic is clearly explained (not hidden in code) |

---

### Business Impact Metrics (Portfolio-Friendly)

![ğŸ“ˆ](https://fonts.gstatic.com/s/e/notoemoji/16.0/1f4c8/72.png)

| Before                 | After                  | Impact                            |
| ---------------------- | ---------------------- | --------------------------------- |
| Manual Excel reporting | Automated ELT pipeline | 90% reduction in reporting effort |
| Metric inconsistencies | Single Source of Truth | 100% KPI alignment                |
| Slow ad-hoc analysis   | Star Schema + Power BI | Questions answered in minutes     |
| No validation          | dbt tests + CI         | Trustworthy analytics             |

---

### Final Project Sign-Off Criteria

![âœ…](https://fonts.gstatic.com/s/e/notoemoji/16.0/2705/72.png)

The project is considered successful when:

- All business questions are answered via dashboards
- All dbt models and tests pass in CI
- Power BI reports are performant and secure
- Documentation is complete and reusable
- Business value is clearly demonstrated

---

### Interview-Ready Line

![ğŸ’¬](https://fonts.gstatic.com/s/e/notoemoji/16.0/1f4ac/72.png)

> â€œI defined clear success criteria across business impact, data quality, automation, and performance to ensure the project was production-ready, not just a demo.â€

---

### **Category A: Business Impact (The "Value" Metrics)**

_Did we solve the business problem?_

| Metric               | Target                                 | Why it matters                                                                                                 | Validation Method                                     |
| -------------------- | -------------------------------------- | -------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- |
| **User Adoption**    | **Daily Active Use**Â by Logistics Team | The dashboard isn't just "pretty"; it is part of their morning routine.                                        | Power BI Usage Metrics Report.                        |
| **Trust / Accuracy** | **100% Match**Â with Bank Data          | If the "Revenue" number in Power BI differs from the Bank Statement by even $1, the CFO will stop using it.    | Reconciliation test:Â `SUM(Sales)`Â vsÂ `Finance_Excel`. |
| **Time-to-Insight**  | **< 10 Minutes**                       | Before, it took 2 days to get a report. Now, they can find the answer in minutes.                              | Interview with the Logistics Manager.                 |
| **Self-Service %**   | **50% Reduction**Â in Ad-Hoc Requests   | Stakeholders should be able to filter the dashboard themselves instead of asking you for a new CSV every time. | Count of "Can you pull this data?" emails.            |

### **Category B: Technical Performance (The "Health" Metrics)**

_Is the system stable, fast, and scalable?_

| Metric                 | Target                       | Why it matters                                                                                     | Validation Method                             |
| ---------------------- | ---------------------------- | -------------------------------------------------------------------------------------------------- | --------------------------------------------- |
| **Data Freshness**     | **Daily (T-1)**Â by 8:00 AM   | Stakeholders need yesterday's numbers before their 9 AM standup meeting.                           | CheckÂ `last_refreshed`Â timestamp in Power BI. |
| **Dashboard Speed**    | **< 4 Seconds**Load Time     | If a visual takes >10 seconds to load, users assume it's broken and close the tab.                 | Power BI Performance Analyzer.                |
| **Pipeline Stability** | **99% Success Rate**         | The dbt/Snowflake pipeline should not fail more than once per quarter.                             | Azure Data Factory / Snowflake History Logs.  |
| **Data Quality**       | **0 Critical Test Failures** | If aÂ `Primary Key`Â is duplicated or aÂ `Price`Â is negative, the pipeline should stop automatically. | `dbt test`Â results (Severity: Error).         |

### 8) Tools and Technologies

| **Stage**       | **Tool**           | **Purpose**                                         |
| --------------- | ------------------ | --------------------------------------------------- |
| Data Ingestion  | Azure Blob Storage | Raw data landing zone for Olist datasets            |
| Storage         | Snowflake          | Central cloud data warehouse                        |
| Transformation  | dbt Fusion         | Data modeling, business logic, tests, documentation |
| Analytics Layer | dbt Marts          | Fact & dimension tables for BI consumption          |
| Visualization   | Power BI           | Semantic model, dashboards, KPI tracking            |
| Version Control | Git + GitHub       | Code versioning and collaboration                   |
| CI / Validation | GitHub Actions     | Automated dbt runs and tests                        |
| Documentation   | Notion             | ADLC tracking, business rules, decisions            |

[Examples](https://www.notion.so/Examples-2cd1bb84f4a481f99a44d724b13a3a28?pvs=21)

# **ğŸ“ŠÂ Phase 2: Data Acquisition & Source Integration**

[Phase 1: Summary](https://www.notion.so/Phase-1-Summary-2cd1bb84f4a48170bc1fd2eafab92962?pvs=21)

[DataOps Strategy](https://www.notion.so/DataOps-Strategy-2cd1bb84f4a481748206fd191dbbd9e3?pvs=21)

### ğŸ¯ **Objective**

To load, validate, and organize raw data into the **Snowflake RAW schema**, ensuring itâ€™s clean, traceable, and version-controlled before transformation begins in dbt.

### DataFinOps Strategy

### **ğŸ’¸ DataFinOps Strategy (Cost Design)**

- [ ] [ ]Â **Define Warehouse Sizing Policy:**
  - **Dev/Loading:**Â AssignÂ `X-SMALL`Â (1 credit/hr).
  - **Prod Transformation:**Â AssignÂ `SMALL`Â (2 credits/hr) orÂ `MEDIUM`Â only if data volume > 10GB.
  - **Justification:**Â Document why these sizes were chosen based on estimated scan volume.
- [ ] [ ]Â **Define Storage Lifecycle Policy (Azure):**
  - **Hot Tier:**Â Current data (< 30 days).
  - **Cool Tier:**Â Data accessed monthly (30-180 days).
  - **Archive Tier:**Â Compliance data (> 180 days).
  - **Action:**Â Enable "Lifecycle Management" rules in Azure Storage Account settings.
- [ ] [ ]Â **Select Data Formats:**
  - **Raw Layer:**Â MandateÂ **Parquet**Â orÂ **Avro**Â (Compressed) instead of CSV/JSON to reduce storage & scan costs.
- [ ] [ ]Â **Plan Incremental Strategy:**
  - **Fact Tables:**Â Identify all Fact tables > 1M rows. Mark them forÂ `materialized='incremental'`Â in dbt.
  - **Snapshots:**Â Define validity strategy (e.g.,Â `check`Â vsÂ `timestamp`) to minimize row churning.
- [ ] [ ]Â **Configure Time Travel Retention:**
- *Action:*Â SetÂ **Data Retention = 0 days**Â for Dev/Staging schemas. Keep 1-7 days only forÂ `PROD_MARTS`.
- *Rationale:*Â We don't need to recover yesterday's test data.
- [ ] [ ]Â **Configure "Aggressive" Auto-Suspend:**
  - *Action:*Â Run SQL command:Â `ALTER WAREHOUSE DEV_WH SET AUTO_SUSPEND = 60;`
  - *Impact:*Â Stops billing 1 minute after query finishes (vs. default 10 mins).

### **2. Snowflake Transient Tables**

- **The Logic:**Â Snowflake keeps "Fail-safe" backups (7 days) for all tables, which costs storage. Staging data doesn't need backups because it can be re-loaded from Azure.
- **The FinOps Move:**Â SetÂ transient: trueÂ inÂ dbt_project.ymlÂ for allÂ stagingÂ andÂ intermediateÂ models.
  - *Result:*Â No Fail-safe costs for temporary data.
- _Code:_
- [ ] [ ]Â **Apply "Shift-Left" Filtering:**
  - *Action:*Â AddÂ `WHERE`Â clauses in Staging models (e.g.,Â `order_date >= '2024-01-01'`) to limit data volume early.
- [ ] [ ]Â **Set Resource Monitors:**
  - *Action:*Â Create a Snowflake Resource Monitor toÂ **Suspend & Notify**Â at 80% of monthly budget (e.g., 20 Credits).
  - [ ] **Zero-Copy Cloning for Dev Environments:**
  - *Action:*Â Instead of loading raw data twice (paying compute), useÂ `CLONE`Â to create Dev environments from Prod.
  - *Rationale:*Â Instant environment setup with $0 storage cost.
  ### **3. "Slim CI" (The GitHub Action Saver)**
  - **The Logic:**Â When you open a Pull Request to changeÂ *one*Â model, standard CI runsÂ *all*Â models. This wastes credits.
  - **The FinOps Move:**Â Configure dbt to run only modified models.
    - *Command:*Â dbt build --select state:modified+
    - *Result:*Â Drastically reduces the compute cost of your CI pipeline.
  ### **2. Query Timeouts**
  - **The Logic:**Â Sometimes BI tools hang or queries get stuck.
  - **The FinOps Move:**Â SetÂ STATEMENT_TIMEOUT_IN_SECONDS = 3600Â (1 hour) on the Warehouse level. Kill the query automatically before it drains the wallet.

### \*) AI Assistant & Workflow

### Custom Chat Modes

# **1)Â `Snowflake_Infra_Architect`**

## âœ” What This Mode Will Do

This mode is responsible forÂ **everything related to Snowflake infrastructure setup**, including:

### **RBAC / SECURITY**

- Create human roles (`ANALYTICS_ROLE`,Â `REPORTER_ROLE`)
- Create service roles (`LOADER_ROLE`,Â `CI_SERVICE_ROLE`)
- Build role hierarchy & apply least privilege
- Assign users to correct roles safely

### **COMPUTE (WAREHOUSES)**

- Create Loading, Transform, Reporting warehouses
- Configure autosuspend, autresume, scaling policy
- Add comments, cost-optimised configs

### **DATABASE ARCHITECTURE**

- RAW_DB (landing zone)
- ANALYTICS_PROD (gold layer)
- ANALYTICS_DEV (zero-copy clone)
- COMMON_DB (utility tables)

### **SCHEMAS**

- RAW schemas (LANDING, SYSTEM schemas)
- Analytics schemas (STAGING, INTERMEDIATE, MARTS)

### **GOVERNANCE / COST**

- Resource monitors
- Time Travel configuration
- Fail-safe best practices
- Warehouse cost isolation

### **What NOT to do**

- No Azure blob storage
- No ingestion
- No dbt modeling
- No loading scripts

# **2)`Snowflake_Data_Loader`**

## âœ” What This Mode Will Do

This mode handlesÂ **all ingestion, staging-area setup, metadata creation, and load audit logic**, including:

### **FILE FORMATS**

- Create CSV / JSON file formats
- Handle delimiters, quotes, skip headers, etc.

### **STAGES (EXTERNAL OR INTERNAL)**

- Create stages (internal only â€” Azure not included)

### **TABLE CREATION**

- Create raw tables using:
  - `USING TEMPLATE`
  - Manual DDL if needed

### **COPY INTO Ingestion**

- Build patterns for folder/file matching
- Add match-by-column-name logic
- Add error handling (`RETURN_ERRORS`,Â `SKIP_FILE`, etc.)

### **LOAD METADATA**

Automatically manage:

- `_source_file`
- `_loaded_at`
- `_file_row_number`
- `_load_batch_id`

### **DATA VALIDATION**

- Row count checks
- Load history checks
- Data anomaly detection SQL

## Data Quality Checks on Tables

- Outlier detection (numeric / date anomalies)
- Null Checks
- Duplicate checks
- Triming Checks and Cleaning Check
- Categorial Checks

# **3)`Snowflake_Debug_Optimizer`**

### **Role:**Â Debugging, error resolution, ingestion performance, metadata QC\*\*

Used when ingestion fails, loads incorrectly, or performance drops.

---

## **What This Mode Does in Phase 2**

### âœ” Fixes ingestion errors:

- Column mismatch
- Encoding errors
- File corruption
- Incorrect formats

### âœ” Improves COPY performance:

- Pattern tuning
- Warehouse tuning

### âœ” Improves metadata quality:

- Detects duplicate loads
- Detects missing rows
- Detects corrupted data

### âœ” Sets best practices for ingestion scaling:

- Incremental loading
- Multi-file loading
- Caching optimizations

### 1) Data Source Register

> Purpose:

> Create a single source of truth for where data comes from, how often it changes, and who owns it, before ingestion into Snowflake.

| Source Name                        | Description                                                                               | Format | Frequency                      | Owner          |
| ---------------------------------- | ----------------------------------------------------------------------------------------- | ------ | ------------------------------ | -------------- |
| olist_orders                       | Order lifecycle data including order status, purchase timestamp, approval, delivery dates | CSV    | One-time (Historical Snapshot) | Olist / Kaggle |
| olist_order_items                  | Line-item level data for each order (products, sellers, price, freight)                   | CSV    | One-time (Historical Snapshot) | Olist / Kaggle |
| olist_customers                    | Customer identifiers and geographic information (ZIP, city, state)                        | CSV    | One-time (Historical Snapshot) | Olist / Kaggle |
| olist_products                     | Product catalog with category and physical attributes                                     | CSV    | One-time (Historical Snapshot) | Olist / Kaggle |
| olist_sellers                      | Seller master data including seller location                                              | CSV    | One-time (Historical Snapshot) | Olist / Kaggle |
| olist_payments                     | Payment method, installments, and payment value per order                                 | CSV    | One-time (Historical Snapshot) | Olist / Kaggle |
| olist_geolocation                  | ZIP code to latitude/longitude mapping                                                    | CSV    | One-time (Reference Data)      | Olist / Kaggle |
| olist_product_category_translation | Portuguese â†’ English product category mapping                                             | CSV    | One-time (Reference Data)      | Olist / Kaggle |

### 2) Naming Conventions

# ğŸ“ Governance & Naming Standards

> Goal:Â Maintain strict consistency across the full stack.
>
> - **SQL/Snowflake:**Â `UPPER_SNAKE_CASE`Â (To avoid quoting identifiers).
> - **Python/dbt:**Â `lower_snake_case`Â (Industry standard).
> - **Azure/Git:**Â `kebab-case`Â (Lowercase with hyphens).
> - **Power BI:**Â `Title Case`Â (User-friendly natural language).

### **1. â˜ï¸ Azure Infrastructure (The Lake)**

_Azure enforces lowercase for many resources. Do not fight this._

| **Object**          | **Pattern**                     | **Example**                        |
| ------------------- | ------------------------------- | ---------------------------------- |
| **Resource Group**  | `rg-[project]-[env]-[region]`   | `rg-retail-dev-eus2`               |
| **Storage Account** | `st[project][env]`Â (No chars)   | `stretailanalyticsprod`            |
| **Containers**      | `[content]-[layer]`             | `raw-landing`,Â `archive-processed` |
| **File Path**       | `[source]/[table]/v=[version]/` | `stripe/orders/v=1/`               |
| **Files**           | `[table]_[timestamp].csv`       | `orders_20251025.csv`              |

### **2. â„ï¸ Snowflake (The Warehouse)**

_Strictly UPPERCASE to ensure SQL queries don't need double quotesÂ `""`._

| **Object**         | **Pattern**            | **Example**                           |
| ------------------ | ---------------------- | ------------------------------------- |
| **Warehouse**      | `[FUNCTION]_WH_[SIZE]` | `LOADING_WH_XS`,Â `TRANSFORM_WH_S`     |
| **Database**       | `[LAYER]_DB`           | `RAW_DB`,Â `ANALYTICS_PROD`            |
| **Schema (Raw)**   | `[SOURCE_SYSTEM]`      | `STRIPE`,Â `SALESFORCE`                |
| **Schema (Marts)** | `[DOMAIN]`Â orÂ `MARTS`  | `FINANCE`,Â `MARKETING`,Â `MARTS`       |
| **Role (Service)** | `[TOOL]_USER_ROLE`     | `DBT_CLOUD_ROLE`,Â `POWERBI_READ_ROLE` |
| **Role (Human)**   | `[JOB]_ROLE`           | `ANALYTICS_ROLE`                      |

### **3. ğŸŸ§ dbt (The Transformation)**

_Follows dbt Labs style guide. Strictly lowercase._

| **Layer**        | **File Prefix** | **Structure**                    | **Example**                                          |
| ---------------- | --------------- | -------------------------------- | ---------------------------------------------------- |
| **Sources**      | `src_`          | `src_[source].yml`               | `src_stripe.yml`                                     |
| **Staging**      | `stg_`          | `stg_[source]__[table].sql`      | `stg_stripe__payments.sql`Â (Note: double underscore) |
| **Intermediate** | `int_`          | `int_[entity]_[verb].sql`        | `int_orders_joined.sql`                              |
| **Facts**        | `fct_`          | `fct_[process/event].sql`        | `fct_orders.sql`,Â `fct_monthly_revenue.sql`          |
| **Dimensions**   | `dim_`          | `dim_[entity].sql`               | `dim_customers.sql`,Â `dim_products.sql`              |
| **Bridge**       | `bridge_`       | `bridge_[table_a]_[table_b].sql` | `bridge_orders_tags.sql`                             |

### **4. ğŸ“Š Power BI (The Presentation)**

_Optimized for Q&A, AI, and Business Users. No underscores._

| **Object**         | **Pattern**                  | **Example**                                        |
| ------------------ | ---------------------------- | -------------------------------------------------- |
| **Workspace**      | `[Project] - [Env]`          | `Sales Analytics - PROD`                           |
| **Semantic Model** | `[Domain] Data Model`        | `Retail Sales Data Model`                          |
| **Tables**         | `[Entity]`Â (Singular/Plural) | `Customer`,Â `Sales`,Â `Date`Â (RemoveÂ `dim_`/`fct_`) |
| **Key Columns**    | `[Entity] ID`                | `Customer ID`,Â `Order ID`                          |
| **Date Columns**   | `[Entity] Date`              | `Order Date`,Â `Ship Date`                          |
| **Measures**       | `[Name]`Â (Clean)             | `Total Revenue`,Â `YoY Growth %`                    |
| **Tech Folders**   | `_Technical`                 | Folder forÂ `_Last Refresh`,Â `_Sort Columns`        |

### **5. ğŸ™ Git & Version Control**

_Optimized for CLI and Windows/Linux compatibility._

| **Object**         | **Pattern**               | **Example**                  |
| ------------------ | ------------------------- | ---------------------------- |
| **Repo Name**      | `[project]-data-pipeline` | `retail-analytics-pipeline`  |
| **Main Branch**    | `main`                    | `main`                       |
| **Feature Branch** | `feature/[ticket]-[desc]` | `feature/dt-101-add-stripe`  |
| **Fix Branch**     | `fix/[ticket]-[desc]`     | `fix/dt-105-cast-error`      |
| **Commit Msg**     | `[type]: [description]`   | `feat: add fct_orders model` |

---

### **6. ğŸ§  dbt Semantic Layer (MetricFlow)**

_Optimized for readability by Business Users and AI Agents._

| **Object**         | **Pattern**                    | **Example**                           | **Why?**                                                                                 |
| ------------------ | ------------------------------ | ------------------------------------- | ---------------------------------------------------------------------------------------- |
| **Semantic Model** | `[entity_name]`Â (Plural)       | `orders.yml`,Â `customers.yml`         | Maps 1:1 to a logical concept, usually a Mart table.                                     |
| **Entity (Keys)**  | `[entity_name]`(Singular)      | `order`,Â `customer`,Â `product`        | These act as the "Join Keys". Must be unique across the project.                         |
| **Dimensions**     | `[attribute_name]`             | `status`,Â `country`,Â `is_churned`     | Descriptive attributes. AvoidÂ `dim_`Â prefix here.                                        |
| **Time Dimension** | `[event]_at`Â orÂ `[event]_date` | `ordered_at`,Â `created_date`          | Explicitly states the timeline.Â `metric_time`Â is the system default.                     |
| **Measures**       | `[aggregation]_[column]`       | `sum_amount`,Â `count_id`,Â `min_price` | **Technical building blocks.**Â Not for end users. Prefix helps distinguish from metrics. |
| **Metrics**        | `[business_concept]`           | `total_revenue`,Â `aov`,Â `churn_rate`  | **The Final Product.**Â Clean, business-friendly names. No prefixes                       |

### ğŸ“ To-Do: How to Enforce This

_Add this task to your Phase 3 or 4 checklist._

- [ ] [ ]Â **CreateÂ `docs/NAMING_CONVENTIONS.md`**
  - *Action:*Â Copy the tables above into this file in your GitHub repo.
  - *Reason:*Â When you use GitHub Copilot, you can reference this file (`@docs/NAMING_CONVENTIONS.md`) so the AI automatically names your files correctly.

### 3) Azure Blob Storage Setup (done)

> Goal:Â Create a secure, organized cloud storage location to act as the "Landing Zone" for raw data before it hits Snowflake.

### **âœ” 2.1 Storage Infrastructure**

- [ ] [ ]Â **Create Resource Group**
  - *Action:*Â Create a new group namedÂ `rg-portfolio-data`Â to keep project resources isolated.
  - *Region:*Â Select theÂ **exact same region**Â as your Snowflake account (e.g., East US 2) to avoid data transfer fees.
- [ ] [ ]Â **Create Storage Account**
  - *Performance:*Â `Standard`Â (Premium is unnecessary for this).
  - *Redundancy:*Â `LRS`Â (Locally-redundant storage) to minimize costs.
  - *Access Tier:*Â `Hot`Â (Best for active projects).
  - *Hierarchical Namespace:*Â **Disabled**Â (standard Blob) is fine for personal projects, orÂ **Enabled**Â (Data Lake Gen2) if you want to demonstrate enterprise skills.
- [ ] [ ]Â **Apply Metadata Tags**
  - `Project`:Â `[Your Project Name]`
  - `Environment`:Â `Dev`

### **âœ” 2.2 Container & Folder Architecture**

- [ ] [ ]Â **Create Main Container**
  - *Name:*Â `raw-landing`
  - *Public Access Level:*Â **Private (no anonymous access)**.
- [ ] [ ]Â **Create Archive Container**Â (Optional best practice)
  - *Name:*Â `archive-processed`
- [ ] [ ]Â **Implement Directory Hierarchy**
  - *Standard:*Â `container / source_system / entity / YYYY / MM / file.csv`
  - *Task:*Â Create folder shells for your specific data:Plaintext
    ```jsx
    raw-landing/
    â”œâ”€â”€ stripe/
    â”‚   â”œâ”€â”€ payments/
    â”‚   â””â”€â”€ customers/
    â””â”€â”€ google_analytics/
        â””â”€â”€ page_views/
    ```

### **âœ” 2.3 Naming Conventions & Governance**

- [ ] [ ]Â **Define File Naming Rules**Â (Write these in your project README)
  - UseÂ `snake_case`Â only (no spaces).
  - Include data extraction date in filename.
  - *Format:*Â `entity_extract_YYYY_MM_DD.csv`
  - *Example:*Â `payments_extract_2025_11_29.csv`
- [ ] [ ]Â **Validate Source Files**
  - Ensure headers are present in the first row.
  - Ensure delimiter is consistent (commaÂ `,`Â or pipeÂ `|`).

### **âœ” 2.4 Security & Access (SAS Token)**

- [ ] [ ]Â **Generate Shared Access Signature (SAS)**
  - *Scope:*Â **Container Level**Â (strictly forÂ `raw-landing`), not Account Key.
  - *Permissions:*Â `Read`Â andÂ `List`Â only.
  - *Start/Expiry:*Â Set expiry for 6-12 months from today.
  - *Allowed Protocols:*Â HTTPS only.
- [ ] [ ]Â **Secure Credentials**
  - *Action:*Â Copy theÂ **Blob SAS URL**Â andÂ **SAS Token**.
  - *Action:*Â Paste them into a localÂ `.env`Â file or password manager.
  - *Warning:*Â **NEVER**Â commit these keys to Git or paste them into clear text in Notion.

### **âœ” 2.5 Data Seeding**

- [ ] [ ]Â **Perform Initial Upload**
  - Upload your sample/historicalÂ `.csv`Â orÂ `.json`Â files into the correct folder paths created in step 2.2.
  - Verify files appear correctly in the Azure Portal "Storage Browser."

---

### âœ”Â **2.5 Connect Azure Blob â†’ Snowflake**

This is done in Snowflake SQL (next step), but prepare inputs here:

- [ ] Container URL
- [ ] Storage Account Name
- [ ] SAS Token
- [ ] File path

### 4) Create a Warehouse and Database and Schema in Snowflake(done)

Goal:Â configure a secure, cost-efficient, and scalable architecture. This setup separates "Compute" (Warehouses) from "Storage" (Databases) and prepares the environment for automation.

### **âœ” 3.1 Role-Based Access Control (RBAC) Foundation**

- [ ] [ ]Â **Create Functional Roles (Human Roles)**
  - `ANALYTICS_ROLE`: For you (and future developers). Has full read/write access to Dev and Prod models.
  - `REPORTER_ROLE`: For Power BI/Dashboard viewers. Read-only access to the final Marts only.
- [ ] [ ]Â **Create Service Roles (Machine Roles)**
  - `LOADER_ROLE`: Specifically for ingestion tools (e.g., Azure integration or Fivetran).
  - `CI_SERVICE_ROLE`: Specifically for GitHub Actions to runÂ `dbt`Â in the CI pipeline.
- [ ] [ ]Â **Establish Role Hierarchy**
  - GrantÂ `ANALYTICS_ROLE`Â toÂ `SYSADMIN`Â (Keeps objects visible to admin).
  - GrantÂ `LOADER_ROLE`Â andÂ `CI_SERVICE_ROLE`Â toÂ `SYSADMIN`.
- [ ] [ ]Â **Create Users & Assign Roles**
  - Create your user and assignÂ `ANALYTICS_ROLE`.
  - Create a Service User (`SVC_GITHUB_ACTIONS`) and assignÂ `CI_SERVICE_ROLE`.

### **âœ” 3.2 Compute Configuration (Virtual Warehouses)**

- [ ] [ ]Â **Create Loading Warehouse**
  - *Name:*Â `LOADING_WH`
  - *Size:*Â `X-SMALL`
  - *Max Clusters:*Â `1`Â (Standard scaling).
  - *Auto-Suspend:*Â `60 seconds`Â (Aggressive cost saving).
  - *Comment:*Â "Dedicated to raw data ingestion."
- [ ] [ ]Â **Create Transformation Warehouse**
  - *Name:*Â `TRANSFORM_WH`
  - *Size:*Â `X-SMALL`
  - *Auto-Suspend:*Â `60 seconds`.
  - *Auto-Resume:*Â `True`.
  - *Comment:*Â "Dedicated to dbt transformations."
- [ ] [ ]Â **Create Reporting Warehouse**
  - *Name:*Â `REPORTING_WH`
  - *Size:*Â `X-SMALL`.
  - *Auto-Suspend:*Â `300 seconds`Â (5 mins to keep cache warm).
  - *Comment:*Â "Dedicated to Power BI DirectQuery/Import."

### **âœ” 3.3 Database Architecture (The "Three-Legged Stool")**

- [ ] [ ]Â **Create Raw Database (`RAW_DB`)**
  - *Purpose:*Â Immutable landing zone.
  - *Time Travel:*Â SetÂ `DATA_RETENTION_TIME_IN_DAYS = 0`Â (Save storage costs; data is backed up in Azure).
  - *Permissions:*Â GrantÂ `OWNERSHIP`Â toÂ `LOADER_ROLE`; GrantÂ `USAGE/READ`Â toÂ `ANALYTICS_ROLE`.
- [ ] [ ]Â **Create Production Analytics Database (`ANALYTICS_PROD`)**
  - *Purpose:*Â The "Golden Copy" for BI.
  - *Time Travel:*Â SetÂ `DATA_RETENTION_TIME_IN_DAYS = 1`Â (Standard data protection).
  - *Permissions:*Â GrantÂ `OWNERSHIP`Â toÂ `CI_SERVICE_ROLE`Â (or dbt); GrantÂ `READ`Â toÂ `REPORTER_ROLE`.
- [ ] [ ]Â **Create Development Database (`ANALYTICS_DEV`)**
  - *Purpose:*Â Sandbox for development.
  - *Method:*Â **Zero-Copy Clone**Â `ANALYTICS_PROD`Â to create this initially.
  - *Permissions:*Â GrantÂ `OWNERSHIP`Â toÂ `ANALYTICS_ROLE`.
- [ ] [ ]Â **Create Common Database (`COMMON_DB`)**
  - *Purpose:*Â Utility tables (Date Dimension, Country Codes).

### **âœ” 3.4 Schema Definition Strategy**

- [ ] [ ]Â **Define Raw Schemas (inÂ `RAW_DB`)**
  - `LANDING`: For External Stages and File Formats.
  - `[SOURCE_SYSTEM]`: e.g.,Â `STRIPE`,Â `SAP`Â (For the raw tables).
- [ ] [ ]Â **Define Analytics Schemas (inÂ `ANALYTICS_PROD`)**
  - *Note:*Â dbt will manage these, but define the convention now.
  - `STAGING`: Views cleaning raw data.
  - `INTERMEDIATE`: Logic and joins.
  - `MARTS`: Final Star Schema tables.

### **âœ” 3.5 Advanced Features Configuration**

- [ ] [ ]Â **Configure Time Travel & Fail-safe**
  - VerifyÂ `ANALYTICS_PROD`Â hasÂ **1 Day**Â retention.
  - VerifyÂ `RAW_DB`Â hasÂ **0 Day**Â retention.
- [ ] [ ]Â **Initialize Development Environment (Cloning)**
  - ExecuteÂ `CREATE DATABASE ANALYTICS_DEV CLONE ANALYTICS_PROD;`
  - *Goal:*Â This ensures your Dev environment starts with the exact same structure as Prod without paying for double storage.

### **âœ” 3.6 Resource Monitors (Cost Guardrails)**

- [ ] [ ]Â **Create Global Resource Monitor**
  - *Name:*Â `PORTFOLIO_BUDGET_MONITOR`
  - *Credit Quota:*Â `20`Â (Approx $40-$60 depending on region/edition).
  - *Cycle:*Â Monthly.
- [ ] [ ]Â **Set Triggers**
  - Notify atÂ `75%`.
  - Notify atÂ `90%`.
  - **Suspend Immediately**Â atÂ `100%`Â (Hard stop to prevent accidental bills).
- [ ] [ ]Â **Apply Monitor**
  - Assign toÂ `LOADING_WH`,Â `TRANSFORM_WH`, andÂ `REPORTING_WH`.

Benefits of Zero Copy Clone

- **Cost-effective:** No additional storage costs until data in the clone is modified
- **Fast:** Instantaneous creation regardless of database size
- **Safe testing:** Test transformations and queries without affecting production data
- **Easy rollback:** Can drop and recreate clones as needed for fresh testing environments

<aside>
ğŸ’¡ **Best Practice:** Use the DEV clone for testing dbt models and transformations before deploying to production. Refresh the clone periodically to sync with production data changes.

</aside>

### Best Practices

- **Never use `ACCOUNTADMIN` for daily work:** Use it only for initial setup and billing. Switch to `SYSADMIN` or `ANALYTICS_ROLE` to prevent accidental deletion.
- **The "60-Second" Rule:** Set `AUTO_SUSPEND` to 60 seconds for Dev and Loading warehouses to avoid unnecessary charges.
- **Isolate Workloads:** Keep heavy data loads separate from `REPORTING_WH` to prevent dashboard performance issues.
- **Tag Everything:** Use `QUERY_TAG` in dbt and Session Tags in load scripts to track costs.
- **Zero-Copy for Safety:** `CLONE` to a temporary database before major Prod updates to avoid breaking changes.

### 5) Data Ingestion & Schema Definition(done)

### **âœ” 4.1 Governance & Naming Standards**

- [ ] [ ]Â **Define Naming Conventions**Â (Update yourÂ `README.md`Â or Notion Wiki)
  - **Tables:**Â Plural, snake_case (e.g.,Â `orders`,Â `customer_payments`).
  - **Columns:**Â Snake_case (e.g.,Â `order_date`Â notÂ `OrderDate`).
  - **Audit Columns:**Â Prefix with underscore (e.g.,Â `_loaded_at`).
- [ ] [ ]Â **Define Primary Keys & Constraints**
  - *Action:*Â Identify the unique identifier for each source file (e.g.,Â `order_id`).
  - *Note:*Â Snowflake constraints are "Not Enforced" but are critical for the Query Optimizer and dbt testing.

### **âœ” 4.1 Configure Ingestion Objects (Stage & Formats)**

- [ ] [ ]Â **Create File Formats**
  - *Location:*Â `RAW_DB.LANDING`Â schema.
  - *CSV Format:*Â CreateÂ `CSV_GENERIC_FMT`Â withÂ `TYPE = CSV`,Â `FIELD_OPTIONALLY_ENCLOSED_BY = '"'`,Â `SKIP_HEADER = 1`.
  - *JSON Format:*Â CreateÂ `JSON_GENERIC_FMT`Â withÂ `TYPE = JSON`,Â `STRIP_OUTER_ARRAY = TRUE`.
- [ ] [ ]Â **Create External Stage**Â (The Bridge)
  - *Action:*Â RunÂ `CREATE STAGE`.
  - _Parameters:_
    - `URL = 'azure://<account>.blob.core.windows.net/raw-landing'`
    - `STORAGE_INTEGRATION = [Your_Integration_Name]`Â (from Phase 2).
    - `FILE_FORMAT = CSV_GENERIC_FMT`.
  - *Validation:*Â RunÂ `LIST @RAW_DB.LANDING.AZURE_STAGE;`Â to ensure you can see the files.
- [ ] [ ]Â **Create Tables Automatically**
  - *Action:*Â Use theÂ `CREATE TABLE ... USING TEMPLATE`Â command.
  - *Why:*Â This creates the table structure inÂ `RAW_DB`Â with the correct column names and data types (String, Number, etc.) derived directly from the file.
  - *Alternative:*Â If manual control is needed, write theÂ `CREATE TABLE`Â DDL manually with strict types.

### **âœ” 4.3 Execute Data Loading (Ingestion)**

- [ ] [ ]Â **RunÂ `COPY INTO`Â Command**
  - *Source:*Â `@RAW_DB.LANDING.AZURE_STAGE/folder/`
  - *Target:*Â `RAW_DB.SOURCE_SYSTEM.TABLE_NAME`
  - *Pattern:*Â UseÂ `PATTERN='.*orders.*.csv'`Â to load all matching files in one go.
- [ ] [ ]Â **Implement Error Handling**
  - *Parameter:*Â SetÂ `ON_ERROR = 'CONTINUE'`Â (to load good rows and skip bad ones) ORÂ `ON_ERROR = 'SKIP_FILE'`(to reject the whole file if one row is bad).
  - *Logging:*Â EnableÂ `RETURN_FAILED_ONLY = TRUE`Â initially to see what broke.

### **âœ” 4.4 Data Validation & Auditing**

- [ ] [ ]Â **Verify Row Counts**
  - Compare theÂ `ROW_COUNT`Â in the Snowflake "Load History" view vs. the number of records in your source CSV.
- [ ] [ ]Â **Check Load History**
  - QueryÂ `INFORMATION_SCHEMA.LOAD_HISTORY`Â to see a log of exactly which files were loaded and when.
- [ ] [ ]Â **Spot Check Data**
  - RunÂ `SELECT * FROM table LIMIT 100`Â to ensure columns aren't aligned incorrectly (e.g., Dates appearing in the Name column).

---

### ğŸ“ Deliverables for this Phase

- **DDL Script:**Â AÂ `.sql`Â file containing theÂ `CREATE FILE FORMAT`Â andÂ `COPY INTO`Â commands.
- **Load Report:**Â A screenshot of the query result fromÂ `LOAD_HISTORY`Â showing "Status: LOADED".

### ğŸ’¡ Best Practices & Tips

- **UseÂ `MATCH_BY_COLUMN_NAME`:**Â If your CSV columns might change order in the future, addÂ `MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE`Â to yourÂ `COPY INTO`Â command. This maps data by name, not by position.
- **Don't transform yet:**Â Do not try to rename columns or change logic during theÂ `COPY INTO`Â step. Load itÂ **exactly**Â as it is. Fix the names later in dbt.
- **Snowflake Constraints:** You can runÂ `ALTER TABLE orders ADD CONSTRAINT pk_orders PRIMARY KEY (order_id);`.**Tip:**Â Even though Snowflake doesn't stop you from inserting duplicates, defining this tells Power BI and dbt exactly how to join tables later.
- **indempotency:**Â Your loading process should be repeatable. If you run the load script twice, it shouldn't duplicate data.*Snowflake Native:*Â `COPY INTO`Â automatically tracks files it has already loaded and won't load them again unless you modify the file or useÂ `FORCE = TRUE`.

### ğŸš€ Pro Tips & Tricks (Performance & Workflow)

- **The "Dry Run" (Validation Mode):**
  - Before you actually load data, run theÂ `COPY INTO`Â command withÂ `VALIDATION_MODE = 'RETURN_ERRORS'`.
  - *Result:*Â Snowflake simulates the load and tells you exactly which rowsÂ *would*Â fail, without actually inserting any data. This is a lifesaver for debugging.
  - **The "Audit" Columns:**
  - `_source_file`: Essential for debugging. If a specific file was corrupted, you can quickly find and delete only those rows usingÂ `DELETE FROM table WHERE _source_file = 'bad_file.csv'`.
  - `_loaded_at`: Helps you track data freshness.

### 6) Data Dictionary(done)

Create comprehensive documentation of every field, including business definitions and calculation logic. Essential for collaboration and future maintenance.

[Example](https://www.notion.so/Example-2cd1bb84f4a481f0ae3adac89024e837?pvs=21)

### 7) Record Metadata and Data Quality Issues(done)

Track load timestamps, source file versions, and any anomalies discovered. This audit trail is critical for troubleshooting and stakeholder communication.

[Example](https://www.notion.so/Example-2cd1bb84f4a48107a7eecd218e694cd8?pvs=21)

### 8) Optimization and Best Practices(done)

- **Use Clustering Keys:** Apply on frequently filtered columns (date, region) to boost query speed and cut costs
- **Leverage Zero-Copy Cloning:** Create instant, free database copies for dev/test without duplicating storage
- **Optimize Warehouse Sizing:** Start small (X-SMALL/SMALL), scale as needed; enable auto-suspend/resume
- **Implement Incremental Loading:** Use COPY INTO with pattern matching to load only new/changed files
- **Enable Query Result Caching:** Leverage automatic 24-hour cache for identical queries
- **Set Resource Monitors:** Create credit quotas to prevent cost overruns with threshold alerts
- **Monitor Query Performance:** Review Query History regularly to identify and optimize slow queries
- **Use Multi-Table Insert:** Load into multiple tables in one operation for better efficiency

### 9) Version Control in Git and Github

```jsx
my-data-project/
â”œâ”€â”€ docs/                     # For Phase 2 Documentation
â”‚   â”œâ”€â”€ DATA_DICTIONARY_RAW.md
â”‚   â””â”€â”€ NAMING_CONVENTIONS.md
â”œâ”€â”€ snowflake/                # For Phase 2 SQL Scripts
â”‚   â”œâ”€â”€ 01_admin/             # Admin setup scripts
â”‚   â”‚   â”œâ”€â”€ 01_roles_and_rbac.sql
â”‚   â”‚   â””â”€â”€ 02_warehouses_and_monitors.sql
â”‚   â”œâ”€â”€ 02_infrastructure/    # Storage setup
â”‚   â”‚   â”œâ”€â”€ 01_databases_and_schemas.sql
â”‚   â”‚   â””â”€â”€ 02_stages_and_formats.sql
â”‚   â””â”€â”€ 03_raw_tables/        # Data loading
â”‚       â”œâ”€â”€ stripe_tables.sql
â”‚       â””â”€â”€ sap_tables.sql
â”œâ”€â”€ .gitignore                # Critical security file
â””â”€â”€ README.md                 # Project summary
```

> Goal:Â Treat your database infrastructure as "Code." You will write SQL scripts locally in VS Code, version them to track changes, and use GitHub Pull Requests to merge them into your main project, simulating a real-world Data Engineering workflow.

Here is exactly how to document the GitHub Personal Access Token (PAT) configuration in your Notion ADLC framework.
This belongs in your "Build Phase" under a page or toggle named "ğŸ” Security & Access Control".
The Notion Copy-Paste Block
Copy the section below directly into your Notion. It looks professional and proves you understand Security Best Practices (DataOps).
ğŸ” GitHub Authentication (PAT Configuration)
Objective: Configure secure, programmatic access for local development and CI/CD pipelines without using raw passwords.

1. Token Creation Strategy

- [ ] Token Type: Used Classic Token (for broad compatibility with dbt Cloud/local CLI) or Fine-Grained Token (for specific repo access).
- [ ] Expiration Policy: Set to 90 Days (Enforcing key rotation security).
- [ ] Permissions/Scopes Selected:
  - repo (Full control of private repositories) - Required for pushing code.
  - workflow (Update GitHub Action workflows) - Required for CI/CD.
  - read:org - Required if accessing organization-level secrets.

1. Secret Storage (The "No-Leak" Policy)

> âš ï¸ SECURITY WARNING: Never store the actual PAT string in this Notion doc or commit it to Git.

- [x] Local Environment:
  - Stored in .env file (ensure .env is added to .gitignore).
  - Format: GITHUB_TOKEN=ghp_xxxx...
- [x] CI/CD Environment (GitHub Actions):
  - Navigate to: Repo Settings \rightarrow Secrets and variables \rightarrow Actions.
  - Saved as Repository Secret: GH_PERSONAL_ACCESS_TOKEN.
  - Usage: Used in YAML workflows via ${{ secrets.GH_PERSONAL_ACCESS_TOKEN }}.
    How to explain this in an interview ğŸ—£ï¸
    If they ask about how you handle access:

> "I don't use my GitHub password for git operations. I configured a Personal Access Token (PAT) with an expiration date.
> For the pipeline, I stored this PAT as an Encrypted Secret in GitHub Actions. This ensures that even if someone clones my repo, they cannot see my credentials. I treat my portfolio security like a production environment."

Why document this?
It proves you aren't just hacking things together. You have a "Security First" mindset, which is a massive green flag for Financial (JPMC) and Enterprise clients.

### **âœ” 9.1 Initial Repository Setup (One-Time Task)**

- [ ] [ ]Â **Initialize Local Repository**
  - *Action:*Â Open your project folder in VS Code terminal.
  - *Command:*Â `git init`
  - *Check:*Â Verify a hiddenÂ `.git`Â folder appears
- [ ] [ ]Â **Configure Security (`.gitignore`)**
  - *Action:*Â Create a file namedÂ `.gitignore`Â in the root.
  - *Content:*Â Add the following lines to prevent leaking secrets:Plaintext
    `.env
*.creds
*.pem
.DS_Store
venv/
__pycache__/`
- [ ] [ ]Â **Connect to Remote**
  - *Action:*Â Create a new empty repository on GitHub.
  - *Command:*Â `git remote add origin https://github.com/[your-username]/[repo-name].git`

### **âœ” 9.2 The Development Workflow (The Daily Loop)**

- [ ] [ ]Â **Create a Feature Branch**
  - *Rule:*Â Never work directly onÂ `main`.
  - *Action:*Â Create a branch for Phase 2 setup.
  - *Command:*Â `git checkout -b feat/ph2-snowflake-setup`
- [ ] [ ]Â **Develop & Test SQL**
  - *Action:*Â Write your warehouse creation script inÂ `snowflake/01_admin/02_warehouses.sql`.
  - *Test:*Â Copy/Paste the SQL into Snowflake Worksheet and run it.Â **Only save the file if it runs successfully.**
- [ ] [ ]Â **Stage Changes**
  - *Action:*Â Prepare files for saving.
  - *Command:*Â `git add .`Â (Stages all changes) ORÂ `git add snowflake/01_admin/`Â (Stages specific folder).
- [ ] [ ]Â **Commit Changes (Save Snapshot)**
  - *Action:*Â Save the changes with a descriptive message.
  - *Command:*Â `git commit -m "infra: configured roles and warehouses for phase 2"`
  - *Standard:*Â Use prefixes likeÂ `infra:`,Â `docs:`, orÂ `feat:`Â in your message.

### **âœ” 9.3 The Review & Merge Workflow (The "Senior" Step)**

- [ ] [ ]Â **Push to GitHub**
  - *Action:*Â Send your branch to the cloud.
  - *Command:*Â `git push origin feat/ph2-snowflake-setup`
- [ ] [ ]Â **Open Pull Request (PR)**
  - *Action:*Â Go to your GitHub repo URL. Click the green "Compare & pull request" button.
  - *Title:*Â "Setup Snowflake Infrastructure (Phase 2)".
  - *Description:*Â "Created roles, warehouses, and external stages for Azure ingestion."
- [ ] [ ]Â **Self-Review (The Security Check)**
  - *Action:*Â Click the "Files changed" tab in the PR.
  - *Check:*Â **CRITICAL:**Â Ensure no passwords, SAS tokens, or AWS keys are visible. If found, revoke the key and fix the code locally.
- [ ] [ ]Â **Merge to Main**
  - *Action:*Â Click "Merge pull request" -> "Confirm merge".
  - *Result:*Â Your code is now effectively "In Production."

### **âœ” 9.4 Milestone Tagging**

- [ ] [ ]Â **Create Release Tag**
  - *Trigger:*Â When Phase 2 is 100% complete (Data is loaded in Snowflake).
  - *Action:*Â Switch back to main and pull the latest changes.Bash
    `git checkout main
git pull origin main`
  - *Tag:*Â Create a permanent marker.Bash
    `git tag -a v0.2-raw-ingestion -m "Completed Phase 2: Raw Data Ingestion Pipelines"
git push origin v0.2-raw-ingestion`

---

### **ğŸŒŸ Git Best Practices for Phase 2**

- **Atomic Commits:**Â Don't wait until you finish the whole phase to commit. Commit after every logical step (e.g., "created roles," then "created warehouses," then "created stages").
- **No Secrets:**Â Use placeholders in your code likeÂ `azure_sas_token='<INSERT_SECRET_HERE>'`.
- **Cleanup:**Â After merging, delete the feature branch:Â `git branch -d feat/ph2-snowflake-setup`.

###

[Phase 2: Summary and Deliverables](https://www.notion.so/Phase-2-Summary-and-Deliverables-2cd1bb84f4a481289ef7e0d5656e2da2?pvs=21)

# **ğŸ”§Â Phase 3: Data Modeling, Transformation & Quality**

### **Objectives**

1. Set up and configure dbt environment for local and team development.
2. Load raw data (CSV/API/S3) into Snowflake and/or use dbt seeds.
3. Design the logical & physical **data model (Star Schema)** â€” define fact and dimension tables with clear grains.
4. Build dbt models: **staging â†’ intermediate â†’ marts (facts & dims)**.
5. Implement **data quality tests**, **freshness checks**, and **documentation** in dbt.
6. Create **semantic layer and metrics definitions** in dbt.
7. Prepare clean, tested data for Power BI semantic modeling (Phase 6).

### \*) AI Workflow

### Custom Chat Modes

# 1ï¸âƒ£Â **Mode Name:**Â `AE_Environment_Architect`

### **Purpose:**

HandlesÂ **all environment, configuration, tooling, packages, version control**Â for dbt + Snowflake.

### **This Mode Will Do:**

- BuildÂ `profiles.yml`Â for dev/prod
- BuildÂ `dbt_project.yml`Â with layer configs
- Set up Python venv + install dbt-snowflake, sqlfluff
- SetupÂ `.gitignore`Â for dbt
- Setup Git branching strategy
- Setup folder structure: staging / intermediate / marts / macros / tests
- SetupÂ `packages.yml`Â with dbt_utils + dbt_expectations
- Validate environment (`dbt debug`)

# 2ï¸âƒ£Â **CUSTOM MODE:Â `AE_Source_Registrar`**

### âœ…Â **This Mode Covers EVERYTHING About:**

- sources.yml creation
- Mapping RAW_DB tables
- Adding identifiers for weird table names
- Freshness checks (warn + error)
- Source-level tests (unique, not_null)
- seeds folder creation
- seed configuration in dbt_project.yml
- documenting sources
- metadata + ownership (meta tags)
- linking sources â†’ staging lineage

# 3ï¸âƒ£Â **Mode Name:**Â `AE_Staging_Modeler`

### **Purpose:**

BuildsÂ **clean, standardized staging models**.

### **This Mode Will Do:**

- CreateÂ `stg_*`Â models using Import CTE pattern
- Standardize naming & cast data types
- Add surrogate key generation
- Fix categorical values / apply text cleaning
- Add generic tests
- Add singular tests
- Document every staging model

# 4ï¸âƒ£Â **Mode Name:**Â `AE_Intermediate_Modeler`

### **Purpose:**

HandlesÂ **joining, aggregations, fan-out/fan-in, window functions, business logic**, andÂ **grain transformations**.

### **This Mode Will Do:**

- Join staging models
- Enrich data
- Apply business logic
- Define & enforce grain
- Handle duplicates
- Apply window functions
- Pivot / unpivot
- Add tests
- Document transformation logic

# 4ï¸âƒ£Â **Mode Name:**Â `AE_Intermediate_Modeler`

### **Purpose:**

HandlesÂ **joining, aggregations, fan-out/fan-in, window functions, business logic**, andÂ **grain transformations**.

### **This Mode Will Do:**

- Join staging models
- Enrich data
- Apply business logic
- Define & enforce grain
- Handle duplicates
- Apply window functions
- Pivot / unpivot
- Add tests
- Document transformation logic

# 5ï¸âƒ£Â **Mode Name:**Â `AE_Marts_Architect`

### **Purpose:**

CreatesÂ **Star Schema (Fact + Dimension models)**Â with incremental logic & conformed dimensions.

### **This Mode Will Do:**

- Build fact tables (`fct_`)
- Build dimensions (`dim_`)
- Define grain for fact tables
- Generate surrogate keys
- Add incremental configs
- Add relationship tests
- Add business metrics pre-calculation
- Document marts layer
- Prepare tables for Power BI

# 6ï¸âƒ£Â **Mode Name:**Â `AE_Semantic_Modeler`

### **Purpose:**

GeneratesÂ **Semantic Layer (MetricFlow)**Â +Â **Business Metrics**Â and aligns with BI tools.

### **This Mode Will Do:**

ğŸ”§ What this mode delivers

- Set up MetricFlow/dbt Semantic Layer tooling
- CreateÂ **time spine**Â + semantic model YAMLs
- DefineÂ **entities, dimensions, and measures**
- DefineÂ **metrics: simple, ratio, derived, cumulative**
- Validate & query metrics usingÂ **dbt sl**
- Push changes with clean Git workflow

# 7ï¸âƒ£Â **CUSTOM MODE:Â `AE_Debug_Optimizer`**

### ğŸ¯Â **Purpose:**

Handles EVERYTHING related to:

- Debugging dbt errors
- Debugging Snowflake errors
- Query optimization
- Model performance tuning
- Warehouse tuning
- Best practices enforcement
- SQL debugging
- Testing failures
- Lineage issues
- Dependency issues
- CI/CD failures

This is yourÂ **â€œFix Anythingâ€**Â mode.

### 1) Environment Setup & Configuration(done)

**Task:**

Set up your local development environment and configure dbt to connect to Snowflake. Establish proper project structure and version control.

### ğŸ› ï¸ Phase 5: Environment Setup & Configuration

> Goal:Â Establish a robust local development environment, secure the connection between dbt and Snowflake, and initialize version control with best practices.

### **âœ” 5.1 Local Tooling & Prerequisites**

- [ ] [ ]Â **Install Python & Virtual Environment**
  - Install Python (ensure version compatibility with dbt, e.g., 3.10+).
  - Create a virtual environment:Â `python -m venv venv`.
  - Activate environment:Â `source venv/bin/activate`Â (Mac) orÂ `venv\Scripts\activate`Â (Win).
- [ ] [ ]Â **Install Development Tools**
  - InstallÂ **VS Code**.
  - InstallÂ **"dbt Power User"**Â extension (Vital for autocomplete and lineage in VS Code).
  - InstallÂ **"GitGraph"**Â extension (To visualize branches).

### **âœ” 5.2 dbt Core Installation & Initialization**

- [ ] [ ]Â **Install dbt Adapter**
  - RunÂ `pip install dbt-snowflake`.
  - RunÂ `pip install sqlfluff`Â (The linter we discussed in Phase 3).
- [ ] [ ]Â **Initialize Project**
  - RunÂ `dbt init [project_name]`.
  - SelectÂ `snowflake`Â as the database.

### **âœ” 5.3 Connection Configuration (`profiles.yml`)**

- [ ] [ ]Â **Locate Profiles File**
  - Navigate toÂ `~/.dbt/profiles.yml`.
- [ ] [ ]Â **Configure Development Target (`dev`)**
  - *Type:*Â `snowflake`
  - *Account:*Â `[your_account_locator]`
  - *User:*Â `[your_username]`
  - *Role:*Â `ANALYTICS_ROLE`
  - *Warehouse:*Â `TRANSFORM_WH`Â (X-Small)
  - *Database:*Â `ANALYTICS_DEV`
  - *Schema:*Â `dbt_[yourname]`
  - *Threads:*Â `4`
- [ ] [ ]Â **Configure Production Target (`prod`)**
  - *Database:*Â `ANALYTICS_PROD`
  - *Schema:*Â `MARTS`Â (or leaving it blank to use schema configurations in dbt_project.yml)
- [ ] [ ]Â **Secure Credentials**
  - **Action:**Â Use environment variables for passwords (`{{ env_var('DBT_PASSWORD') }}`) instead of hardcoding text.

### **âœ” 5.4 Git & Version Control Strategy**

- [ ] [ ]Â **Initialize Repository**
  - RunÂ `git init`Â inside your project folder.
  - Create aÂ `main`Â branch.
- [ ] [ ]Â **ConfigureÂ `.gitignore`Â (CRITICAL SECURITY STEP)**
  - AddÂ `venv/`Â (Do not commit python libraries).
  - AddÂ `logs/`Â (dbt logs).
  - AddÂ `target/`Â (Compiled SQL artifacts).
  - AddÂ `.env`Â (If using environment variables).
  - AddÂ `profiles.yml`Â (If a local copy exists).
- [ ] [ ]Â **Connect to Remote**
  - Create a new repository on GitHub.
  - RunÂ `git remote add origin [url]`.
  - Perform initial commit and push.

### **âœ” 5.5 dbt Project Configuration (`dbt_project.yml`)**

- [ ] ## Define Project Structure
  - SetÂ `model-paths: ["models"]`.
  - SetÂ `seed-paths: ["seeds"]`.
- [ ] [ ]Â **Configure Model Layers (Materializations)**
  - SetÂ `models:`Â hierarchy:
    - `staging:`Â ->Â `+materialized: view`,Â `+schema: staging`
    - `intermediate:`Â ->Â `+materialized: ephemeral`
    - `marts:`Â ->Â `+materialized: table`,Â `+schema: marts`
- [ ] [ ]Â **Set Query Tags**
  - AddÂ `+query_tag: "dbt_portfolio"`Â to track costs in Snowflake.

### **âœ” 5.6 Dependency Management (`packages.yml`)**

- [ ] [ ]Â **Create Packages File**
  - CreateÂ `packages.yml`Â in the root directory.
- [ ] [ ]Â **Add Standard Libraries**
  - `dbt-labs/dbt_utils`Â (For surrogate keys, date spines).
  - `calogica/dbt_expectations`Â (For advanced testing).
- [ ] [ ]Â **Install Dependencies**
  - RunÂ `dbt deps`.

### **âœ” 5.7 Final Validation**

- [ ] [ ]Â **Test Connectivity**
  - RunÂ `dbt debug`.
  - *Success Criterion:*Â All checks (profiles.yml, dbt_project.yml, git, connection) pass with "OK".
- [ ] [ ]Â **Test Execution**
  - RunÂ `dbt run`Â (on the sample models) to verify write access to Snowflake.

---

### ğŸŒŸ Phase 5 Best Practices

- **Virtual Environments are Non-Negotiable:**Â Never install dbt in your global Python environment. It causes dependency conflicts later. Always useÂ `venv`.
- **Threads Configuration:**Â InÂ `profiles.yml`, settingÂ `threads: 4`Â orÂ `8`Â allows dbt to build multiple models in parallel. This speeds up your run time significantly without costing extra Snowflake credits (since the Warehouse is running anyway).
- **Schema Naming Convention:**
  - By default, dbt concatenates schemas (e.g.,Â `ANALYTICS_DEV_STAGING`).
  - **Tip:**Â Add a macroÂ `generate_schema_name.sql`Â later if you want custom control over this (e.g., justÂ `STAGING`Â in Prod, butÂ `dbt_jdoe_staging`Â in Dev).
- **Environment Variables:**
  - In your terminal/`.zshrc`/`.bash_profile`, export your password:Â `export DBT_PASSWORD='my_secret_password'`.
  - InÂ `profiles.yml`:Â `password: "{{ env_var('DBT_PASSWORD') }}"`.
  - *Why:*Â This ensures that if you accidentally commit your profiles file, your password isn't leaked.

### ğŸ“¦ Phase 5 Deliverables (Checklist)

1. **âœ…Â `dbt debug`Â Screenshot:**Â Proof that your local machine is talking to Snowflake successfully.
2. **ğŸ“„Â `.gitignore`Â File:**Â Verified list of excluded files (security proof).
3. **ğŸ”— GitHub Repo Link:**Â A link to the initialized repository with the initial commit.

### ğŸ› ï¸ Reference: SampleÂ `profiles.yml`Â Structure

YAML

```jsx
# ~/.dbt/profiles.yml

portfolio_project:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: "{{ env_var('SNOWFLAKE_ACCOUNT') }}"
      user: "{{ env_var('SNOWFLAKE_USER') }}"
      password: "{{ env_var('SNOWFLAKE_PASSWORD') }}"
      role: ANALYTICS_ROLE
      warehouse: TRANSFORM_WH
      database: ANALYTICS_DEV
      schema: dbt_yourname
      threads: 4
      client_session_keep_alive: False
```

| **Component**       | **Configuration Item** | **Example Value**                                |
| ------------------- | ---------------------- | ------------------------------------------------ |
| dbt Profile         | Target Schema          | DEV_ANALYTICS / PROD_ANALYTICS                   |
| Snowflake Warehouse | Size                   | X-SMALL (dev), SMALL (prod)                      |
| Git Branching       | Strategy               | main (prod), develop (staging), feature/\* (dev) |
| dbt Materialization | Default                | Staging: view, Marts: table                      |

<aside>

**Best Practices:**

- Use environment variables for sensitive credentials (never commit to Git)
- Establish naming conventions early (e.g., stg*, int*, fct*, dim* prefixes)
- Set up pre-commit hooks to run dbt tests before pushing code
- Document your connection setup in project README for team onboarding
</aside>

### 2) Data Loading & Raw Layer (Source Registration)(done)

**Task:**

> Goal:Â Formally register Snowflake raw tables into dbt, configure automated freshness monitoring, and establish the initial "Staging" layer for data cleaning.

- **Goal:**Â Configure local tooling, dbt connection, and version control.
- [ ] [ ]Â **Install Tooling:**Â Python, Virtual Env (`venv`), VS Code, git.
- [ ] [ ]Â **Install dbt:**Â `pip install dbt-snowflake sqlfluff`.
- [ ] [ ]Â **ConfigureÂ `profiles.yml`:**Â Set up connection to SnowflakeÂ `TRANSFORM_WH`.
- [ ] [ ]Â **Initialize Git:**Â `git init`, configureÂ `.gitignore`Â (hide secrets!).
- [ ] [ ]Â **ConfigureÂ `dbt_project.yml`:**Â Define folders and materialization defaults.
  - *Staging:*Â View
  - *Intermediate:*Â Ephemeral
  - *Marts:*Â Table
- [ ] [ ]Â **Install Packages:**Â AddÂ `dbt_utils`,Â `dbt_expectations`Â toÂ `packages.yml`.

### **âœ” 6.1 Source Registration (`sources.yml`)**

- [ ] [ ]Â **Create Source Definition File**
  - *Location:*Â `models/staging/_sources.yml`Â (Note: Using an underscore ensures it sits at the top).
- [ ] [ ]Â **Define Source Properties**
  - *Database Mapping:*Â Map the dbt sourceÂ `name`Â to the SnowflakeÂ `RAW_DB`.
  - *Schema Mapping:*Â Map the sourceÂ `schema`Â to your specific Snowflake schemas (e.g.,Â `STRIPE`,Â `GOOGLE_ANALYTICS`).
- [ ] [ ]Â **Register Raw Tables**
  - *Action:*Â List every raw table you loaded in Phase 4 (e.g.,Â `orders`,Â `customers`).
  - *Identifier:*Â Use theÂ `identifier`Â property if the Snowflake table name is case-sensitive or messy (though ideally, you fixed this in Phase 4).

### **âœ” 6.2 DataOps: Freshness & Quality Controls**

- [ ] [ ]Â **Configure Freshness Pass/Fail Thresholds**
  - *Warn:*Â SetÂ `warn_after`Â (e.g.,Â `count: 12, period: hour`) to get alerts if data is slightly late.
  - *Error:*Â SetÂ `error_after`Â (e.g.,Â `count: 24, period: hour`) to break the pipeline if data is stale.
  - *Filter:*Â SetÂ `loaded_at_field`Â to your audit columnÂ `_loaded_at`Â (created in Phase 4) for accurate checking.
- [ ] [ ]Â **Apply Source-Level Tests**
  - *Action:*Â AddÂ `unique`Â andÂ `not_null`Â tests strictly to the Primary Keys inÂ `sources.yml`.
  - *Why:*Â This catches data issuesÂ *before*Â any transformation logic runs.

### **âœ” 6.3 Static Data Management (Seeds)**

- [ ] [ ]Â **Prepare Reference Files**
  - *Action:*Â Create CSVs for static lookups (e.g.,Â `country_codes.csv`,Â `subscription_statuses.csv`,`dim_security_rls`).
  - *Formatting:*Â Ensure headers are clean (snake_case).
- [ ] [ ]Â **Configure Seeds (`dbt_project.yml`)**
  - *Action:*Â Define column types explicitly if needed (e.g., ensureÂ `zip_code`Â isÂ `string`, notÂ `integer`Â to preserve leading zeros).
- [ ] [ ]Â **Execute Seed Load**
  - *Command:*Â RunÂ `dbt seed`.
  - *Verification:*Â CheckÂ `ANALYTICS_DEV`Â schema to ensure tables were created.

### **âœ” 6.4 Documentation & Metadata**

- [ ] [ ]Â **Document Sources**
  - *Description:*Â Add a description to theÂ `source`Â level (e.g., "Data ingested from Stripe API via Azure Blob").
  - *Table Descriptions:*Â Briefly explain what each table represents.
- [ ] [ ]Â **Document External Loader**
  - *Meta Tag:*Â Use theÂ `meta:`Â tag to indicate the owner (e.g.,Â `loader: "Fivetran"`Â orÂ `loader: "Snowpipe"`).
  ### ğŸŒŸ Phase 6 Best Practices
  - **The "One Source" Rule:**Â Never reference a raw Snowflake table (e.g.,Â `RAW_DB.STRIPE.ORDERS`) in more thanÂ **one**dbt model. Reference it once in yourÂ `stg_`Â model, and then everyone else queries theÂ `stg_`Â model. This creates a "Single Point of Failure" which is actually goodâ€”if the source changes, you fix it in one place.
  - **Source Freshness is Critical:**
    - RunningÂ `dbt source freshness`Â should be theÂ **first step**Â in your CI/CD pipeline. If the raw data is old, there is no point in running the expensive transformation models.
  - **Naming Convention (Double Underscore):**
    - UseÂ `stg_[source_system]__[entity].sql`Â (e.g.,Â `stg_sap__users.sql`).
    - *Why?*Â It groups files by source system alphabetically in your file explorer, keeping your project organized.
  - **Docs Blocks:**
    - For long descriptions, do not write them insideÂ `sources.yml`. Use aÂ `docs`Â block in a markdown file and reference it:Â `description: "{{ doc('stripe_orders_table') }}"`.
  ### ğŸ“¦ Phase 6 Deliverables (Checklist)
  1. **ğŸ“„Â `sources.yml`Â File:**Â A fully defined YAML file with freshness blocks and descriptions.
  2. **âœ… Freshness Report:**Â A screenshot of the terminal output after runningÂ `dbt source freshness`Â (showing Green "PASS" status).
  3. **ğŸ•¸ï¸ Lineage Graph:**Â A screenshot fromÂ `dbt docs serve`Â showing the green "Source" node connecting to your blue "Staging" node.
  ### ğŸ› ï¸ Key dbt Commands for this Phase
  - `dbt source freshness`: Checks if data is up to date.
  - `dbt seed`: Loads your CSVs into Snowflake.
  - `dbt run --select tag:staging`: Runs only your staging models.
  - `dbt test --select source:*`: Runs tests defined specifically on your raw sources.

| **Command**                  | **Purpose**                              | **Example**                                      |
| ---------------------------- | ---------------------------------------- | ------------------------------------------------ |
| `dbt source freshness`       | Check if sources are up-to-date          | `dbt source freshness --select source:raw_sales` |
| `dbt run --select source:*`  | Run models that depend on sources        | `dbt run --select source:raw_sales`              |
| `dbt test --select source:*` | Test source data quality                 | `dbt test --select source:raw_sales`             |
| `dbt seed`                   | Load seed files into database            | `dbt seed --select country_codes`                |
| `dbt docs generate`          | Generate documentation including sources | `dbt docs generate`                              |

### Example Staging Model (stg_sales\_\_orders.sql):

```
status_code,status_name,status_description
1,Pending,Order placed but not processed
2,Processing,Order is being prepared
3,Shipped,Order has been shipped
4,Delivered,Order delivered to customer
5,Cancelled,Order was cancelled

```

<aside>

**Pro Tips:**

- Run `dbt source freshness` daily in production to monitor data pipeline health
- Use source tests to catch data quality issues before they reach downstream models
- Keep staging models simple - just light cleaning and renaming, no business logic
- Document sources thoroughly - this becomes your data catalog for the team
- Use `{{ source() }}` function instead of hard-coding table names for lineage tracking
- Set up alerts in dbt Cloud or orchestration tool when source freshness checks fail
</aside>

<aside>

**Common Issues & Solutions:**

- **Source not found:** Verify database/schema names in sources.yml match Snowflake exactly (case-sensitive)
- **Freshness checks failing:** Ensure loaded_at_field is correctly specified and exists in source table
- **Permission errors:** Confirm dbt role has SELECT access on RAW schema and all source tables
- **Large seed files:** If seeds >1MB, load via Snowflake COPY INTO instead and register as source
</aside>

### 4) Build Staging Layer(done)

### ğŸ—ï¸ Phase 7: Building the Staging Layer

> Goal:Â Create a clean, standardized, 1-to-1 mirror of your raw data. This layer prepares data for joining but performsÂ noÂ joins itself.

### **âœ” 7.1 Configuration & Architecture**

- [ ] [ ]Â **Configure Materialization (Global)**
  - *Action:*Â OpenÂ `dbt_project.yml`.
  - *Setting:*Â underÂ `models: -> staging:`, setÂ `+materialized: view`.
  - *Why:*Â Staging models should always be views to ensure fresh data and zero storage costs.
- [ ] [ ]Â **Enforce File Naming Convention**
  - *Pattern:*Â `stg_[source]__[entity].sql`Â (Double underscore separates source from table).
  - *Example:*Â `stg_stripe__payments.sql`,Â `stg_salesforce__accounts.sql`.

### **âœ” 7.2 Model Construction (The SQL Structure)**

- [ ] [ ]Â **Implement Import CTEs**
  - *Action:*Â Start every model with a CTE namedÂ `source`Â that selects fromÂ `{{ source('src', 'tbl') }}`.
  - *Why:*Â Separates dependency definition from logic.
- [ ] [ ]Â **Standardize Column Names**
  - *Action:*Â Rename all columns toÂ `snake_case`.
  - *Action:*Â Prefix ambiguous IDs (e.g., renameÂ `id`Â toÂ `customer_id`).
- [ ] [ ]Â **Cast Data Types**
  - *Action:*Â Explicitly cast every column (e.g.,Â `::boolean`,Â `::timestamp`,Â `::numeric(10,2)`). Do not rely on implicit types.
- [ ] [ ]Â **Generate Surrogate Keys (Crucial Step)**
  - *Action:*Â UseÂ `{{ dbt_utils.generate_surrogate_key(['col_a', 'col_b']) }}`Â to create a unique hash key if the source lacks a reliable Primary Key.

### **âœ” 7.3 Data Cleaning & Standardization**

- [ ] [ ]Â **Handle Null Values**
  - *Action:*Â UseÂ `COALESCE(col, 'Unknown')`Â for text orÂ `COALESCE(col, 0)`Â for measures where appropriate.
- [ ] [ ]Â **Standardize Categorical Data**
  - *Action:*Â UseÂ `CASE WHEN`Â to fix messy values (e.g., Map 'USA', 'U.S.', 'US' -> 'United States').
- [ ] [ ]Â **Format Text**
  - *Action:*Â ApplyÂ `TRIM()`,Â `LOWER()`, orÂ `INITCAP()`Â to string columns to ensure consistency for downstream joins.
- [ ] [ ]Â **Convert Units**
  - *Action:*Â Standardize money to cents (integer) or major currency (decimal), and convert weights/distances to a common metric.

### **âœ” 7.4 Quality Assurance (What NOT to do)**

- [ ] [ ]Â **Verify No Joins**
  - *Check:*Â Ensure there areÂ `0`Â `JOIN`Â statements in this folder.
- [ ] [ ]Â **Verify No Aggregations**
  - *Check:*Â Ensure there areÂ `0`Â `GROUP BY`Â clauses. Granularity must match the source row-for-row.
- [ ] [ ]Â **Verify No Filtering**
  - *Check:*Â Do not useÂ `WHERE`Â clauses to remove business data (e.g., "Active Users"). Only useÂ `WHERE`Â to remove technical garbage (e.g.,Â `_fivetran_deleted = true`Â or completely empty test rows).

### **âœ” 7.5 Testing Strategy (Generic & Singular)**

- [ ] [ ]Â **Apply Generic Tests (Schema.yml)**
  - *Unique:*Â Apply to Primary Key (or Surrogate Key).
  - *Not Null:*Â Apply to PK and critical Foreign Keys.
  - *Accepted Values:*Â Apply to status columns (e.g.,Â `status`Â must be 'placed', 'shipped', 'returned').
- [ ] [ ]Â **Write Singular Tests (Custom Logic)**
  - *Location:*Â CreateÂ `.sql`Â files inÂ `tests/`.
  - *Logic:*Â Write queries that return "Bad Data".
  - *Example:*Â `assert_order_date_before_ship_date.sql`Â ->Â `SELECT * FROM {{ ref('stg_orders') }} WHERE ship_date < order_date`Â (If this returns rows, the test fails).

### **âœ” 7.6 Documentation**

- [ ] [ ]Â **Generate YAML Documentation**
  - *Action:*Â CreateÂ `models/staging/[source]/_schema.yml`.
  - *Content:*Â AddÂ `description:`Â for the table and every single column.
- [ ] [ ]Â **Visualize Lineage**
  - *Action:*Â RunÂ `dbt docs serve`Â to verify the Staging node sits correctly between Source and Marts.

---

### ğŸŒŸ Best Practices & Optimization

- **The "Import CTE" Pattern:**SQL
  - This is the gold standard for readability. It enables "Partition Pruning" in Snowflake if you filter by date in the first CTE.
  ```jsx
  with source as (
      select * from {{ source('stripe', 'charges') }}
  ),
  renamed as (
      select
          id as charge_id,
          amount / 100 as amount_dollars,
          created_at::timestamp as charge_at
      from source
  )
  select * from renamed
  ```
- **Optimization (View Definition):**
  - Since these are Views, Snowflake doesn't store data. However, complex regex or massiveÂ `CASE`Â statements here runÂ *every time*Â you query downstream. Keep logic simple here. Heavy lifting belongs in Intermediate (Ephemeral/Table).
- **Macro Usage:**
  - Don't writeÂ `amount * 100`Â in 5 different models. Write a macroÂ `cents_to_dollars(column_name)`Â and use it. This makes your code DRY (Don't Repeat Yourself).

### ğŸ› ï¸ What You Missed / Corrections

1. **Surrogate Keys:**Â You missed this, but it is vital. If your source system resets IDs or if you combine two systems, you need a new, reliable ID. UseÂ `dbt_utils`.
2. **Outliers:**Â You mentioned "Flag Outliers."Â **Correction:**Â Do not filter them out here. You can add a flag column (e.g.,Â `is_amount_suspicious`), but keep the row. The raw data should be represented faithfully.
3. **Singular Tests:**Â These are powerful. Generic tests (`unique`) checkÂ *structure*. Singular tests checkÂ *business logic*Â (e.g., "Discount cannot be greater than Total Price").

### ğŸ“¦ Phase 7 Deliverables

1. **âœ… Codebase:**Â AÂ `staging`Â folder with consistent SQL files using Import CTEs.
2. **âœ… Test Suite:**Â A populatedÂ `schema.yml`Â and at least 2 Custom Singular Tests in theÂ `tests/`Â folder.
3. **âœ… Documentation:**Â Descriptions for all columns.

[[prompt.md](http://prompt.md) for Data Model](https://www.notion.so/prompt-md-for-Data-Model-2cd1bb84f4a481a39a90e42330c9b517?pvs=21)

### 3) Data Model Design (Star Schema)

**Task:**

Design and implement a star schema (dimensional model) with fact and dimension tables that align with your business questions and KPIs. This transforms your staging data into business-friendly analytics tables.

Example

**Actions Required:**

1. **Design Logical Data Model:** Create conceptual diagram showing entities, relationships, and business rules without technical implementation details
2. **Design Physical Data Model:** Build technical diagram with specific table names, column data types, indexes, constraints, and partitioning strategy
3. **Identify Star Schema Components:**
   - **Fact Tables:** Business processes/events with measurable metrics (e.g., fct_orders, fct_payments, fct_shipments, fct_returns)
   - **Dimension Tables:** Descriptive context for facts (e.g., dim_customers, dim_products, dim_dates, dim_geographies, dim_store_locations)
   - **Fact Table Grain:** Define the most atomic level of detail (e.g., one record per order line item, one record per daily product inventory snapshot)
   - **Grain Validation:** Document and test that every row in fact table represents exactly one instance of the defined grain
4. **Define Key Structure:** Specify primary keys (natural and surrogate), foreign keys linking facts to dimensions, and composite keys where needed
5. Remove Tables and Columns and That does not Answer the business Questions and Requirements
6. Ensure Perfect Naming convention of each columns
7. **Create Surrogate Key Strategy:** Use `dbt_utils.generate_surrogate_key()` for dimension tables to ensure stable, efficient joins
8. **Map Data Transformations:** Document column-level mappings from raw source tables to final model (renaming, type casting, business rules, calculations)
9. **Define Calculated Measures:** Identify metrics to pre-calculate in fact tables (e.g., profit = revenue - cost, discount_amount, margin_percentage)
10. **Implement Slowly Changing Dimensions:**
    - **SCD Type 1:** Overwrite dimension attributes (for corrections or non-historical attributes)
    - **SCD Type 2:** Track historical changes with effective_from/effective_to dates and is_current flag (for auditing and historical analysis)
    - **Choose SCD Strategy:** Decide per dimension based on business requirements and reporting needs
11. **Define Conformed Dimensions:** Identify dimensions shared across multiple fact tables (e.g., dim_dates used by fct_orders, fct_shipments, fct_returns)
12. **Create Date Dimension:** Build comprehensive date table with calendar attributes (year, quarter, month, week, day_of_week, fiscal_period, holidays, is_weekend)
13. **Document Business Rules:** Capture logic for calculated fields, filtering criteria, aggregation rules, and handling of null/missing values
14. **Design Incremental Load Strategy:** For large fact tables, define incremental materialization approach with appropriate unique_key and filters
15. **Plan Indexing Strategy:** Identify columns for clustering keys in Snowflake to optimize query performance (typically date columns and frequently filtered dimensions)
16. **Create Relationship Tests:** Define dbt relationship tests to validate referential integrity between facts and dimensions
17. **Tag Models by Domain:** Apply tags (e.g., sales, finance, operations) to organize models and enable selective runs

### Star Schema Design Pattern:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  dim_customers  â”‚
â”‚  - customer_key â”‚â—„â”€â”€â”€â”
â”‚  - customer_id  â”‚    â”‚
â”‚  - name         â”‚    â”‚
â”‚  - segment      â”‚    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dim_products  â”‚    â”‚    â”‚    dim_dates    â”‚
â”‚  - product_key  â”‚â—„â”€â”€â”€â”¼â”€â”€â”€â–ºâ”‚   - date_key    â”‚
â”‚  - product_id   â”‚    â”‚    â”‚   - date        â”‚
â”‚  - category     â”‚    â”‚    â”‚   - year        â”‚
â”‚  - subcategory  â”‚    â”‚    â”‚   - quarter     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚   - month       â”‚
                       â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚           â–²
                  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                  â”‚    fct_orders       â”‚
                  â”‚  - order_key        â”‚
                  â”‚  - customer_key (FK)â”‚
                  â”‚  - product_key (FK) â”‚
                  â”‚  - date_key (FK)    â”‚
                  â”‚  - quantity         â”‚
                  â”‚  - revenue          â”‚
                  â”‚  - cost             â”‚
                  â”‚  - profit           â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### Naming Conventions:

| **Model Type** | **Prefix** | **Example**           | **Materialization**  |
| -------------- | ---------- | --------------------- | -------------------- |
| Staging        | `stg_`     | `stg_sales__orders`   | view                 |
| Intermediate   | `int_`     | `int_orders_enriched` | view or ephemeral    |
| Fact           | `fct_`     | `fct_orders`          | table or incremental |
| Dimension      | `dim_`     | `dim_customers`       | table                |
| Metrics        | `met_`     | `met_monthly_revenue` | view                 |

### Key dbt Commands:

| **Command**                      | **Purpose**                                   | **Example**                          |
| -------------------------------- | --------------------------------------------- | ------------------------------------ |
| `dbt run --select +fct_orders`   | Run fact table and all upstream dependencies  | Builds entire lineage for fact table |
| `dbt run --select dim_*`         | Run all dimension models                      | Build all dimensions first           |
| `dbt test --select fct_orders`   | Test fact table relationships and constraints | Validate data quality                |
| `dbt run --select tag:dimension` | Run all models tagged as dimensions           | Build by model type                  |
| `dbt docs generate`              | Generate documentation with lineage graph     | Visualize star schema                |

<aside>

**Star Schema Design Best Practices:**

- **Define clear grain:** Each fact table should have one clearly defined level of detail (e.g., one row per order line item)
- **Use surrogate keys:** Generate surrogate keys for dimensions using `dbt_utils.generate_surrogate_key()` for better performance
- **Denormalize dimensions:** Include descriptive attributes in dimensions rather than normalizing (star schema, not snowflake)
- **Pre-calculate measures:** Include calculated measures in fact tables (e.g., profit = revenue - cost) for reporting efficiency
- **Build conformed dimensions:** Reuse dimensions across multiple fact tables when possible (e.g., shared date dimension)
- **Document relationships:** Use `relationships` tests to validate foreign keys and document lineage
- **Implement SCD for changing dimensions:** Use Type 2 SCD for tracking historical changes in important dimensions
</aside>

<aside>

**Common Star Schema Patterns:**

- **Transaction Facts:** One row per business event (fct_orders, fct_payments, fct_shipments)
- **Periodic Snapshot Facts:** Regular snapshots of measurements (fct_daily_inventory, fct_monthly_balances)
- **Accumulating Snapshot Facts:** Track lifecycle of processes (fct_order_fulfillment with multiple date keys for order, ship, delivery)
- **Factless Fact Tables:** Track events without measures (fct_student_attendance, fct_promotion_coverage)
- **Role-Playing Dimensions:** Same dimension used multiple times with different meanings (dim_dates as order_date, ship_date, delivery_date)
</aside>

<aside>

**Integration with Power BI:**

- Star schema in dbt translates perfectly to Power BI's semantic model structure
- Fact and dimension tables become tables in Power BI with automatic relationship detection
- Pre-calculated measures in facts become implicit measures; explicit DAX measures built on top
- Date dimension becomes basis for time intelligence functions in Power BI
- Surrogate keys enable efficient relationships and better query performance
- Model documentation from dbt helps Power BI developers understand table purposes and relationships
</aside>

### 5) Build Intermediate Layer(done)

> Goal:Â Transform cleaned staging data into business-logic-rich models. Join tables, calculate metrics, and handle grain shifts (aggregation/fan-out) without yet creating the final presentation layer.

### **âœ” 8.1 Configuration & Materialization Strategy**

- [ ] [ ]Â **Define Materialization Default**
  - *Action:*Â InÂ `dbt_project.yml`, setÂ `models: -> intermediate:`Â toÂ `+materialized: ephemeral`.
  - *Why:*Â `ephemeral`Â acts like a CTE (snippet) injected into downstream models. It keeps your Snowflake storage clean.
- [ ] [ ]Â **Identify Performance Bottlenecks (Override to Table)**
  - *Action:*Â If a specific model involves massive joins or is referenced by 5+ downstream models, change configÂ *at the file level*Â toÂ `{{ config(materialized='table') }}`.
  - *Why:*Â Compute it once and save it, rather than re-computing it every time a Mart runs.
- [ ] [ ]Â **Enforce Naming Conventions**
  - *Pattern:*Â `int_[primary_entity]__[action_or_concept].sql`
  - _Examples:_
    - `int_orders__joined_to_payments.sql`Â (Integration)
    - `int_customers__segmentation_rules.sql`Â (Logic application)
    - `int_web_events__sessionized.sql`Â (Window functions)

### **âœ” 8.2 Structural Transformations (Joins & Shapes)**

- [ ] [ ]Â **Join Staging Models**
  - *Action:*Â Combine related staging tables (e.g.,Â `stg_orders`Â +Â `stg_payments`) using standard SQLÂ `JOINs`.
- [ ] [ ]Â **Handle Granularity (Fan-Out/Fan-In)**
  - *Check:*Â explicitly document the "Grain" of the model (e.g., "One row per Order" vs "One row per Order Item").
  - *Action:*Â UseÂ `GROUP BY`Â to aggregate child rows (e.g., payment attempts) back to the parent grain (order) if necessary.
- [ ] [ ]Â **Reshape Data**
  - *Pivot:*Â Convert rows to columns (e.g.,Â `payment_method`Â rows ->Â `amount_usd_credit_card`,Â `amount_usd_paypal`Â columns).
  - *Unpivot:*Â Convert wide columns back to rows if normalizing data.
- [ ] [ ]Â **Deduplicate Records**
  - *Action:*Â UseÂ `QUALIFY ROW_NUMBER() OVER (...) = 1`Â to remove duplicates based on strict business rules.

### **âœ” 8.3 Business Logic & Calculations**

- [ ] [ ]Â **Apply Conditional Logic**
  - *Action:*Â UseÂ `CASE WHEN`Â statements to create buckets (e.g.,Â `Customer Segment`,Â `Order Priority`).
- [ ] [ ]Â **Create Calculated Fields**
  - *Action:*Â Compute business metrics (e.g.,Â `margin = revenue - cost`,Â `days_to_ship = shipped_at - ordered_at`).
- [ ] [ ]Â **Apply Window Functions**
  - *Action:*Â Calculate "Running Totals," "Rankings," or "Previous Value" (`LAG`/`LEAD`) to analyze trends across rows.
- [ ] [ ]Â **Filter Business Rows**
  - *Action:*Â ApplyÂ `WHERE`Â clauses to remove records that are technically valid but business-irrelevant (e.g., filtering out internal test accounts).

### **âœ” 8.4 Testing Strategy (Generic & Singular)**

- [ ] [ ]Â **Apply Structural Tests (`schema.yml`)**
  - *Unique:*Â Verify the Primary Key (composite or single) is unique.
  - *Not Null:*Â Ensure keys and critical metrics are populated.
  - *Relationships:*Â Validate Foreign Keys (e.g., ensureÂ `customer_id`Â exists inÂ `stg_customers`).
- [ ] [ ]Â **Apply Business Rule Tests (dbt_utils)**
  - *Accepted Values:*Â Check categorical fields (e.g.,Â `status`Â IN ('active', 'churned')).
  - *Expression is True:*Â Validate logic (e.g.,Â `total_revenue >= 0`,Â `end_date >= start_date`).
  - *Not Null Proportion:*Â Ensure critical columns aren't mostly empty.
- [ ] [ ]Â **Write Singular Tests (Custom SQL)**
  - *Action:*Â CreateÂ `.sql`Â files inÂ `tests/`Â for complex logic that generic tests can't catch.
  - *Example:*Â "A customer cannot be 'Active' if their last login was > 1 year ago."

### **âœ” 8.5 Documentation & Lineage**

- [ ] [ ]Â **Document Logic (`schema.yml`)**
  - *Action:*Â AddÂ `description:`Â to the model explainingÂ *what*Â logic was applied (e.g., "Joins orders to payments and pivots payment methods").
- [ ] [ ]Â **Verify Lineage Graph**
  - *Action:*Â RunÂ `dbt docs serve`Â and check the graph.
  - *Check:*Â Ensure Staging models flow into Intermediate, and Intermediate flows into Marts (no skipping layers!).

---

### â“ Should I include Singular Tests in the Intermediate Layer?

**YES. Absolutely.**

The Intermediate Layer is exactly whereÂ **Business Logic**Â is applied. Therefore, it is theÂ *best*Â place to test that logic.

- **Generic Tests**Â (Unique, Not Null) checkÂ *Data Integrity*.
- **Singular Tests**Â (Custom SQL) checkÂ *Business Logic*.

**Example:**Â If you write logic in an intermediate model to calculateÂ `days_to_ship`, you should write aÂ **Singular Test**Â to ensureÂ `ship_date`Â is never earlier thanÂ `order_date`.

- *File:*Â `tests/assert_ship_date_after_order_date.sql`
- *Query:*Â `SELECT * FROM {{ ref('int_orders') }} WHERE ship_date < order_date`
- If this returns rows, your intermediate logic is flawed.

---

### ğŸŒŸ Best Practices & Pitfalls

**Best Practices:**

- **Use Ephemeral Materialization:**Â Default toÂ `ephemeral`Â to keep your Snowflake "Views" list clean. Think of Intermediate models as reusable code blocks, not necessarily physical tables.
- **Define the Grain:**Â Before writingÂ `SELECT`, write a comment at the top:Â `- Grain: One row per Customer`. Stick to it.
- **Modularize Complex Joins:**Â If you need to join 5 tables, don't do it in one massive query. Join A to B (`int_a_b`), Join C to D (`int_c_d`), then join the results.

**What NOT to do:**

- **âŒ No Final Presentation:**Â Don't rename columns to "pretty" names (e.g., "Customer Name"). Keep them snake_case (`customer_name`). "Pretty" names belong in Power BI.
- **âŒ No Star Schema Keys:**Â Don't generate the finalÂ `dim_customer_key`Â here. Do that in the Marts.
- **âŒ Don't Drop Columns Prematurely:**Â Unless a column is massive PII, keep it. You don't know if the Mart layer will need it later.

### 6) Build Marts Layer(done)

> Goal:Â Transform complex business logic into a governed, performance-optimizedÂ Star SchemaÂ ready for Power BI. This layer is the "Product" you deliver to stakeholders.

### **âœ” 9.1 Pre-Work & Planning**

- [ ] [ ]Â **Create Engineering Ticket**
  - *Action:*Â Create a GitHub Issue/Jira Ticket outlining the business requirements and KPIs.
  - *Deliverable:*Â A 1-page design doc linked in the ticket.
- [ ] [ ]Â **Define Branching Strategy**
  - *Action:*Â Create a new branch:Â `feature/marts-[domain]-[description]`Â (e.g.,Â `feature/marts-finance-revenue`).
- [ ] [ ]Â **Folder Architecture**
  - *Standard:*Â Organize strictly by domain:
    - `models/marts/core/`Â (Shared dimensions like Date, Customers).
    - `models/marts/finance/`Â (Revenue, Costs).
    - `models/marts/marketing/`Â (Campaigns, Leads).

### **âœ” 9.2 The "Backbone" Structural Tables**

- [ ] [ ]Â **Build Date Dimension (`dim_date`)**
  - *Location:*Â `models/marts/core/dim_date.sql`
  - *Source:*Â UseÂ `dbt_utils.date_spine`Â macro.
  - *Logic:*Â Generate dates fromÂ `2015-01-01`Â toÂ `2035-01-01`.
  - *Attributes:*Â `date_key`Â (YYYYMMDD),Â `year`,Â `quarter`,Â `month_name`,Â `is_weekend`,Â `fiscal_year`.
  - *Materialization:*Â `table`.
- [ ] [ ]Â **Build Bridge Tables (If needed)**
  - *Location:*Â `models/marts/core/bridge_[entity_a]_[entity_b].sql`
  - *Logic:*Â Handle Many-to-Many relationships (e.g., Orders <-> Tags).
  - *Columns:*Â `entity_a_key`,Â `entity_b_key`,Â `allocation_weight`Â (if applicable).
  - *Tests:*Â Unique constraint on theÂ *composite*Â key (A + B).

### **âœ” 9.3 Building Dimensions (`dim_`)**

- [ ] [ ]Â **Standardize Dimension Structure**
  - *Primary Key:*Â `[entity]_key`Â (Surrogate Key, MD5 hash).
  - *Natural Key:*Â `[entity]_id`Â (Source System ID).
  - *Attributes:*Â Descriptive columns (Name, Status, Category).
- [ ] [ ]Â **Handle Nulls & Unknowns**
  - *Action:*Â UseÂ `COALESCE(col, 'N/A')`Â orÂ `'-1'`Â for keys. Power BI needs clean relationships.
- [ ] [ ]Â **Configure Materialization**SQL
  - _Config:_
    `{{ config(
    materialized = 'table',
    schema       = 'marts',
    alias        = 'dim_customers',
    tags         = ['marts', 'core']
) }}`

### **âœ” 9.4 Building Fact Tables (`fct_`)**

- [ ] [ ]Â **Define Grain**
  - *Rule:*Â One row per [Event] (e.g., Line Item, Click, Transaction).
- [ ] [ ]Â **Select Measures**
  - *Action:*Â Bring in numeric columns (`revenue`,Â `quantity`,Â `duration_seconds`).
  - *Monetary:*Â Convert all amounts to cents/base units OR standard decimals. Be consistent.
- [ ] [ ]Â **Bring in Foreign Keys**
  - *Action:*Â Ensure everyÂ `_key`Â column matches aÂ `dim_`Â table.
- [ ] [ ]Â **Add Degenerate Dimensions**
  - *Action:*Â KeepÂ `order_id`Â orÂ `invoice_number`Â for auditing.

### **âœ” 9.5 Optimization: Incremental Materialization**

- [ ] [ ]Â **Configure Incremental Strategy**SQL
  - *Target:*Â Apply only to large Fact tables (e.g.,Â `fct_orders`).
  - _Config:_
    `{{ config(
    materialized='incremental',
    unique_key='order_id',
    on_schema_change='fail'
) }}`
- [ ] [ ]Â **Implement Filter Logic**SQL
  - *Action:*Â AddÂ `is_incremental()`Â macro at the end of the query.
    `{% if is_incremental() %}
  where updated_at > (select max(updated_at) from {{ this }})
{% endif %}`
- [ ] [ ]Â **SCD Type 2 (History)**
  - *Optional:*Â If tracking history, ensureÂ `valid_from`Â andÂ `valid_to`Â columns are preserved.

### **âœ” 9.6 Documentation & Metadata**

- [ ] [ ]Â **Write Descriptions**
  - *Rule:*Â Every model must have a description (2-4 sentences).
  - *Rule:*Â Every column inÂ `schema.yml`Â must have a description.
- [ ] [ ]Â **Use Doc Blocks**
  - *Action:*Â UseÂ `{{ doc("business_term") }}`Â for shared definitions (e.g., "Revenue").
- [ ] [ ]Â **Power BI Specifics**
  - *Action:*Â AddÂ `synonyms`Â in YAML to help Power BI Q&A find columns.

### **âœ” 9.7 Testing Strategy (The Safety Net)**

- [ ] [ ]Â **Configure Singular Tests (Business Logic)**
  - *Dates:*Â `assert_ship_after_order.sql`Â (`ship_date >= order_date`).
  - *Monetary:*Â `assert_positive_revenue.sql`Â (`revenue >= 0`).
  - *Relationships:*Â `assert_bridge_integrity.sql`Â (Bridge keys exist in Dims).
- [ ] [ ]Â **Configure Generic Tests (`schema.yml`)**
  - *Keys:*Â `unique`,Â `not_null`.
  - *Foreign Keys:*Â `relationships`.
  - *Cardinality:*Â `dbt_expectations.expect_table_row_count_to_be_between`.
  - *Marts Rules:*Â `dbt_project_evaluator.no_direct_model_references_from_marts`.
  ### **âœ” 9.8 Data Governance (Contracts)**
  - [ ] [ ]Â **Select Critical Models**
    - *Target:*Â Identify high-value Mart models (e.g.,Â `dim_customers`,Â `fct_orders`) that serve Power BI.
    - *Reason:*Â Enforcing contracts here prevents breaking changes from reaching dashboards.
  - [ ] [ ]Â **Define Contract Configuration**
    - *Action:*Â InÂ `models/marts/[domain]/schema.yml`, add theÂ `contract`Â config block to the model definition.
    - *Code:*YAML
      ```jsx
      config: contract: enforced: true
      ```
  - [ ] [ ]Â **Define Explicit Column Types**
    - *Action:*Â For every column in the contracted model, specify the exact data type.
    - *Code:*YAML
      ```jsx
      columns:
        - name: customer_id
          data_type: INT
        - name: email
          data_type: STRING
      ```
  - [ ] [ ]Â **Define Constraints**
    - *Action:*Â Add constraints to critical columns to enforce data integrity at the database level (if supported by Snowflake/dbt adapter).
    - *Code:*YAML
      ```jsx
          constraints:
            - type: not_null
            - type: primary_key
      ```
  - [ ] [ ]Â **Validate Contract Enforcement**
    - *Action:*Â RunÂ `dbt build --select [model_name]`Â to verify that the model builds successfully with the contract enabled.
    - *Test:*Â Try changing a column type in the SQL model (e.g., cast an INT to STRING) and verify thatÂ `dbt run`Â fails, proving the contract is working.
  ***

### **âœ” 9.8 Power BI Preparation Tasks**

- [ ] [ ]Â **Grant Access**
  - *Action:*Â `GRANT SELECT ON SCHEMA MARTS TO ROLE REPORTER_ROLE;`Â (Automate via Post-Hook or dbt grants).
- [ ] [ ]Â **Hide Technical Columns**
  - *Action:*Â Prefix Surrogate Keys or System columns withÂ `_`Â or list them in a "Hidden" meta-field for the BI developer.

### **âœ” 9.9 Final Review & Deployment**

- [ ] [ ]Â **Local Validation**
  - *Command:*Â `dbt build --select tag:marts`Â (Runs models + tests).
  - *Check:*Â All tests pass?
- [ ] [ ]Â **Documentation Check**
  - *Command:*Â `dbt docs generate`.
  - *Check:*Â Does the Lineage Graph look like a clean flow (Staging -> Int -> Marts)?
- [ ] [ ]Â **Pull Request**
  - *Action:*Â Open PR. Paste test results and lineage screenshot.
  - *Merge:*Â Merge toÂ `develop`Â (CI Run) -> Merge toÂ `main`Â (Prod Run).

---

### ğŸŒŸ Phase 9 Best Practices

- **NoÂ `SELECT *`:**Â strictly forbidden in Marts. Explicitly name every column. If a source column changes, your Mart should fail loudly, not silently succeed with broken data.
- **Separation of Duties:**
  - **Staging:**Â Cleaning & Standardizing.
  - **Intermediate:**Â Joins & Logic.
  - **Marts:**Â Selection & Renaming for End Users.
  - *Rule:*Â Never joinÂ `stg_`Â tables directly in aÂ `mart_`. Always go throughÂ `int_`Â if there is ANY logic involved.
- **Query Acceleration:**
  - For massive tables in Snowflake, consider enablingÂ **Auto-Clustering**Â orÂ **Search Optimization Service**Â if performance lags (Advanced).
- **Cost Control:**
  - UseÂ `incremental`Â strategies aggressively.
  - UseÂ `transient`Â tables for Intermediate models if they don't need Fail-safe retention.

### ğŸ“¦ Phase 9 Deliverables

1. **âœ… Star Schema Code:**Â Clean SQL files for Facts, Dims, and Bridge tables.
2. **âœ… Date Dimension:**Â A populatedÂ `dim_date`Â table.
3. **âœ… Test Suite:**Â A robustÂ `schema.yml`Â with generic and singular tests.
4. **âœ… Incremental Logic:**Â ProvenÂ `is_incremental()`Â blocks in Fact tables.
5. **âœ… Documentation Site:**Â A hosted static site showing the full data dictionary.

### 7) Semantic Layer & Metrics(done)

> Goal:Â Codify business logic into a centralized semantic layer so that "Revenue" means the exact same thing in dbt, the CLI, and Power BI.

### **âœ” 13.1 Environment & Project Configuration**

- [ ] [ ]Â **Enable Semantic Layer**
  - *Action:*Â Ensure you have a dbt Cloud account (Team/Enterprise) OR are setting up MetricFlow locally for development.
- [ ] [ ]Â **Install MetricFlow Package**
  - *Action:*Â VerifyÂ `dbt-metricflow`Â is installed/compatible with your dbt Core version.
- [ ] [ ]Â **Create Directory Structure**
  - *Action:*Â CreateÂ `models/semantic_models/`Â (for mapping tables).
  - *Action:*Â CreateÂ `models/metrics/`Â (for business calculations).

### **âœ” 13.2 Define Semantic Models (The Mapping Layer)**

- [ ] [ ]Â **Create Semantic Model YAMLs**
  - *Location:*Â `models/semantic_models/[entity].yml`.
  - *Action:*Â define theÂ `model:`Â block referencing a specific Mart table (`ref('fct_orders')`).
- [ ] [ ]Â **Define Entities (Keys)**
  - *Action:*Â Map Primary and Foreign keys (e.g.,Â `entity: order_id`,Â `type: primary`).
  - *Action:*Â Define relationships to other semantic models (e.g.,Â `orders`Â links toÂ `customers`).
- [ ] [ ]Â **Define Dimensions (Attributes)**
  - *Action:*Â Map time dimensions (e.g.,Â `order_date`) and set time granularity.
  - *Action:*Â Map categorical dimensions (e.g.,Â `status`,Â `region`) for slicing.
- [ ] [ ]Â **Define Measures (Raw Aggregations)**
  - *Concept:*Â These are the building blocks, NOT the final metrics.
  - *Action:*Â DefineÂ `measures:`Â block (e.g.,Â `name: orders_total`,Â `agg: sum`,Â `expr: amount`).

### **âœ” 13.3 Define Business Metrics (The Logic Layer)**

- [ ] [ ]Â **Create Metrics YAML**
  - *Location:*Â `models/metrics/[business_area].yml`Â (e.g.,Â `sales_metrics.yml`).
- [ ] [ ]Â **Define Simple Metrics**
  - *Type:*Â `simple`
  - *Action:*Â Reference the measures defined in 13.2 (e.g.,Â `label: Total Revenue`,Â `type: simple`,Â `measure: orders_total`).
- [ ] [ ]Â **Define Ratio Metrics**
  - *Type:*Â `ratio`
  - *Action:*Â Define numerator and denominator metrics (e.g.,Â `Average Order Value`Â =Â `Revenue`Â /Â `Order Count`).
- [ ] [ ]Â **Define Derived Metrics**
  - *Type:*Â `derived`
  - *Action:*Â Use expressions to combine metrics (e.g.,Â `Profit`Â =Â `Revenue - Cost`).
- [ ] [ ]Â **Define Cumulative Metrics**
  - *Type:*Â `cumulative`
  - *Action:*Â Set window settings (e.g.,Â `Rolling 7 Day Revenue`).

### **âœ” 13.4 Testing & Validation (CLI)**

- [ ] [ ]Â **Validate Configuration**
  - *Command:*Â RunÂ `dbt parse`Â to ensure YAML syntax is correct.
- [ ] [ ]Â **Query Metrics via CLI**
  - *Command:*Â RunÂ `dbt sl query --metrics total_revenue --group-by metric_time`.
  - *Check:*Â Does the number returned match your SQL query in Snowflake?
- [ ] [ ]Â **Test Dimension Slicing**
  - *Command:*Â RunÂ `dbt sl query --metrics total_revenue --group-by status`.
  - *Check:*Â Do the categories match the raw data?

### **âœ” 13.5 Documentation & Governance**

- [ ] [ ]Â **Add Business Descriptions**
  - *Action:*Â In the YAML, addÂ `description:`Â fields for every metric. This is what the CEO reads.
  - *Action:*Â AddÂ `label:`Â fields for pretty printing (e.g.,Â `label: "Gross Margin %"`).
- [ ] [ ]Â **Tag Metrics**
  - *Action:*Â Use tags (e.g.,Â `tags: ['financial', 'kpi']`) to organize metrics by department.

### **âœ” 13.6 Power BI Integration Strategy**

- [ ] [ ]Â **Establish "Single Source" Policy**
  - *Action:*Â Decide which metrics live in dbt vs. Power BI.
  - *Best Practice:*Â "Base" metrics (Sum, Count) live in dbt. "Visual" metrics (Time Intelligence, Dynamic Formatting) often live in Power BI DAX but reference the dbt base.
- [ ] [ ]Â **Align DAX Measures**
  - *Action:*Â Create DAX measures in Power BI that strictly reference the columns generated by the Semantic Layer logic.
  - *Action:*Â Copy the description from dbt into the Power BI measure description property.

---

### ğŸŒŸ Phase 13 Best Practices

- **Measures vs. Metrics:**
  - **Measure:**Â `SUM(amount)`Â inside a specific table. (Technical).
  - **Metric:**Â "Revenue". A globally accessible concept that might join Orders and Refunds. (Business).
  - *Rule:*Â Never expose "Measures" to business users. Only expose "Metrics".
- **Don't Over-Engineer:**
  - If a calculation is extremely specific to one visual (e.g., "Top 3 Products by Region colored by Profit"), do that in Power BI DAX.
  - If a calculation is a company-wide KPI (e.g., "Churn Rate"), do it in dbt Semantic Layer.
- **MetricFlow Joins:**
  - MetricFlow handles joins automatically if you define yourÂ **Entities**Â correctly. Ensure your Primary and Foreign keys are strictly defined inÂ `semantic_models`Â so dbt knows how to joinÂ `Orders`Â toÂ `Customers`Â automatically.

### ğŸ“¦ Phase 13 Deliverables

1. **ğŸ“„ Semantic YAMLs:**Â PopulatedÂ `models/semantic_models/`Â folder mapping your Marts.
2. **ğŸ“„ Metrics YAMLs:**Â PopulatedÂ `models/metrics/`Â folder with Ratio and Derived metrics.
3. **âœ… CLI Validation:**Â A screenshot of a successfulÂ `dbt sl query`Â outputting a data table.
4. **ğŸ”— Power BI Matrix:**Â A simple table in Notion mapping "dbt Metric Name" to "Power BI Measure Name" to prove alignment.

**Common Metrics by Business Area:**

| **Business Area** | **Metric Examples**                                            | **Metric Type**   |
| ----------------- | -------------------------------------------------------------- | ----------------- |
| Sales             | total_revenue, order_count, average_order_value                | Simple, Ratio     |
| Finance           | gross_profit, profit_margin, revenue_growth_rate               | Derived, Ratio    |
| Marketing         | customer_acquisition_cost, conversion_rate, return_on_ad_spend | Ratio, Derived    |
| Customer          | customer_lifetime_value, churn_rate, active_customers          | Cumulative, Ratio |
| Operations        | fulfillment_time, inventory_turnover, order_accuracy_rate      | Simple, Ratio     |

**Git Workflow:**

- Create branch: `git checkout -b feature/semantic-metrics`
- Add metrics: `git add models/metrics/ &amp;&amp; git commit -m "Add sales and finance metrics"`
- Test metrics: `dbt sl query --metrics revenue,profit --group-by date`
- Push changes: `git push origin feature/semantic-metrics`

### 8) Freshness Validation

### 8) **Documentation & Testing**

**TheÂ `dbt-project-evaluator`Â Package:**Â This is a package built by dbt Labs that automatically checks if you are following best practices (e.g., "Are all models documented?", "Are you joining in staging?").

- **Add to To-Do:**Â "RunÂ `dbt build --select package:dbt_project_evaluator`Â to audit project structure."

**Exposures (For Lineage):**Â You didn't mentionÂ **Exposures**. This allows you to tell dbt "This model is used in a Power BI Dashboard." It adds a node to the lineage graph representing the dashboard.

- **Add to To-Do:**Â "DefineÂ `exposures.yml`Â to link Marts to the downstream Power BI Dashboard."
- Generate comprehensive dbt documentation using `dbt docs generate`
- Review and enhance model descriptions in schema.yml files
- Document column-level descriptions for all models
- Add data lineage diagrams showing source-to-mart flow
- Create data dictionary with business definitions
- Write comprehensive tests for data quality and business logic
- Set up dbt test coverage reports
- Document testing strategy and acceptance criteria
- Create runbook for troubleshooting common issues
- Review all Git commits and ensure clear commit history

### 9) Optimization and Best Practices

Phase 3 is where the magic happens. You are turning raw, messy data into clean, valuable insights. This is the core of "Analytics Engineering."

To impress hiring managers, you shouldn't just write SQL queries; you need to structure your project forÂ **scalability, readability, and performance**.

Here is a guide to Optimization and Best Practices for implementing Phase 3 with dbt and Snowflake.

### 1. Architectural Optimization: The "Layer Cake" Approach

Do not write one massive SQL query that goes from raw data to the final dashboard. Break it down into modular layers. This is the industry standard.

### **Layer A: Staging (`models/staging`)**

- **Goal:**Â Clean 1-to-1 representation of your source data.
- **Best Practices:**
  - **Materialization:**Â Configure asÂ `view`Â (saves storage costs in Snowflake).
  - **Renaming:**Â Rename obscure column names (e.g.,Â `CUST_ID_01`) to friendly names (`customer_id`).
  - **Casting:**Â Fix data types here (e.g., cast strings to dates).
  - **No Joins:**Â strictly avoid joining tables in staging.
- **Snowflake Optimization:**Â Because these are views, Snowflake computes them only when queried. This keeps your storage footprint low.

### **Layer B: Intermediate (`models/intermediate`)**

- **Goal:**Â Isolate complex logic and joins.
- **Best Practices:**
  - **Materialization:**Â `ephemeral`Â (acts like a CTE, doesn't create an object in Snowflake) orÂ `table`Â (if the logic is heavy and reused often).
  - **Business Logic:**Â Perform your currency conversions, heavy joins, and complex aggregations here.
  - **Reusability:**Â If you need to calculate "User Churn" for three different dashboards, build the logic once here, then reference it everywhere.

### **Layer C: Marts (`models/marts`)**

- **Goal:**Â The final tables ready for Power BI.
- **Best Practices:**
  - **Materialization:**Â `table`Â orÂ `incremental`. This ensures Power BI reads from a pre-built table, making the dashboard fast.
  - **Star Schema:**Â Structure these as Fact tables (measurements, events) and Dimension tables (attributes, people, products).
  - **Hiding:**Â Hide helper columns (like surrogate keys) that the end-user doesn't need.

---

### 2. Coding Best Practices: Writing "Clean" dbt SQL

Readable code is maintainable code. Adoption of a style guide is a strong signal of seniority.

- **Use CTEs (Common Table Expressions):**Â Start every model by importing your data using CTEs at the top, then your logic, then a final select.
  - *Why?*Â It makes debugging in Snowflake incredibly easy. You can just highlight the logic part and run it.
- **TheÂ `ref()`Â Function is Mandatory:**Â Never hardcode a table name (e.g.,Â `FROM RAW_DB.PUBLIC.ORDERS`). Always useÂ `FROM {{ ref('stg_orders') }}`.
  - *Why?*Â This allows dbt to build the dependency graph (DAG) and run models in the correct order.
- **Lowercase Everything:**Â SQL keywords can be capitalized, but table/column names should be lowercase to avoid Snowflake's case-sensitivity headaches (quoting identifiers).

**Example of a Clean Model Structure:**

SQL

```jsx
/* models/marts/fct_orders.sql */

with orders as (
    select * from {{ ref('stg_orders') }}
),

payments as (
    select * from {{ ref('stg_payments') }}
),

final as (
    select
        orders.order_id,
        orders.customer_id,
        orders.order_date,
        payments.amount
    from orders
    left join payments using (order_id)
)

select * from final
```

---

### 3. Snowflake Optimization Strategies

Snowflake charges by credit (compute) and storage. Optimizing your dbt project saves money and time.

- **Use Incremental Models for Large Tables:**
  - *Scenario:*Â You have a transaction table with 10 million rows.
  - *Bad Practice:*Â Dropping and recreating the table every day (`materialized='table'`).
  - *Optimization:*Â UseÂ `materialized='incremental'`. This tells dbt to only processÂ *new*Â rows added since the last run.
  - *How:*SQL
    ```jsx
    {{ config(materialized='incremental', unique_key='transaction_id') }}
    select * from {{ ref('stg_transactions') }}
    {% if is_incremental() %}
      where transaction_date > (select max(transaction_date) from {{ this }})
    {% endif %}
    ```
- **Separate Warehouses (If scaling up):**
  - In a real company, you use a "Loading Warehouse" for Fivetran/ingestion and a separate "Transforming Warehouse" for dbt.
  - *Portfolio Tip:*Â For your project, stick to anÂ **X-Small**Â warehouse. It is powerful enough for almost any portfolio dataset and is the cheapest option.
- **Snowflake Zero-Copy Cloning (The "Killer Feature"):**
  - Mention in your documentation that for testing, you utilize Snowflake's "Zero-Copy Cloning" to create instant copies of your production database for development without doubling storage costs.

---

### 4. DataOps Implementation in Phase 3

This is how you ensure reliability.

- **Testing is NOT Optional:**
  - In yourÂ `schema.yml`Â files, every primary key must have:YAML
    - `name: customer_id tests: - unique - not_null`
  - *Optimization:*Â Add testsÂ *upstream*Â (in Staging). If data is bad at the source, stop the pipeline before it breaks your dashboard.
- **Documentation as Code:**
  - Use theÂ `description:`Â field in yourÂ `.yml`Â files.
  - *Best Practice:*Â Do not describeÂ *what*Â the column is (e.g., "This is the created_at column"). Describe theÂ *context*(e.g., "The UTC timestamp when the user clicked the checkout button. Nulls indicate abandoned carts.").
- **DRY (Don't Repeat Yourself) with Macros:**
  - If you find yourself writing the same SQL snippet 3 times (e.g., converting cents to dollars, or specific date formatting), turn it into aÂ **Jinja Macro**.
  - *Portfolio Win:*Â Creating a custom macro demonstrates advanced dbt usage.

---

### 5. Summary Checklist for Phase 3

| **Area**        | **Best Practice**                | **Why it matters**                              |
| --------------- | -------------------------------- | ----------------------------------------------- |
| **Structure**   | Staging -> Intermediate -> Marts | Keeps logic modular and organized.              |
| **Coding**      | Import CTEs at the top           | Makes debugging significantly faster.           |
| **Coding**      | UseÂ `{{ ref() }}`Â everywhere     | Builds the lineage graph correctly.             |
| **Performance** | Incremental Models               | Drastically reduces run time and compute costs. |
| **Reliability** | `unique`Â andÂ `not_null`Â tests    | Prevents bad data from reaching Power BI.       |
| **Context**     | `description`Â fields in YAML     | Ensures the data is understandable by others.   |

### Next Step for You

Would you like me to provide aÂ **template for aÂ `schema.yml`Â file**Â or aÂ **sample code block for a custom dbt Macro**Â (like a currency converter) that you can copy directly into your project?

##

[Phase 3: Delivarables](https://www.notion.so/Phase-3-Delivarables-2cd1bb84f4a481a4894ef03a5a24e4cb?pvs=21)

[What to ask AI ](https://www.notion.so/What-to-ask-AI-2cd1bb84f4a4814186bbf45f09ea6a06?pvs=21)

# **ğŸš€Â Phase 4: Deployment, Orchestration & DataOps**

### **Objectives**

1. Establish a strictÂ **"Gated Promotion Protocol"**Â to move code from Dev (`ANALYTICS_DEV`) to Prod (`ANALYTICS_PROD`).
2. Ensure Production data is protected from "work in progress" code.
3. Document the Standard Operating Procedure (SOP) for updating the pipeline so the process is repeatable and audit-ready.

### **AI Workflow**

### **Custom Chat Modes**

**8ï¸âƒ£ Mode Name:Â `AE_Ops_Manager`**

### **1) The Deployment Protocol (SOP)**

> Goal:Â A repeatable checklist to ensure you never accidentally break Production. This acts as your "Human CI/CD."

**âœ” 14.1 Pre-Deployment Validation (The "Gate")**

- [ ] [ ]Â **Code Freeze**
  - *Action:*Â Ensure all Feature Branches are merged intoÂ `main`. No open PRs should be pending for this release.
- [ ] [ ]Â **Clean State**
  - *Action:*Â RunÂ `dbt clean`Â locally to remove old artifacts/compiled SQL.
- [ ] [ ]Â **Local Test Run**
  - *Action:*Â RunÂ `dbt build --target dev`Â one last time to ensure 0 errors on your machine.

**âœ” 14.2 Execution (The "Push")**

- [ ] [ ]Â **Freshness Check**
  - *Command:*Â `dbt source freshness`
  - *Check:*Â If this fails,Â **STOP**. Do not deploy stale data to production.
- [ ] [ ]Â **Switch Target**
  - *Command:*Â `dbt build --target prod`
  - *Note:*Â This explicitly targets theÂ `ANALYTICS_PROD`Â warehouse defined in yourÂ `profiles.yml`.
- [ ] [ ]Â **Verify Artifacts**
  - *Action:*Â Open Snowflake. QueryÂ `ANALYTICS_PROD.MARTS.FCT_ORDERS`.
  - *Check:*Â Is theÂ `_loaded_at`Â timestamp current? Are row counts consistent with Dev?

**âœ” 14.3 Post-Deployment Audit**

- [ ] [ ]Â **Documentation Re-Build**
  - *Command:*Â `dbt docs generate`
  - *Action:*Â Commit the newÂ `target/`Â files to yourÂ `gh-pages`Â branch (if hosting docs) or save them locally.

### **2) Power BI Refresh Strategy**

> Goal:Â Ensure the Dashboard sees the new Production data immediately.

**âœ” 15.1 Dataset Refresh**

- [ ] [ ]Â **Manual Trigger**
  - *Action:*Â Go to Power BI Service -> Workspace -> Dataset.
  - *Action:*Â Click theÂ **"Refresh Now"**Â icon.
- [ ] [ ]Â **Validation**
  - *Action:*Â Open the Dashboard. Check the "Last Refreshed" timestamp on the header card.
  - *Action:*Â Spot check one key metric (e.g., "Yesterday's Revenue") to ensure it matches Snowflake.

### **3) Incident Recovery (The "Undo" Button)**

> Goal:Â A pre-planned strategy for what to do if you deploy a bug.

**âœ” 16.1 Rollback Plan**

- [ ] [ ]Â **Immediate Code Revert**
  - *Action:*Â Revert the Git merge commit onÂ `main`.
- [ ] [ ]Â **Re-Deploy**
  - *Action:*Â RunÂ `dbt build --target prod`Â immediately with the reverted code to restore the previous state.
- [ ] [ ]Â **Communication**
  - *Action:*Â (Simulated) "Notify stakeholders that data is stale until 10:00 AM."

[Deliverables](https://www.notion.so/Deliverables-2cd1bb84f4a481728447f9d3e0f35f1c?pvs=21)

# **ğŸ¨Â Phase 5: Semantic Modeling & Visualization (Power BI)**

### DataFinOps Strategy

### **A. Compute & License Optimization (The "Snowflake Bill" Saver)**

- [ ] [ ]Â **Prefer Import Mode over DirectQuery:**
  - *The Logic:*Â **DirectQuery**Â sends a SQL query to SnowflakeÂ *every time*Â a user clicks a slicer.Â This keeps your Snowflake warehouse running (and billing) constantly.
  - *The FinOps Move:*Â UseÂ **Import Mode**. It hits SnowflakeÂ *once*Â a day (during refresh), caches the data, and then costsÂ **$0**Â in Snowflake compute for the rest of the day, no matter how many users view it.
- [ ] [ ]Â **Incremental Refresh:**
  - *The Logic:*Â Reloading 10 years of history every morning burns Snowflake credits.
  - *The FinOps Move:*Â Configure Power BIÂ **Incremental Refresh**Â to only fetch the last 3 days of data.

### **B. Memory Optimization (The "Capacity" Saver)**

- [ ] [ ]Â **Disable "Auto Date/Time":**
  - *The Logic:*Â By default, Power BI creates hidden date tables for every date column.Â This bloats file size by 30-50%.
  - *The FinOps Move:*Â Turn itÂ **OFF**Â globally. Use a single central Date Dimension.
- [ ] [ ]Â **Vertical Partitioning (Remove Unused Columns):**
  - *The Logic:*Â Power BI is a columnar database.Â High-cardinality columns (like a uniqueÂ `Transaction_GUID`Â orÂ `Description`Â text) that aren't used in charts consume massive RAM.
  - *The FinOps Move:*Â "Select Columns" in Power Query to remove anything not strictly needed for visualization.
- [ ] [ ]Â **Star Schema Enforcement:**
  - *The Logic:*Â One big flat table is slow and heavy.
  - *The FinOps Move:*Â Star Schema (Fact/Dim) reduces the number of rows and unique values stored, lowering memory usage.
- [ ] [ ]Â **Visual Rendering Cost:**
  - *The Move:*Â Limit the number of visuals per page (e.g., max 8).
  - *The Save:*Â Complex pages generate complex DAX queries. If using DirectQuery, this spikes Snowflake CPU usage.

### 1) Connection Strategy & Execution(done)

> Goal:Â Select the optimal data architecture (Import vs. DirectQuery) and establish a secure, optimized connection between Power BI and the Snowflake Production Marts.

### **âœ” 12.1.1 Strategy: Choose Connection Mode**

- [ ] [ ]Â **Assess Data Volume & Latency**
  - **Decision:**Â ChooseÂ **Import Mode**Â for your portfolio.
  - *Reason:*Â Best performance, full DAX capabilities, and easiest to share online. DirectQuery is likely overkill unless you have >10GB of data.
- [ ] [ ]Â **(Optional) Plan for Hybrid/Composite**
  - *Concept:*Â If you have one massive Fact table, you might DirectQuery that one table while Importing Dimensions. (Stick to Import for now to keep it simple).

### **âœ” 12.1.2 Execution: Connect to Snowflake**

- [ ] [ ]Â **Gather Credentials (The "Connection String")**
  - **Server:**Â Copy your Snowflake URL (e.g.,Â `xy12345.us-east-1.snowflakecomputing.com`).
  - **Warehouse:**Â UseÂ `REPORTING_WH`Â (The X-Small warehouse we set up earlier).
  - **Role:**Â UseÂ `REPORTER_ROLE`Â (Read-only access to Marts).
- [ ] [ ]Â **Perform "Get Data"**
  - **Action:**Â Power BI Desktop -> Get Data -> Database -> Snowflake.
  - **Input:**Â Paste Server and Warehouse.
  - **Advanced Options:**Â Set "Command Timeout" to 15 minutes (optional, prevents timeouts on initial heavy loads).
  - **SQL Statement:**Â **Leave Blank.**Â (Best practice is to navigate the explorer, not write custom SQL, to ensure Query Folding works).
- [ ] [ ]Â **Authenticate**
  - **Method:**Â SelectÂ **Microsoft Account**Â (if using SSO/Azure AD) orÂ **Basic**Â (Username/Password) depending on your setup.
  - **Action:**Â Sign in and connect.

### **5.1 Environment & Git Setup (Do This First)**

_Set this up properly so you can track changes from the start._

- [ ] [ ] **Enable PBIP Format**
  - _Action:_ File > Options > Preview Features > Check "Power BI Project (.pbip) save option".
- [ ] [ ] **Save as Project**
  - _Action:_ File > Save As > Browse > Select **"Power BI Project files (\*.pbip)"**.
  - _Result:_ Creates `[Project].Report` and `[Project].Dataset` folders.
- [ ] [ ] **Configure `.gitignore`**
  - _Action:_ Add `.pbix`, `.abf`, and `localSettings.json` to your gitignore file.
  - _Why:_ Keeps your repo cleanâ€”you only commit the definitions, not the cached data.

### âœ” 12.1.3 Select & Load Data

- [ ] [ ]Â **Navigate to Schema**
  - **Path:**Â `ANALYTICS_PROD`Â ->Â `MARTS`.
  - **Selection:**Â Check the boxes for yourÂ `fct_`Â andÂ `dim_`Â tables.
  - **Selection:**Â Check the box forÂ `dim_date`Â andÂ `dim_security`Â (if applicable).
  - **Crucial:**Â DoÂ **NOT**Â select tables fromÂ `STAGING`Â orÂ `RAW`.
- [ ] [ ]Â **Pre-Load Configuration**
  - **Action:**Â ClickÂ **"Transform Data"**Â (Power Query) instead of "Load".
  - **Why:**Â You always want to verify types before the first load.

### âœ” 12.1.4 Incremental Refresh

- [ ] [ ]Â **Create Range Parameters (Mandatory Names)**
  - *Action:*Â In Power Query, go toÂ **Manage Parameters**Â ->Â **New Parameter**.
  - *Name:*Â `RangeStart`Â (Type: Date/Time, Value:Â `2024-01-01 00:00:00`).
  - *Name:*Â `RangeEnd`Â (Type: Date/Time, Value:Â `2024-12-31 00:00:00`).
  - *Note:*Â Case sensitivity matters exactly.
- [ ] [ ]Â **Apply Parameter Filter**
  - *Action:*Â SelectÂ `fct_orders`Â -> SelectÂ `order_date`Â column.
  - *Action:*Â Custom Filter -> "is after or equal to"Â `RangeStart`Â **AND**Â "is before"Â `RangeEnd`.
- [ ] [ ]Â **Verify Query Folding**
  - *Action:*Â Right-click the last "Filtered Rows" step ->Â **View Native Query**.
  - *Check:*Â If the SQL is visible, folding is active. If greyed out, move this step to the top.
- [ ] [ ]Â **Define Refresh Policy**
  - *Action:*Â Close Power Query. Right-clickÂ `fct_orders`Â in the sidebar ->Â **Incremental Refresh**.
  - *Setting:*Â Toggle "Incremental refresh"Â **On**.
  - *Setting:*Â Archive data starting:Â **2 Years**.
  - *Setting:*Â Refresh data starting:Â **3 Days**.
  - *Action:*Â Apply. (Note: The first publish will be slow; subsequent ones will be fast).

---

### ğŸŒŸ Best Practices for Step 12.1

- **The "Reporting Warehouse" Rule:**
  - Always connect Power BI to a dedicated Snowflake Warehouse (e.g.,Â `REPORTING_WH`).
  - *Why?*Â If you connect to yourÂ `TRANSFORM_WH`Â (used for dbt), and dbt is running a heavy job, your dashboard will become slow. Isolating them protects the user experience.
- **Native Query vs. Exploring:**
  - Avoid writingÂ `SELECT * FROM table`Â inside the Power BI connection box.
  - Instead, connect leaving the SQL box empty, and select the table from the list.
  - *Why?*Â This supportsÂ **Query Folding**. Power BI can push filter logic back to Snowflake more effectively if it knows it's looking at a specific table object.
- **Case Sensitivity:**
  - Snowflake is case-sensitive. If you type your warehouse name asÂ `reporting_wh`Â but in Snowflake it isÂ `REPORTING_WH`, the connection might fail. Use ALL CAPS if your objects were created that way.

---

### ğŸ“Œ Summary Update for Your To-Do List

_You can replace your previous "12.1 Connection" section with this more detailed version._

### **âœ” 12.1 Connection & Source Configuration**

- [ ] [ ]Â **Select Strategy**Â Mode: SelectÂ **Import**Â (Recommended for Portfolio) orÂ **DirectQuery**Â (Enterprise/Real-time).
- [ ] [ ]Â **Configure Snowflake Source**Â Server:Â `[your_account].snowflakecomputing.com`Â Warehouse:Â `REPORTING_WH`Â (Isolated compute). Role:Â `REPORTER_ROLE`Â (Read-only access).
- [ ] [ ]Â **Select Objects**Â Database:Â `ANALYTICS_PROD`Â (Never connect to Dev). Schema:Â `MARTS`. Tables: Select Facts, Dims, and Security tables only.
- [ ] [ ]Â **Initial Transformation**Â Action: ClickÂ **Transform Data**Â to open Power Query Editor (Phase 12.1.5).

| **Connection Mode**    | **Best For**                                          | **Pros**                                                              | **Cons**                                                       | **When to Use**                                              |
| ---------------------- | ----------------------------------------------------- | --------------------------------------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------ |
| **Import**             | Small-medium datasets (<1GB), fast performance needed | Â·Fastest query performanceÂ·Offline accessÂ·All DAX functions available | Â·Scheduled refresh onlyÂ·Dataset size limitsÂ·Memory consumption | Historical analysis, static dimensions, performance critical |
| **DirectQuery**        | Real-time data, large datasets (>10GB)                | Â·Real-time dataÂ·No size limitsÂ·Lower memory usage                     | Â·Slower performanceÂ·Limited DAX functionsÂ·Network dependent    | Live dashboards, massive datasets, real-time monitoring      |
| **Hybrid (Composite)** | Mix of real-time facts and static dimensions          | Â·Balanced performanceÂ·Flexible refreshÂ·Optimized queries              | Â·Complex setupÂ·Harder to troubleshootÂ·Mixed limitations        | Large facts with small dims, mixed freshness needs           |

**Key Steps:**

1. Assess data size and refresh requirements
2. Connect Power BI to Snowflake using chosen mode
3. Load dbt mart tables (fct*\*, dim*\*)
4. Test performance with sample queries
5. Configure refresh schedule (Import) or optimize queries (DirectQuery)
6. Document choice and monitor performance

**Quick Decision Guide:**

- Data <1GB + scheduled refresh OK â†’ **Import**
- Data >10GB + real-time needed â†’ **DirectQuery**
- Small dims + large facts â†’ **Hybrid**

### 2) Power BI Table and Model View & Semantic Model Configuration(done)

> Goal:Â Configure a high-performance Semantic Model that enforces the Star Schema, secures data via RLS, and provides a polished experience for end-users.

_Set this up properly so you can track changes from the start._

- [ ] [ ]Â **Enable PBIP Format**
  - *Action:*Â File > Options > Preview Features > Check "Power BI Project (.pbip) save option".
- [ ] [ ]Â **Save as Project**
  - *Action:*Â File > Save As > Browse > SelectÂ **"Power BI Project files (\*.pbip)"**.
  - *Result:*Â CreatesÂ `[Project].Report`Â andÂ `[Project].Dataset`Â folders.
- [ ] [ ]Â **ConfigureÂ `.gitignore`**
  - *Action:*Â AddÂ `.pbix`,Â `.abf`, andÂ `localSettings.json`Â to your gitignore file.
  - *Why:*Â Keeps your repo cleanâ€”you only commit the definitions, not the cached data.

### **âœ” 12.1 Connection & Environment Setup**

- [ ] [ ]Â **Connect to Snowflake**
  - Source: Connect toÂ `ANALYTICS_PROD`Â (orÂ `MARTS`Â schema).
  - Selection: ImportÂ **only**Â the necessaryÂ `fct_`Â andÂ `dim_`Â tables.
  - Mode: ChooseÂ **Import**Â (for speed) orÂ **DirectQuery**Â (for real-time/security compliance).
- [ ] [ ]Â **Configure Global Settings**
  - Action: Go toÂ **Options -> Current File -> Data Load**.
  - Action: UncheckÂ **"Auto Date/Time"**Â (Prevents hidden table bloat).
  - Action: UncheckÂ **"Autodetect new relationships after data is loaded"**Â (Manual control is safer).

**âœ” 12.1.5 Schema Drift Protection (Power Query â€“ Table.SelectColumns)**

- [ ] Add â€œSchema Hardeningâ€ Step per Table
  - Action: In Power Query, for each fct* and dim* table, add a final step named
    Schema Hardening â€“ Select Columns.
  - Action: Use Table.SelectColumns() to keep only the columns you want in the model.
    Example:

```jsx
- Example:
-

#`"Schema Hardening â€“ Select Columns" =`

`Table.SelectColumns(`

`#"Previous Step",`

`{"order_id", "order_date", "customer_id", "revenue"}`

`)`
```

- Why:
  - If new columns are added upstream (dbt/Snowflake) â†’ they are ignored safely.
  - If a required column is renamed or removed â†’ Power Query errors clearly, instead of silently loading wrong data.
- [ ] Apply to All Fact & Dimension Tables
  - Scope: fct_orders, fct_payments, dim_customers, dim_products, dim_date, etc.
  - Rule: The list in Table.SelectColumns() is your Power BI data contract for that table.
- [ ] Keep Query Folding (Performance)
  - Action: Ensure Schema Hardening â€“ Select Columns stays near the top of the step list (but after the Source step) so that View Native Query is still enabled.
  - Check: Right-click last step â†’ View Native Query is not greyed out.
- [ ] Document Selected Columns
  - Action: For each table, copy the column list into your Notion Data Dictionary or Model Documentation page.
  - Purpose: Makes it clear which columns are part of the contract between dbt Marts â†’ Power BI model
  - [ ] **Configure Incremental Refresh Parameters**
  - *Action:*Â CreateÂ `RangeStart`/`RangeEnd`Â -> FilterÂ `fct_orders`Â -> Set Policy (Archive 2 Years, Refresh 3 Days).

### **âœ” 12.2 Data Modeling (The Star Schema)**

- [ ] [ ]Â **Organize Model Layout**
  - Layout: ArrangeÂ `fct_`Â tables in the center andÂ `dim_`Â tables surrounding them.
- [ ] [ ]Â **Define Relationships**
  - Cardinality: Ensure all relationships areÂ **One-to-Many (1:\*)**.
  - Direction: Set Cross-filter direction toÂ **Single**Â (Dimensions filter Facts).
  - Active: Ensure primary relationships are Active; secondary (e.g., Ship Date) are Inactive.
- [ ] [ ]Â **Configure Date Table**
  - Action: SelectÂ `dim_date`Â (imported from Snowflake).
  - Setting: Right-click table ->Â **Mark as Date Table**Â -> SelectÂ `date_key`.
- [ ] [ ]Â **Configure Many-to-Many (If Bridge Table exists)**
  - Action: Set Bidirectional filteringÂ **only**Â on the relationship betweenÂ `Fact <-> Bridge <-> Dim`.

### **âœ” 12.3 Column Configuration (Metadata Layer)**

- [ ] [ ]Â **Data Type Validation**
  - Action: specific check: Key columns = Text/Int, Amounts = Decimal/Currency, Dates = Date.
- [ ] [ ]Â **Hide Technical Columns**
- *Action:*Â Select andÂ **Hide**Â all: Surrogate Keys (`_key`), Source IDs, Audit Flags (`_loaded_at`), and Sort Columns (`month_sort`).
- [ ] [ ]Â **Configure Sort Order**
  - Action: SelectÂ `Month Name`Â ->Â **Sort by Column**Â ->Â `Month Number`.
  - Action: SelectÂ `Day Name`Â ->Â **Sort by Column**Â ->Â `Day of Week`.
- [ ] [ ]Â **Define Data Categories**
  - Action: SetÂ `City`,Â `Country`,Â `Zip`Â to geospatial categories (enables Map visuals).
  - Action: Set URL columns toÂ **Web URL**Â orÂ **Image URL**.
- [ ] [ ]Â **Create Hierarchies**
  - Action: BuildÂ `Date Hierarchy`Â (Year > Quarter > Month > Day).
  - Action: BuildÂ `Geo Hierarchy`Â (Country > State > City).

### **âœ” 12.4 DAX Development & Calculations**

- [ ] **Setup Dedicated Measures Table**
  - Action: Create a blank table namedÂ `_Measures`Â (Underscore keeps it at the top).
  - Organization: CreateÂ **Display Folders**Â inside (e.g.,Â `1. Sales`,Â `2. Ratios`,Â `3. Time Intel`).
- [ ] [ ]Â **Create Base Measures**
  - Logic:Â `Total Revenue = SUM(fct_orders[revenue])`.
  - Formatting: Apply Currency (`$`), Thousands Separator (`,`), and decimal places (`0`Â orÂ `2`).
- [ ] [ ]Â **Create Time Intelligence**
  - Logic: CreateÂ `Revenue YTD`,Â `Revenue YoY`,Â `Revenue MoM`Â usingÂ `CALCULATE`Â andÂ `SAMEPERIODLASTYEAR`.
- [ ] [ ]Â **Integrate dbt Semantic Layer (Optional)**
  - Action: If using dbt MetricFlow, connect to the semantic model or replicate logic exactly.
- [ ] [ ]Â **Implement Calculation Groups**
  - Tool:Â **Tabular Editor**.
  - Action: Create a "Time Intelligence" group to toggle visuals between Current/YTD/YoY dynamically.

### **âœ” 12.5 Security (Row Level Security)**

- [ ] [ ]Â **Import Security Table**
  - Action: EnsureÂ `dim_security_rls`Â is in the model.
- [ ] [ ]Â **Define Roles**
  - Action: Modeling ->Â **Manage Roles**.
  - Logic: Create roleÂ `DynamicAccess`. FilterÂ `dim_security_rls`Â whereÂ `[user_email] = USERPRINCIPALNAME()`.
- [ ] [ ]Â **Validate Security**
  - Action:Â **View As**Â -> CheckÂ `DynamicAccess`Â -> Enter a specific user email to verify data filters correctly.

### **âœ” 12.6 Quality Assurance & Optimization**

- [ ] [ ]Â **Run Best Practice Analyzer (BPA)**
  - Tool:Â **Tabular Editor 3**Â (or 2.x).
  - Action: Scan model for rules (e.g., "Avoid Bi-Directional," "Hide Foreign Keys"). Fix all High Severity warnings.
- [ ] [ ]Â **Audit Performance**
  - Tool:Â **DAX Studio**.
  - Action: RunÂ **VertiPaq Analyzer**. Identify large columns taking up RAM that aren't used.
- [ ] [ ]Â **Validate Data Accuracy**
  - Action: Create a generic "QA Page".
  - Check: CompareÂ `Total Revenue`Â in PBI vs.Â `SELECT SUM(REVENUE) FROM MARTS.FCT_ORDERS`Â in Snowflake.
- [ ] [ ]Â **Generate Documentation**
  - Action: Use DAX Studio to export a list of all Measures, descriptions, and dependencies.

**A) Create Data Dictionary Table (DAX)**

- Modeling â†’ New Table â†’ Paste the full \_Data Dictionary DAX Script

[DAX Script](https://www.notion.so/DAX-Script-2cd1bb84f4a481318a8cf9539a0a5acc?pvs=21)

- Confirm it shows:
  - Columns (from INFO.VIEW.COLUMNS)
  - Measures (from INFO.VIEW.MEASURES)
  - Tables (from INFO.VIEW.TABLES)
  - Relationships (metadata)

ğŸ“Œ This table should be hidden from report pages (only for documentation)

**B) Export Documentation for AI**

- Create a table visual using \_Data Dictionary
- Export as CSV
- Upload CSV in Notion â†’ â€œData Dictionary â€” Exportâ€
- Take model screenshots for evidence (relationships, measure folders)

**C) Generate Measure Descriptions Using AI**

(Tabular Editor preferred)

- Copy-paste full metadata TMDL code into AI
- Use the prompt below

ğŸ“¥ AI Prompt to paste

`The TMDL code below contains details about the DAX measures in my Power BI semantic model.`

`Please add clear, concise, human-readable descriptions for each measure, based on the DAX calculation. Each measure should be preceded by "I/I" and added to a line directly above the measure name. Use the description for Total Cost as an exampleto follow.`

`Do not modify any other code, only add measure descriptions. The TMDL code structure, including tabs, indentation and formatting must remain exactly consistent with the original format below:`

(Immediately paste your DAX TMDL code after this line)

**D) Add Descriptions Back to Model**

- Paste the TMDL in DAX view and hit apply

**E) Create a Documentation Page in Report (Optional)**

- A hidden tab with search + filters to explore Data Dictionary

**F) Governance Audit**

- Add â€œDocumentation Versionâ€ column manually:
  - v1.0 = First AI-generated
  - Update when new measures added
- Add Change Log in Notion:
  - Why change?
  - Impacted reports?
  - Date + Owner

**ğŸ“Œ Summary Deliverables (Add to your Phase Deliverables list)**

| **Deliverable**                        | **Evidence to Attach**                    |
| -------------------------------------- | ----------------------------------------- |
| Live Data Dictionary Table in Power BI | Screenshot of \_Data Dictionary           |
| CSV Export                             | File uploaded to Notion                   |
| AI-generated Descriptions              | Before/After screenshot in Tabular Editor |
| Documentation Page (Optional)          | Screenshot                                |
| Governance Log                         | Notion table                              |

#

| **Value to Employers**                       | **Proof in Portfolio**                   |
| -------------------------------------------- | ---------------------------------------- |
| You deliver self-service analytics           | Tooltips + metadata visible to end-users |
| You reduce support tickets                   | Q&A understands semantic model           |
| You build governed BI                        | Change logs + definitions                |
| Shows understanding of Data Governance       | ADLC Documentation                       |
| Shows you know Power BI Enterprise Practices | Not just visuals                         |

This directly increases your portfolio quality score.

# **If you want, I can now generate:**

âœ” Notion template of this whole phase

âœ” Checklist table formatted for copy-paste

âœ” Example Change Log entries (real-world style)

âœ” A visual architecture graphic showing flow: dbt â Marts â Power BI â Governance

Which one should I deliver next?

---

### ğŸŒŸ Phase 12 Best Practices

- **Version Control (.pbip):**Â Save your file as aÂ **Power BI Project (.pbip)**Â instead ofÂ `.pbix`. This allows you to track changes in Git (e.g., seeing exactly which measure formula changed).
- **Explicit Measures:**Â Never drag a raw numeric column onto a visual. Always wrap it in a Measure (e.g.,Â `SUM(Sales)`). This allows you to change logic centrally later without breaking 50 reports.
- **Star Schema Discipline:**Â If you are tempted to create a "Calculated Column" to join two tables,Â **stop**. Go back to dbt and fix the model there. Power BI is for aggregation, not cleaning.
- **Descriptions:**Â Add descriptions to every Measure in the Model View. These appear as tooltips for users in the "Data" pane, reducing "What does this mean?" questions.

### **âœ” 13.1 Mark Unique Identifiers (Crucial for AI & Q&A)**

- **Why?**Â Power BI's Q&A and Copilot need to know which column represents the "ID" of a row (uniqueness) vs. the "Label" of a row (display name).
- **Where?**Â Go toÂ **Model View**Â > Select a Dimension Table >Â **Properties Pane**Â >Â **Advanced**Â section.
- [ ] [ ]Â **Set "Key Column" (The ID)**
  - *Action:*Â Select your Primary Key (e.g.,Â `Customer Key`).
  - *Setting:*Â In Properties > Advanced >Â **Key Column**, selectÂ `Customer Key`.
  - *Effect:*Â Tells Power BI "This column is unique." Required for "Featured Tables" in Excel.
- [ ] [ ]Â **Set "Row Label" (The Name)**
  - *Action:*Â Select your Name Column (e.g.,Â `Customer Name`).
  - *Setting:*Â In Properties > Advanced >Â **Row Label**, selectÂ `Customer Name`.
  - *Effect:*Â When users ask Q&A "Count Customers," it counts distinct Keys but displays Names.

### **âœ” 13.2 Create Data Dictionary using DAX**

- **Why?**Â Instead of manually writing documentation in Excel, you create a live table inside Power BI that reads its own metadata.
- **Where?**Â Go toÂ **Modeling Ribbon**Â >Â **New Table**.
- [ ] [ ]Â **Create the "Model Metadata" Table**
  - *Action:*Â Paste the following DAX code to generate a live list of all your measures and descriptions.
  ```jsx
  *Code:*Code snippet
  ```
  ```jsx
  Model Dictionary =
  UNION(
      SELECTCOLUMNS(
          INFO.VIEW.MEASURES(),
          "Type", "Measure",
          "Name", [Name],
          "Description", [Description],
          "Formula", [Expression],
          "Table", [Table]
      ),
      SELECTCOLUMNS(
          INFO.VIEW.COLUMNS(),
          "Type", "Column",
          "Name", [Name],
          "Description", [Description],
          "Formula", [Expression],
          "Table", [Table]
      )
  )
  ```
- [ ] [ ]Â **Build the Dictionary Page**
  - *Action:*Â Create a hidden report page.Â Add aÂ **Table Visual**Â using columns from your newÂ `Model Dictionary`Â table.
  - *Benefit:*Â You now have a searchable list of every metric and calculation in your model, directly accessible to users.

### ğŸ“¦ Phase 12 Deliverables

1. **âœ… Optimized Semantic Model:**Â A cleanÂ `.pbip`Â file with a verified Star Schema.
2. **âœ… BPA Report:**Â A screenshot from Tabular Editor showing 0 High-Severity issues.
3. **âœ… Data Dictionary:**Â An Excel export of all Measures and Definitions.
4. **âœ… QA Validation:**Â A simple matrix showing Power BI totals matching Snowflake totals.

- **âœ… Key Columns Set:**Â All Dimension tables have "Key Column" and "Row Label" defined in properties.\
- **âœ… Dictionary Table:**Â A genericÂ `Model Dictionary`Â table present in the model usingÂ `INFO.VIEW`Â functions.
- **âœ… Clean Interface:**Â No technical keys visible in Report View; all Sort columns hidden.
- **âœ… Descriptions:**Â Tooltips appear when hovering over Measures in the Data Pane.

### 3) Visualization and Reporting(done)

> Goal:Â Transform your data model into a compelling, interactive story using "Information Design" principles, ensuring accessibility, performance, and mobile responsiveness.

# **ğŸ¯**

# **Slicer & Query Reduction Strategy (Prevent Query Explosion)**

Purpose:

Control unnecessary DAX query execution caused by slicer interactions to protect performance, Snowflake compute, and user experience.

# **ğŸ§  Why This Matters (Context for Reviewers)**

- Every slicer interaction triggers multiple DAX queries
- On large models, this leads to:
  - Slow visuals
  - Excessive CPU usage
  - Poor scalability in Power BI Service
-
- This section enforces controlled, batched filtering â€” a senior BI best practice

# **âœ… 12.X Query Reduction Configuration (Mandatory)**

**âœ” Enable â€œApplyâ€ Button for Slicers**

Action Path

- Power BI Desktop â†’
  File â†’ Options and settings â†’ Options â†’ Query reduction

Settings

- [ ] Enable Add an Apply button to each slicer
- [ ] (Optional) Disable Auto apply

Result

- Slicer selections do not trigger queries immediately
- Queries execute only when user clicks Apply
- Reduces query count by 50â€“70% on interactive pages

ğŸ“Œ Rule:

All reports in this project must use Apply buttons for slicers.

# **âœ… 12.X Slicer Design Rules (Performance Guardrails)**

**ğŸ”¹ Rule 1: Prefer**

**Single-Select Slicers**

Action

- Select slicer â†’ Format pane â†’ Selection controls
- Enable Single select

Why

- Prevents combinatorial filter explosions
- Keeps DAX evaluation paths simple
- Improves cache reuse

ğŸ“Œ Exception:

Multi-select allowed only for low-cardinality dimensions (e.g., Region with <10 values).

**ğŸ”¹ Rule 2: Restrict Slicers to Low-Cardinality Fields**

Allowed Slicer Fields

- Year
- Month
- Region
- Category
- Channel

Disallowed Slicer Fields

- Customer Name
- Product Name / SKU
- Transaction ID
- Free-text attributes

ğŸ“Œ High-cardinality columns belong in the Filter Pane, not slicers.

| **Aspect**      | **Filter Pane** | **Slicer**      |
| --------------- | --------------- | --------------- |
| Query Execution | Batched         | Per interaction |
| UI Clutter      | Hidden          | Visible         |
| Performance     | High            | Lower           |
| User Abuse      | Limited         | High            |

# **âœ… 12.X Filter Pane Usage (Preferred for Detailed Filtering)**

**Why Filter Pane is Preferred**

**Filter Pane Rules**

- Use Filter Pane for:
  - Customer
  - Product
  - Store
  - SKU
-
- Keep slicers only for high-level navigation

ğŸ“Œ Filters apply once, not on every click.

# **âœ… 12.X Recommended Page Layout Pattern (Standard)**

Header Area

- Year (Single-select slicer)
- Month (Single-select slicer)
- Region (Single-select slicer)
- Apply Button enabled

Right-Side Filter Pane

- Customer
- Product
- Channel
- Store

This layout balances:

- Performance
- Discoverability
- Professional UX

# **âŒ Anti-Patterns (Explicitly Forbidden)**

- âŒ More than 5 slicers on a page
- âŒ Multi-select slicers on high-cardinality fields
- âŒ Slicers without Apply button
- âŒ Using slicers instead of Filter Pane for detailed filters

Violations must be fixed before publish.

### **âœ” 14.1 Design Strategy & Wireframing (Pre-Development)**

- [ ] [ ]Â **Define the "Big 4" Business Questions**
  - **Action:**Â Explicitly list the 4 questions this specific report page must answer.
  - **Constraint:**Â If a visual does not answer one of these questions, delete it.
- [ ] [ ]Â **Sketch the Layout (Wireframe)**
  - **Tools:**Â Excalidraw, Figma, or Pen & Paper.
  - **Pattern:**Â Adopt theÂ **"Z-Pattern"**Â layout (KPIs Top-Left â†’ Trends Middle â†’ Detail Data Bottom).
- [ ] [ ]Â **Map Questions to Visuals**
  - **Action:**Â Decide the chart type before clicking.
  - **Logic:**
    - Single Number â†’Â **Card (New)**
    - Trend over Time â†’Â **Line**Â orÂ **Area Chart**
    - Comparison/Ranking â†’Â **Bar Chart**
    - Part-to-Whole â†’Â **Donut Chart**Â (Max 3 slices) orÂ **Tree Map**.

| **Business Question** | **Visual** | **Purpose** | **Why This Visual Works** | **What To Do in the Visual (Action Steps)** | **Best Practices** |
| --------------------- | ---------- | ----------- | ------------------------- | ------------------------------------------- | ------------------ |

### **âœ” 14.2 Canvas & Theme Configuration**

- [ ] [ ]Â **Configure Canvas Settings**
  - **Action:**Â Visualization Pane > Format Page > Canvas Background.
  - **Settings:**Â Color: Light Grey (`#F0F2F5`), Transparency:Â **0%**, Image Fit:Â **Fit**.
  - **Why:**Â A grey background makes white visual containers "pop" with a shadow effect.
- [ ] [ ]Â **Import Corporate JSON Theme**
  - **Action:**Â View > Browse for Themes > SelectÂ `corporate_theme.json`.
  - **Why:**Â Enforces standard fonts (Segoe UI), palettes, and font sizes globally to save time.
- [ ] [ ]Â **Set Page Metadata**
  - **Action:**Â Rename "Page 1" to a descriptive title (e.g., "Executive Summary").
  - **Action:**Â Hide any "Drill-through" or "Tooltip" pages from the navigation bar (Right-click tab > Hide).

### **âœ” 14.3 Visual Construction (The Build)**

- [ ] [ ]Â **Build KPI Header (New Card Visual)**
  - **Action:**Â Use the "Card (New)" visual to group multiple KPIs.
  - **Setup:**Â Add "Reference Labels" to show MoM % change below the main number.
  - **Context:**Â Ensure every KPI has a "vs Target" or "vs Prior Period" indicator.
- [ ] [ ]Â **Configure Chart Hygiene**
  - **Titles:**Â Write descriptive titles (e.g., "Revenue by Region" not "Sum of Revenue by RegionName").
  - **Axes:**Â Remove Y-Axis titles if the chart title is self-explanatory. Remove X-Axis gridlines.
  - **Sorting:**
    - Time-based charts:Â **Chronological**Â (Jan â†’ Dec).
    - Categorical charts:Â **Magnitude**Â (Highest value on top/left).
- [ ] [ ]Â **Apply Conditional Formatting**
  - **Action:**Â AddÂ **Data Bars**Â to Matrix visual columns.
  - **Action:**Â AddÂ **Icons**Â (Red/Green arrows) to Variance columns.
  - **Logic:**Â Use colors strictly for performance (Green=Good, Red=Bad). Use neutral Blue/Grey for categories.

### **âœ” Step 1: Initialize the Metadata Table**

_Ensure the DAX script forÂ `_Data Dictionary`Â table is executed as per Step 12.7.1._

- [ ] Name the table:Â `_Data Dictionary`.
- [ ] SetÂ `IsHidden = True`Â for the table in Model View (Technical users reference it via the Report Page).

### **âœ” Step 2: Configure the Hidden Wiki Page**

- [ ] Create a new Report Page titled:Â `ğŸ“˜ Data Dictionary`.
- [ ] Right-click the page tab >Â **Hide Page**.
- [ ] [ ]Â *Reason:*Â Keeps the dashboard navigation clean; users access it only via an information "â„¹ï¸" button.

### **âœ” Step 3: Build the "Data Glossary" Table Visual**

- [ ] [ ]Â **Visual Type:**Â Table.
- [ ] [ ]Â **Fields to include:**
  - `Table`:Â Groups information by Fact/Dimension (Context).
  - `Column/Measure/Relationship`:Â The exact name visible in the "Data" pane.
  - `Type`:Â Identifies if the row is a base column,Â calculated measure,or join.
  - `Description`:Â Business definition (generated by AI TMDL process).
  - `Expression`:Â The raw DAX logic (Technical source of truth).
- [ ] [ ]Â **Sorting Logic:**Â Sort primary byÂ `Table`Â â†’ secondary byÂ `Type`.

### **âœ” Step 4: Add Search & Wiki Navigation**

- [ ] [ ]Â **Type Slicer:**Â Filter by Column,Â Measure,Â or Table.
- [ ] [ ]Â **Table Slicer:**Â Filter by specific model (e.g.,Â `fct_orders`).
- [ ] [ ]Â **Search Box:**Â Field:Â `Column/Measure/Relationship`.Â Use the "Text Search" slicer or standard slicer with Search enabled.

### **âœ” Step 5: Add Metadata Inventory Cards (The "Model at a Glance")**

_Create these DAX measures to summarize the model complexity:_

- [ ] [ ]Â **Count Tables:**Â `COUNTROWS(FILTER('_Data Dictionary', [Information View]="Table"))`
- [ ] [ ]Â **Count Columns:**Â `COUNTROWS(FILTER('_Data Dictionary', [Information View]="Column"))`
- [ ] [ ]Â **Count Measures:**Â `COUNTROWS(FILTER('_Data Dictionary', [Information View]="Measure"))`
- [ ] [ ]Â **Layout:**Â Place these cards across the top header of the page.

### **âœ” Step 6: Add Governance Context (Text Box)**

- [ ] Add a text box with the following disclaimer:
  > "This page documents the business logic of every field in this semantic model. Use the search bar to explore definitions. If logic appears incorrect or a description is missing, contact the Report Owner at: [Your Professional Email]."

## ğŸ” Permissions Strategy (Professional Context)

In a real enterprise environment,Â the Data Dictionary page often has specific permissions applied,Â which shows advanced architectural thinking:

- **BI Developers/Data Engineers:**Â Full visibility to the page for auditing and debugging.
- **Business Users (Published App):**Â The page is oftenÂ **excluded**Â from the final Published App to keep the navigation simple.Â If included,Â it's often read-only access.

This strategy demonstrates how you would separateÂ **BI Developers â†” Business Users**Â in a secure production environment.

### **âœ” 14.4 Advanced Interactivity**

- [ ] [ ]Â **Configure Navigation (Slicers)**
  - **Setup:**Â Place slicers in a collapsible "Filter Pane" or a dedicated Header strip.
  - **Sync:**Â View >Â **Sync Slicers**Â > Ensure "Year" and "Region" filter all relevant pages simultaneously.
- [ ] [ ]Â **Implement Dynamic Titles (DAX)**
  - **Measure:**Â `Title Sales = "Sales Performance for " & SELECTEDVALUE('Dim_Geo'[Region], "All Regions")`.
  - **Action:**Â Format Visual > General > Title >Â **fx**Â > Field Value > Select Measure.
- [ ] [ ]Â **Setup Drill-Through**
  - **Action:**Â Create a "Transaction Details" page.
  - **Setup:**Â DragÂ `Order ID`Â into the "Drill-through" well on the details page.
  - **UX:**Â Add a "Back" button to the details page.
- [ ] [ ]Â **Configure Custom Tooltips**
  - **Action:**Â Create a page sizedÂ **"Tooltip"**. Add a mini-trend chart.
  - **Link:**Â Select main visual > Format > General > Tooltips > Type:Â **Report Page**Â > Page: [Tooltip Page].

### **âœ” 14.5 Backend Management (The "Pro" Standard)**

- [ ] [ ]Â **Manage Selection Pane (Mandatory)**
  - **Action:**Â View > Selection.
  - **Task:**Â RenameÂ **EVERY**Â object (e.g.,Â `Card - Total Revenue`,Â `Chart - Monthly Trend`).
  - **Task:**Â Group related objects usingÂ **Ctrl+G**Â (e.g.,Â `Group - Header`,Â `Group - KPI Panel`).
- [ ] [ ]Â **Manage Layer Order (Z-Index)**
  - **Action:**Â Move Slicers/Dropdowns to theÂ **Top**Â of the list.
  - **Action:**Â Move Background Shapes to theÂ **Bottom**.
  - **Why:**Â Prevents dropdown menus from getting cut off behind charts.

### **âœ” 14.6 Mobile & Accessibility Polish**

- [ ] [ ]Â **Configure Mobile Layout**
  - **Action:**Â View > Mobile Layout.
  - **Task:**Â Re-arrange visuals into a single vertical column. Do not just shrink the desktop view.
- [ ] [ ]Â **Set Tab Order**
  - **Action:**Â Selection Pane >Â **Tab Order**.
  - **Logic:**Â Number visuals sequentially (Left-to-Right, Top-to-Bottom) for keyboard navigation.
- [ ] [ ]Â **Add Alt Text**
  - **Action:**Â Format > General >Â **Alt Text**.
  - **Content:**Â Add descriptions for Screen Readers (e.g., "Bar chart showing top 5 products by revenue").

### **âœ” 14.7 QA & Final Optimization**

- [ ] [ ]Â **Run Performance Analyzer**
  - **Action:**Â View > Performance Analyzer > Start Recording > Refresh Visuals.
  - **Target:**Â Identify and fix any visual taking >Â **1000ms**Â to load.
- [ ] [ ]Â **Check "Empty States"**
  - **Test:**Â Filter to a region with 0 sales.
  - **Validation:**Â Does the chart show a clean "No Data" message, or does it break?
- [ ] [ ]Â **Add Metadata Footer**
  - **Measure:**Â `Last Refreshed = "Data updated: " & FORMAT(LASTDATE('fct_orders'[order_date]), "DD-MMM-YYYY")`.
  - **Visual:**Â Add a small text box/card in the bottom right corner displaying this measure.

---

### ğŸŒŸ Phase 14 Best Practices

- **The "Visual Header" Cleanup:**
  - Go to Format > General >Â **Header Icons**.
  - TurnÂ **OFF**Â icons for static elements like KPI Cards, Shapes, and Text Boxes. It makes the report look like an App, reduces clutter, and prevents users from "Filtering a Text Box."
- **Visual Density Rule:**
  - Limit the report toÂ **6-8 major visuals**Â per page. Cognitive load increases drastically after 8 visuals.
- **Avoid "Chart Junk":**
  - If a value is labeled on the bar (Data Label), you doÂ **not**Â need the Y-Axis. Delete the axis to save whitespace.
- **Consistent Margins:**
  - Ensure exactlyÂ **10px or 20px**Â padding between all visuals. Use the "Align" and "Distribute" tools in the Format ribbon; never eyeball it.

### ğŸ“¦ Phase 14 Deliverables

1. **âœ… Polished PBI Project:**Â AÂ `.pbip`Â file with a renamed Selection Pane and grouped elements.
2. **âœ… Mobile View:**Â A verified, scrollable mobile layout.
3. **âœ… QA Checklist:**Â A signed-off list confirming Drill-throughs, Slicers, and Tooltips work.
4. **âœ… Performance Report:**Â Screenshot of Performance Analyzer showing fast load times.

### business question to visuals mapping table

###

### 4) Power BI Service(done)

### â˜ï¸ Phase 15: Power BI Service Deployment

> Goal:Â Deploy the polished report to a managed cloud environment, secure it with Row Level Security (RLS), and distribute it via an "App" rather than raw workspace access.

### **âœ” 15.1 Workspace Architecture**

- [ ] [ ]Â **Create Production Workspace**
  - **Naming Standard:**Â Create a new workspace namedÂ `[Project Name] - PROD`Â (e.g.,Â `Sales Analytics - PROD`).
  - **Constraint:**Â Never use "My Workspace" for portfolio or production work.
- [ ] [ ]Â **Assign Workspace Roles (RBAC)**
  - **Admins/Members:**Â Add yourself (and other developers) here.
  - **Viewers:**Â DoÂ **not**Â add end-users here yet (they will access via the App).
- [ ] [ ]Â **Configure Support Contact**
  - **Action:**Â Workspace Settings > Contact List. Add your email so error messages direct to you.

### **âœ” 15.2 Publishing & Connectivity**

- [ ] [ ]Â **Publish Report**
  - **Action:**Â In Power BI Desktop > Publish > SelectÂ `[Project Name] - PROD`.
- [ ] [ ]Â **Configure Data Gateway (If DirectQuery/On-Prem)**
  - **Check:**Â Ensure Standard Gateway is "Online" in "Manage Connections and Gateways".
  - **Mapping:**Â Map the published dataset to the correct Gateway data source.
- [ ] [ ]Â **Update Cloud Credentials (Import Mode)**
  - **Action:**Â Dataset Settings > Data Source Credentials.
  - **Auth:**Â Set toÂ **OAuth2**Â (for Snowflake) and sign in.
- [ ] [ ]Â **Schedule Refresh**
  - **Timing:**Â Set to runÂ *after*Â your dbt Cloud job finishes (e.g., if dbt finishes at 6:00 AM, set PBI to 6:30 AM).
  - **Notifications:**Â Check "Send refresh failure notification to dataset owner".

### **âœ” 15.3 Security & Governance**

- [ ] [ ]Â **Assign RLS Members**
  - **Action:**Â Dataset > Security > Row-Level Security.
  - **Task:**Â Add specific emails or Security Groups to theÂ `DynamicAccess`Â role you created in Phase 12.
- [ ] [ ]Â **Test RLS (Cloud Verification)**
  - **Action:**Â Click ellipsis (...) next to role > "Test as role".
  - **Validation:**Â Confirm you see data filtered as that user.
- [ ] [ ]Â **Endorsement (Trust Signal)**
  - **Action:**Â Dataset Settings > Endorsement.
  - **Selection:**Â Set toÂ **Promoted**Â (Ready for broad usage) orÂ **Certified**Â (Official Golden Dataset).

### **âœ” 15.4 Distribution (The App Method)**

- [ ] [ ]Â **Configure the App**
  - **Action:**Â Click "Create App" (or "Update App") in the Workspace.
  - **Description:**Â Add a clear description and a support link.
- [ ] [ ]Â **Configure Navigation**
  - **Action:**Â Hide any "Validation" or "Draft" reports from the navigation pane.
  - **Structure:**Â Group reports into Sections if you have multiple reports.
- [ ] [ ]Â **Manage Audiences (Advanced View)**
  - **Scenario:**Â If you have an "Executive Summary" page and a "Deep Dive" page.
  - **Action:**Â Create anÂ `Exec Audience`Â (sees only summary) andÂ `Analyst Audience`Â (sees everything).
  - **Access:**Â Assign users/groups to specific Audiences.
- [ ] [ ]Â **Publish App**
  - **Action:**Â Click "Publish App" and copy the link.

### **âœ” 15.5 Automation & Monitoring**

- [ ] [ ]Â **Configure Subscriptions**
  - **Action:**Â Subscribe yourself to the "Executive Summary" page (Email: Daily at 8 AM).
  - **Purpose:**Â Acts as a daily "Smoke Test"â€”if you get the email, the system is working.
- [ ] [ ]Â **Set Data Alerts (Dashboard Only)**
  - **Action:**Â Pin a KPI card to a Dashboard.
  - **Trigger:**Â Set alert rule: "If Revenue drops below $X, email me."
- [ ] [ ]Â **Review Usage Metrics**
  - **Action:**Â Open Usage Metrics Report.
  - **Check:**Â Monitor "Views per day" and "Views per user" to see adoption.

---

### ğŸŒŸ Phase 15 Best Practices

- **App > Workspace:**
  - **Rule:**Â Never share a report by clicking the "Share" button on the report itself.
  - **Why:**Â Managing 100 individual share links is a nightmare. Managing 1 App with 1 Audience is scalable.
  - **Best Practice:**Â Give users "Viewer" access via the App. Give Developers "Member" access via the Workspace.
- **Gateway Cluster:**
  - If using an On-Premise Gateway, ensure you have aÂ **Cluster**Â (2+ gateways). If one machine updates/restarts, the other handles the refresh.
- **Parameterize Connections:**
  - In Power BI Desktop (Power Query), useÂ **Parameters**Â for Server/Warehouse names.
  - *Benefit:*Â You can change the connection fromÂ `DEV`Â toÂ `PROD`Â inside the Power BI Service settings without re-publishing the PBIX file.
- **Deployment Pipelines (Premium Feature):**
  - If you have a Premium/Fabric capacity, useÂ **Deployment Pipelines**Â to promote content fromÂ `Dev Workspace`Â ->Â `Test Workspace`Â ->Â `Prod Workspace`.

### ğŸ“¦ Phase 15 Deliverables

1. **âœ… Live App Link:**Â A distinct URL for the published App (not the workspace).
2. **âœ… Refresh Log:**Â Screenshot showing a successful scheduled refresh history.
3. **âœ… RLS Validation:**Â Screenshot of the "Test as Role" screen proving security works.
4. **âœ… Subscription Email:**Â Proof that the automated report delivery system is active.

### 5) Optimization and Best Practices(done)

### âš¡ Quick Optimization Checklist

- **Data Model Optimization:**
  - Remove unused columns and tables from the model
  - Disable auto date/time hierarchy in Power BI settings
  - Replace calculated columns with measures where possible
  - Use appropriate data types (integers instead of text for IDs)
- **DAX Performance:**
  - Avoid using CALCULATE unnecessarily
  - Use variables (VAR) to store repeated calculations
  - Replace ALL() with ALLSELECTED() when appropriate
  - Avoid using iterators (SUMX, FILTER) on large tables
- **Visual Optimization:**
  - Limit visuals per page to 5-7 maximum
  - Use aggregated tables for large datasets
  - Run Performance Analyzer to identify slow visuals
  - Remove excessive formatting and custom themes

This is the final layer of polish. You have a solid process; now let's add the "Senior Engineer" optimizations that make your project scalable, faster, and team-ready.

Here is theÂ **Optimization & Git Strategy Guide**Â specifically for your Power BI workflow.

---

### ğŸš€ Power BI Performance Optimization Strategy

### **1. Data Model Optimization (The Engine)**

- **Vertical Partitioning (Remove Columns):**Â RunÂ **DAX Studio**Â -> "VertiPaq Analyzer". Look at the "Col Size" column. If a high-cardinality column (like a unique Transaction ID or UUID) takes up 50% of your model size but isn't used for counting,Â **remove it**Â in Power Query.
- **Aggregations (The Speed Layer):**Â If yourÂ `fct_orders`Â has 100 million rows, create a smallerÂ `agg_sales_by_month`table in Snowflake/dbt. Configure Power BI to use this smaller table for high-level visuals and only query the big table when users drill down.
- **Disable Auto Date/Time:**Â **Mandatory.**Â This reduces file size by preventing Power BI from creating hidden date tables for every datetime column.

### **2. Power Query Optimization (The ETL)**

- **Query Folding:**Â In Power Query, right-click your last step -> "View Native Query". If it's greyed out, you broke folding (meaning Power BI is downloading raw data to filter it locally).
  - *Fix:*Â Move filters and column removals to theÂ *top*Â of the steps list.
- **Avoid "Merge Queries" in Power Query:**Â Do joins inÂ **dbt/Snowflake**. Power Query merges are CPU intensive and slow down refresh times.

### **3. DAX Optimization (The Calc)**

- **Variables (`VAR`) are Mandatory:**
  - *Bad:*Â `IF(SUM(Sales) > 100, SUM(Sales) * 0.1, SUM(Sales) * 0.05)`Â -> CalculatesÂ `SUM(Sales)`Â 3 times.
  - *Good:*Code snippet
    `VAR _Sales = SUM(Sales)
RETURN IF(_Sales > 100, _Sales * 0.1, _Sales * 0.05)`
- **Strict Boolean Logic:**Â AvoidÂ `FILTER(Table, Column = Value)`. UseÂ `KEEPFILTERS`Â orÂ `CALCULATE(..., Column = Value)`Â to leverage the storage engine (faster) instead of the formula engine (slower).

### **4. Visual Rendering Optimization (The UX)**

- **Limit Visuals:**Â MaxÂ **8 visuals**Â per page. Each visual sends a separate query to the engine.
- **Reduce Slicer Interaction:**Â Go toÂ **Format > Edit Interactions**. TurnÂ **OFF**Â filtering for visuals that don't need it (e.g., a "Year" slicer shouldn't try to filter a static "Company Logo" or a text box).

---

### ğŸ™ Git & GitHub Strategy for Power BI

Power BI binary files (`.pbix`) are notoriously hard to version control. The game changed withÂ **Power BI Projects (`.pbip`)**. Here is your workflow:

### **Step 1: Save as Project (.pbip)**

- **Action:**Â File -> Save As -> Browse -> Select typeÂ **"Power BI Project files (\*.pbip)"**.
- **Result:**Â This creates a folder structure:
  - `Project.Report/`Â (Visuals, layout - JSON format)
  - `Project.Dataset/`Â (Model, DAX - TOM format)

### **Step 2: ConfigureÂ `.gitignore`**

Add these specific lines to your repo'sÂ `.gitignore`Â file to avoid committing local cache data:

Plaintext

`# Power BI exclusions
*.pbix
.DS_Store
*.abf
*.pbiq
localSettings.json
cache.abf`

### **Step 3: Branching Strategy**

- **Main Branch:**Â The "Golden" production version.
- **Feature Branch:**Â Create a branch for specific tasks (e.g.,Â `feature/add-finance-dashboard`).
  - *Command:*Â `git checkout -b feature/add-finance-dashboard`

### **Step 4: The Commit Workflow**

1. **Work:**Â Open theÂ `.pbip`Â file, make changes (e.g., add a measure), and save.
2. **Stage:**Â `git add .`
   - *Note:*Â Git will detect text changes in theÂ `model.bim`Â file (e.g.,Â `"name": "Total Revenue"`).
3. **Commit:**Â `git commit -m "feat: added Total Revenue measure"`
4. **Push:**Â `git push origin feature/add-finance-dashboard`

### **Step 5: Pull Requests (PR)**

- **Action:**Â Open a PR in GitHub.
- **Review:**Â You can now seeÂ *code diffs*!
  - *Example:*Â You will seeÂ `+ Measure: SUM(Sales)`Â in green. This allows you to code-review DAX without opening the file.

---

### ğŸ› ï¸ Final "Project polish" Checklist (The Portfolio Finisher)

Add this to the very bottom of your Notion. Itâ€™s the "Definition of Done."

- [ ] **The "Blink Test":**Â Look at your dashboard for 3 seconds. Can you answer "Is business good or bad?" immediately? If not, your KPIs are unclear.
- [ ] **The Color Blind Test:**Â Use a simulator (like "Coblis") to ensure your Red/Green indicators are distinguishable by color-blind users (add icons/arrows to be safe).
- [ ] **Tooltip Cleanup:**Â Hover overÂ *every*Â chart. Did you leave a "Count of ID" or weird internal column name visible in the tooltip? Hide them.
- [ ] **Empty State Check:**Â Filter the report to a date/region with zero data. Does it look broken? Add a "No Data Available" text box that appears when the row count is 0.

### ğŸš€ Next Step

You now have a complete, end-to-end Enterprise Data Roadmap.

Would you like me to generate a Standard Readme.md template for your GitHub repository that summarizes this entire architecture so recruiters can understand your project at a glance?

- **Incremental Refresh:**
  - Set up incremental refresh for large fact tables
  - Keep only necessary historical data (e.g., last 2-3 years)
  - Archive older data to reduce model size
- **Best Practice Analyzer:**
  - Install and run Tabular Editor's Best Practice Analyzer
  - Fix all critical and warning issues
  - Document exceptions if you can't fix certain warnings
- **Compression Tips:**
  - Remove high-cardinality text columns
  - Split date-time columns into separate date and time columns
  - Use numeric keys instead of text keys for relationships
- **Monitoring:**
  - Check .pbix file size (should be under 250 MB ideally)
  - Monitor refresh times (should complete within 30 minutes)
  - Track user report load times via Performance Analyzer

### ğŸ¯ Quick Wins (30-Minute Fixes)

1. Run Performance Analyzer â†’ Identify slowest visual â†’ Simplify or remove it
2. Check Model View â†’ Remove unused columns and tables
3. Power Query â†’ Disable load for staging queries
4. DAX measures â†’ Add VAR to store repeated calculations
5. Visuals â†’ Reduce to maximum 5 per page

### ğŸ“Š Performance Targets

- Report load time: Under 3 seconds
- Visual refresh time: Under 1 second per visual
- Data refresh time: Under 10 minutes for typical datasets
- File size: Under 100 MB for optimal performance

###

###

[Deliverables Checklist](https://www.notion.so/Deliverables-Checklist-2cd1bb84f4a48175af1dfe49c77a5067?pvs=21)

# **ğŸ“ŠÂ Phase 6: Analytics, Insights & Business Impact**

### **1) Answer Each Business Question(done)**

### **16.1 The "Q&A" Analysis (Answering Phase 1 Questions)**

- [ ] [ ]Â **Structure the Answers**
  - **Action:**Â Create a section in yourÂ `REPORT.md`Â for each of the 5-8 questions defined in Phase 1.
  - **Format:**Â Use the specific format below for consistency.
- [ ] [ ]Â **Draft Question 1 (The Headline Number)**
  - **Question:**Â "What is total sales revenue this year?"
  - **Answer:**Â "Total sales reached â‚¹12.3 crore."
  - **Visual Used:**Â KPI Card (with trend indicator).
  - **What it means:**Â "Sales grew by 14% compared to last year, indicating strong market recovery."
- [ ] [ ]Â **Draft Question 2 (The Trend)**
  - **Question:**Â "Is revenue growing month-over-month?"
  - **Answer:**Â "Growth is positive but flattening in Q4."
  - **Visual Used:**Â Line Chart with 3-month rolling average.
  - **What it means:**Â "While Q1-Q3 were strong, customer demand has plateaued in the last 3 months."
- [ ] [ ]Â **Draft Remaining Questions**
  - **Action:**Â Repeat this pattern for all 8 questions. Ensure every answer is backed by a specific visual from your dashboard.

###

### **2) Write Key Insights (Simple, Clean, Beginner Friendly(done)**

### **16.2 Key Insights (The "Why" & "So What?")**

- [ ] [ ]Â **Draft Insight 1 (The Problem)**
  - **Insight:**Â "Sales dropped significantly in the South region."
  - **Evidence:**Â "Bar chart shows a âˆ’12% YoY decline specifically in Karnataka and Kerala."
  - **Reason:**Â "A sharp drop in repeat customer transactions (Retention Rate down 15%)."
  - **Impact:**Â "The business is projected to lose â‚¹8â€“10 lakh per quarter if this trend continues."
  - **Recommendation:**Â "Improve local delivery times and launch a targeted loyalty discount campaign."
- [ ] [ ]Â **Draft Insight 2 (The Opportunity)**
  - **Insight:**Â "The 'Electronics' category is the primary growth driver."
  - **Evidence:**Â "Tree Map shows Electronics accounts for 60% of total revenue with +22% growth."
  - **Reason:**Â "High Average Order Value (AOV) combined with low return rates."
  - **Impact:**Â "Focusing here maximizes profit margin."
  - **Recommendation:**Â "Increase inventory stock for top-selling electronic SKUs before the holiday season."

###

### **3) Show Before/After of Your Business Impact of my Data Work (done)**

As a beginner data analyst, you MUST show business value from data project work

**âœ” 16.4 Business Impact Analysis (Before vs. After)**

- [ ] [ ]Â **Quantify Your Value (The "Money Slide")**
  - **Action:**Â Create a table or section highlighting the operational improvements your project delivered.
- [ ] [ ]Â **Draft the Comparison Table**
  - **Feature:**Â **Data Refresh**
    - *Before (Problem):*Â "Reports were manually updated weekly using Excel exports. Stakeholders waited 3-5 days for data."
    - *After (Solution):*Â "Automated ELT pipeline (Snowflake + dbt) refreshes data daily without human intervention."
    - *Business Impact:*Â "Reduced time-to-insight by 90%. Stakeholders now make decisions onÂ *yesterday's*data, not last week's."
  - **Feature:**Â **Ad-hoc Analysis**
    - *Before (Problem):*Â "Answering a new question (e.g., 'Sales by City') required a new manual data pull (4-6 hours)."
    - *After (Solution):*Â "Clean, trusted Star Schema in Power BI allows ad-hoc questions to be answered in seconds via drag-and-drop."
    - *Business Impact:*Â "Saved ~20 hours of analyst time per week, freeing up resources for strategic planning."
  - **Feature:**Â **Data Quality**
    - *Before (Problem):*Â "Metrics often mismatched between departments due to siloed spreadsheets."
    - *After (Solution):*Â "Implemented a 'Single Source of Truth' via dbt Marts and Semantic Layer."
    - *Business Impact:*Â "Eliminated metric discrepancies, building 100% trust in the executive dashboard."

Example:

| **Feature**         | **Before (The Problem) âŒ**                                                                               | **After (Your Solution) âœ…**                                                                                | **Business Impact ğŸ“ˆ**                                                                                      |
| ------------------- | --------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| **Data Refresh**    | Reports were manually updated weekly using Excel/CSV exports. Stakeholders waited days for data.          | AutomatedÂ **ELT pipeline**(Snowflake + dbt) refreshes data daily/hourly without human intervention.         | **Reduced time-to-insight by 90%**. Stakeholders now make decisions onÂ *yesterday's*Â data, not last week's. |
| **Ad-hoc Analysis** | Answering a new question (e.g., "Sales by City") required a new manual data pull and cleanup (4-6 hours). | Clean, trustedÂ **Star Schema**Â in Snowflake allows ad-hoc questions to be answered in minutes via Power BI. | **Saved ~20 hours of analyst time per week**, freeing up resources for strategic analysis.                  |

This shows you haveÂ **real data analyst skills**.

Simple language but powerful impact.

### **4) Write Business Recommendations (Simple, Clear)(done)**

### **âœ” 16.3 Strategic Recommendations (Action Plan)**

- [ ] [ ]Â **List 5-7 Clear Actions**
  - **Action:**Â Create a bulleted list of simple, direct recommendations.
  - **Examples:**
    1. "Focus marketing spend on high-value corporate customers in the North."
    2. "Investigate supply chain delays in the South region immediately."
    3. "Increase discounts on 'Accessories' to clear stagnant inventory."
    4. "Implement a post-purchase email sequence to improve retention."
    5. "Discontinue low-margin products with high return rates."

### **5) Write an Executive Summary (Short Paragraph)(done)**

One paragraph telling the story.

### **âœ” 16.5 The Executive Summary (The Narrative)**

- [ ] [ ]Â **Write the Final Paragraph**
  - **Action:**Â Combine the biggest win and the biggest risk into a short story.
  - **Example:**
    > "Sales grew by 14% this year, driven largely by strong performance in Electronics and high retention among corporate clients. However, the South region is significantly underperforming due to delivery delays, causing an estimated â‚¹8â€“10 lakh loss per quarter. By improving delivery SLAs in the South and doubling down on high-margin Electronic inventory, the business could increase overall revenue by an additional 12â€“15% next year."

This is beginner-friendly but very professional.

### 6) Deliverables and Best Practices(done)

### ğŸŒŸ Phase 16 Best Practices

- **Simplicity Wins:**Â Use simple words. "We lost money" is better than "We experienced a negative fiscal trajectory."
- **Evidence is King:**Â Never make a claim without pointing to a specific chart or number (Evidence).
- **Focus on "Outcome," not "Output":**
  - *Output:*Â "I built a dashboard."
  - *Outcome:*Â "I saved the team 20 hours a week." ->Â **This gets you hired.**

### ğŸ“¦ Phase 16 Deliverables

1. **âœ…Â `REPORT.md`:**Â A polished markdown file in your GitHub repo containing all the above sections.
2. **âœ… Impact Table:**Â The "Before/After" comparison clearly visible in your README.
3. **âœ… Recommendations List:**Â 5-7 clear, actionable steps for the business.
4. **âœ… Executive Summary:**Â A strong opening paragraph that summarizes the entire project.

# **ğŸ”„Â Phase 7: Monitoring, Optimization & Governance**

> Goal:Â Quantify performance improvements, document the project's evolution (V1 vs V2), and package all assets into a "Hired-Ready" GitHub repository.

### **Performance Engineering Audit**

- **Goal:**Â Prove you understand cost and speed.
- [ ] [ ]Â **Snowflake Cost Optimization**
  - **Action:**Â ConfigureÂ `AUTO_SUSPEND = 60`Â (seconds) on all Warehouses.
  - **Action:**Â VerifyÂ `TRANSFORM_WH`Â is set toÂ `X-SMALL`.
  - **Action:**Â Run a query to calculate credits saved:Â `(Old Runtime - New Runtime) * Cost`.
- [ ] [ ]Â **dbt Model Optimization**
  - **Action:**Â VerifyÂ `incremental`Â materialization is active on Fact tables.
  - **Action:**Â Remove any unused "Intermediate" models that are materialized as Tables (switch to Ephemeral).
  - **Action:**Â Add Clustering Keys (Snowflake) on large tables if filtering by Date.
- [ ] [ ]Â **Power BI Performance Tuning**
  - **Action:**Â RunÂ **Performance Analyzer**Â in Desktop. Ensure no visual > 1000ms.
  - **Action:**Â RunÂ **DAX Studio (VertiPaq Analyzer)**. Identify and remove high-cardinality columns (e.g., GUIDs) not used in visuals to reduce file size.
- [ ] [ ]Â **CreateÂ `PERFORMANCE.md`**
  - **Action:**Â Create a Markdown file in your repo.
  - **Content:**Â Create the "Before vs After" table (Query Time, Credits, PBIX Size).

### **Pipeline Health & Automation**

- **Goal:**Â Show you built a robust system, not a fragile script.
- [ ] [ ]Â **Configure Alerts**
  - **Action:**Â In dbt Cloud (or GitHub Actions), configure Email/Slack notifications on "Run Failure."
- [ ] [ ]Â **Implement Freshness Checks**
  - **Action:**Â EnsureÂ `dbt source freshness`Â runsÂ *before*Â `dbt build`Â in your Production Job.
  - **Why:**Â If the raw data is stale, don't waste money running the pipeline.
- [ ] [ ]Â **Create the "Runbook"**
  - **Action:**Â CreateÂ `docs/runbook.md`.
  - **Content:**Â "If the pipeline fails at step X, check Y." (e.g., "If source freshness fails, check Azure Blob permissions").

### Incident Response

**âš ï¸ Operational Context:**Â While this portfolio project utilizes a static dataset (historical snapshot), I have designed the pipeline to handleÂ **Real-World Enterprise Constraints**.

I have manuallyÂ **simulated**Â the following failure scenarios to verify that myÂ `dbt tests`,Â `Schema Hardening`, andÂ `Freshness Checks`function correctly in a live production environment.

> Goal:Â This section defines the standard operating procedures (SOPs) for when the pipeline breaks.

### **Scenario C: Critical Data Loss (The "Fat Finger" Incident)**

- [ ] **Create SQL Artifact:**Â Save the script used for this drill inÂ `snowflake/04_maintenance/dr_time_travel_demo.sql`.
- [ ] ğŸ“¸ 4. The Visual Proof (For your ReadMe)

TakeÂ **one screenshot**Â of your Snowflake History showing theÂ `DROP`Â command followed by theÂ `UNDROP`Â command.

- **Caption:**Â *"Demonstrating Snowflake Time Travel to recover a dropped table in under 2 minutes."*

[ğŸ›¡ï¸ Disaster Recovery Drill: Table Restoration](https://www.notion.so/Disaster-Recovery-Drill-Table-Restoration-2cd1bb84f4a4814c908add48bb642716?pvs=21)

- **What it is:**Â A developer or admin accidentally executes a destructive command likeÂ `DROP TABLE`Â orÂ `DELETE FROM`on a production table (e.g.,Â `dim_customers`) during a hot-fix or maintenance window.
- **The Risk:**Â Immediate outage of all downstream dashboards. Users see "Visual has an error." If backups aren't immediate, this causes significant business downtime and potential permanent data loss.
- **Defense Strategy:**
  - **Snowflake Time Travel:**Â ConfigureÂ `DATA_RETENTION_TIME_IN_DAYS = 1`Â (or more) on all Production Databases. This creates a continuous rolling backup of the state of the data.
  - **Fail-Safe:**Â An additional 7-day backup layer provided by Snowflake (non-configurable) as a last resort.
- **Fix Protocol:**
  1. **Acknowledge the Outage:**Â Confirm the table is missing viaÂ `SHOW TABLES`.
  2. **Identify the Drop:**Â CheckÂ `QUERY_HISTORY`Â to find the timestamp or Query ID of theÂ `DROP`Â command.
  3. **Execute Recovery:**Â Run theÂ `UNDROP TABLE [table_name]`Â command immediately. This restores the table structure, data, and metadata to the exact state before the drop.
  4. **Validate:**Â Run aÂ `SELECT COUNT(*)`Â to confirm row counts match the pre-incident baseline.
     **ğŸ§ª How I Simulated This:**
  - *Action:*Â I explicitly ran aÂ `DROP TABLE`Â command on my populatedÂ `dim_customers`Â table in the Dev environment.
  - *Result:*Â I executedÂ `UNDROP TABLE dim_customers`Â and verified via a screenshot that the table reappeared instantly with zero data loss.
    If you want to look even more senior, show how you fix aÂ **Bad Update**Â (not just a Drop).
    [**Scenario E: Logical Corruption (The "Bad Update")**](https://www.notion.so/Scenario-E-Logical-Corruption-The-Bad-Update-2cd1bb84f4a481b69a84c1e44d94e92a?pvs=21)

### **Scenario A: Schema Drift (The "Silent Killer")**

- **What it is:**Â The source system (e.g., the app database or API) changes without warning. A column might be renamed (e.g.,Â `user_id`Â becomesÂ `cust_id`), a data type might change (String to Integer), or a column might be deleted entirely.
- **The Risk:**Â If not caught, this can break your SQL transformation logic or, worse, feed emptyÂ `NULL`Â values into the CEO's dashboard.
- **Defense Strategy:**
  - **Upstream (dbt):**Â UseÂ **Data Contracts**Â to strictly define what the dataÂ *must*Â look like. If it doesn't match, the pipeline stops immediately.
  - **Downstream (Power BI):**Â UseÂ `Table.SelectColumns`Â in Power Query to "harden" the schema. If a column is missing, the refresh fails loudly instead of showing a broken chart.
- **Fix Protocol:**
  1. **Identify the Change:**Â Look at the error log. (e.g.,Â *"Column 'user_id' not found"*).
  2. **Update Staging:**Â open yourÂ `stg_`Â model in dbt. Rename the new source column to your standard name (e.g.,Â `cust_id as user_id`).
  3. **Update Contract:**Â Edit yourÂ `schema.yml`Â to reflect the new reality if the data type changed.
  4. **Full Refresh:**Â RunÂ `dbt run --full-refresh`Â to rebuild the table history with the new structure.
     **ğŸ§ª How I Simulated This:**
  - *Action:*Â I intentionally renamed a column in my local CSV and ranÂ `dbt run`.
  - *Result:*Â Confirmed that dbt threw a specific compilation error, validating the guardrails.

### **Scenario B: Source Freshness Failure (The "Stale Data" Bug)**

- **What it is:**Â The data pipeline ran successfully, but the data inside it is old. For example, it's Tuesday, but the dashboard still shows Sunday's data.
- **The Risk:**Â Stakeholders make decisions based on outdated information.
- **The Trigger:**Â YourÂ `dbt source freshness`Â check fails because theÂ `_loaded_at`Â timestamp is older than your threshold (e.g., > 24 hours).
- **Fix Protocol:**
  1. **Check the Source:**Â Log into your Azure Blob Storage container. Look at the "Last Modified" date of the CSV files.
  2. **Diagnosis:**
     - *If Azure is empty/old:*Â The issue is with theÂ **Extraction Script**Â (Python) or the API source itself. You need to re-run the extraction job.
     - *If Azure has new data:*Â The issue is withÂ **Snowflake Ingestion**. Check if yourÂ `COPY INTO`Â command failed or if permissions (SAS Token) expired.
  3. **Resolution:**Â Manually trigger the extraction script or refresh the Snowflake pipe, then re-run the dbt pipeline.

### **Scenario C: Data Quality Breach (The "Duplicate" Bug)**

- **What it is:**Â A business rule was violated. The most common one isÂ **Duplication**â€”a Primary Key that should be unique (likeÂ `order_id`) appears twice.
- **The Risk:**Â Revenue numbers are double-counted, leading to inflated reporting.
- **The Trigger:**Â AÂ `dbt test`Â fails (specifically theÂ `unique`Â orÂ `not_null`Â test).
- **Fix Protocol:**

  1.  **Audit the Data:**Â Run a SQL query to see the bad rows:SQL

           `SELECT order_id, COUNT(*)

      FROM fct_orders
      GROUP BY 1
      HAVING COUNT(\*) > 1;`

  2.  **Determine Cause:**

      - *Is it a Bug?*Â Did the ingestion job run twice by accident? ->Â **Fix:**Â Delete the bad rows in the Raw layer and re-load.
      - *Is it Real Life?*Â Did the order actually have two status updates? ->Â **Fix:**Â Update yourÂ **Intermediate**Â logic to handle this. Use a window function (`QUALIFY ROW_NUMBER()`) to select only the latest version of that order.

      **ğŸ§ª How I Simulated This:**

      - *Action:*Â I manually duplicated 5 rows in the source fileÂ `orders.csv`.
      - *Result:*Â `dbt test`Â failed as expected. I then implementedÂ `QUALIFY ROW_NUMBER()`Â in my model to automatically handle future duplicates.

### **Scenario D: Cost or Volume Spike (The "Wallet" Risk)**

- **What it is:**Â A query runs for way too long, or you accidentally process 100x more data than usual (e.g., re-processing the last 5 years instead of just today).
- **The Risk:**Â You burn through your Snowflake free credits or get a massive cloud bill.
- **The Trigger:**Â A SnowflakeÂ **Resource Monitor**Â sends an alert (e.g., "90% of monthly budget used").
- **Fix Protocol:**
  1. **Stop the Bleeding:**Â Immediately go to Snowflake "Query History" and kill any queries running longer than 10-15 minutes.
  2. **Investigate:**Â Look forÂ **Cartesian Joins**Â (a join without a proper key) in recent code changes. This causes row counts to explode (e.g., 1,000 rows x 1,000 rows = 1 million rows).
  3. **Remediate:**Â Revert the bad code change. If the table is simply too big, ensure yourÂ **Incremental Logic**Â is working correctly so you are only processingÂ *new*Â rows.
  ### **Scenario E: The "Access Denied" (Credential Expiry)**
  - **What it is:**Â The pipeline suddenly fails withÂ `403 Forbidden`Â orÂ `Authentication Error`.
  - **The Root Cause:**Â Secrets don't last forever. YourÂ **Azure SAS Token**Â expired (usually set to 6-12 months) or yourÂ **Snowflake Service User**Â password rotated.
  - **The Trigger:**Â Ingestion step fails immediately.
  - **Fix Protocol:**
    1. **Identify the Key:**Â Check the error log. If it saysÂ `Azure`, it's the SAS Token. IfÂ `Snowflake`, it's the user password.
    2. **Rotate Secret:**Â Generate a new SAS Token in Azure Portal or reset the Snowflake password.
    3. **Update Repo:**Â Update theÂ `GitHub Secrets`Â (e.g.,Â `AZURE_SAS_TOKEN`) or yourÂ `.env`Â file.
    4. **Re-Run:**Â Restart the pipeline.
  ### **Scenario F: The "Infinite Loop" (Pipeline Timeout)**
  - **What it is:**Â The job doesn't fail, but it hangs forever (or hits the 2-hour timeout limit).
  - **The Root Cause:**Â A "Table Lock" in Snowflake (another process is updating the table) or a bad join creating billions of rows (Cartesian product) slowly.
  - **The Trigger:**Â GitHub Actions / dbt Cloud reportsÂ `Timeout Exceeded`.
  - **Fix Protocol:**
    1. **Kill the Lock:**Â RunÂ `SHOW TRANSACTIONS`Â in Snowflake and abort any hanging transactions preventing the update.
    2. **Scale Up:**Â If it's just a heavy data day (End of Month), temporarily increase the Warehouse size (`ALTER WAREHOUSE ... SET WAREHOUSE_SIZE = 'SMALL'`).
    3. **Investigate:**Â Check Query Profile for "Spilling to Disk" (means not enough RAM).
  ### **Scenario G: The "Last Mile" Break (Power BI Refresh Failure)**
  - **What it is:**Â The data in Snowflake is perfect, but the Dashboard shows an error triangle.
  - **The Root Cause:**
    - **Memory Limit:**Â You exceeded the 1GB limit (Pro License).
    - **Data Type Mismatch:**Â Snowflake sent a "Text" column that Power BI expected as "Date".
  - **The Trigger:**Â You get a "Refresh Failed" email from Power BI Service.
  - **Fix Protocol:**
    1. **Check Service Logs:**Â Click the warning icon in the Workspace history.
    2. **If Memory Limit:**Â RunÂ **VertiPaq Analyzer**Â (DAX Studio) and drop unused columns.
    3. **If Type Mismatch:**Â Update the Power Query "Schema Hardening" step to explicitly cast the column types again.
  ### **Scenario H: The "Bad Merge" (Logic Regression)**
  - **What it is:**Â The code runs fine, tests pass, but the numbers areÂ *wrong*Â (e.g., Profit is negative).
  - **The Root Cause:**Â A developer changed aÂ `LEFT JOIN`Â to anÂ `INNER JOIN`Â and accidentally dropped 10% of records.
  - **The Trigger:**Â A stakeholder emails you: "Why are sales down 10%?"
  - **Fix Protocol (The Rollback):**
    1. **Verify:**Â Run a quick SQL check comparingÂ `Row Count Yesterday`Â vsÂ `Row Count Today`.
    2. **Revert:**Â Immediately revert the last Git Pull Request (`git revert HEAD`).
    3. **Deploy:**Â Push the revert to Production to restore the dashboard logic.
    4. **Post-Mortem:**Â Add a newÂ `dbt test`Â (e.g.,Â `dbt_expectations.expect_table_row_count_to_be_between`) to catch this drop next time.
  ***
  **âœ” Create the Artifact**
  - [ ] [ ]Â **WriteÂ `docs/RUNBOOK.md`**
    - *Action:*Â Create a markdown file in your repo.
    - *Content:*Â Copy the scenarios above into tables. Add a "Contact Info" section (your email) acting as the "On-Call Engineer."
      **âœ” The "Post-Mortem" Template**
  - [ ] [ ]Â **Create an Incident Log**
    - *Action:*Â Add a section in your Notion or Repo to logÂ *actual*Â bugs you faced while building.
    - *Format:*Â `[Date] | [Error] | [Root Cause] | [Fix]`.
    - *Why:*Â This is your "Cheat Sheet" for interview questions about challenges you faced.
  ### ğŸ“ Summary Checklist for Your Notion

| **Scenario**       | **Layer**      | **Primary Fix**                  |
| ------------------ | -------------- | -------------------------------- |
| **Schema Drift**   | Ingestion      | Update Staging Model + Contracts |
| **Stale Data**     | Source         | Check Azure File Dates           |
| **Quality Breach** | Transformation | Fix Logic or Delete Bad Rows     |
| **Cost Spike**     | Warehouse      | Kill Query + Fix Join            |
| **Auth Failure**   | Security       | Rotate SAS Token/Password        |
| **Timeout**        | Compute        | Kill Locks or Scale Warehouse    |
| **PBI Refresh**    | Visualization  | Optimize Model Size (VertiPaq)   |
| **Bad Logic**      | Code           | Git Revert + Add New Test        |

### The Product Evolution (Showing Iteration)

- **Goal:**Â Show you listen to users and iterate.
- [ ] [ ]Â **Document the Roadmap**
  - **Action:**Â InÂ `README.md`, add a section "Project Evolution."
  - **Log:**
    - *v1.0:*Â MVP Data Pipeline & Basic Sales Report.
    - *v1.1:*Â Added "Drill-through" features based on stakeholder feedback.
    - *v2.0:*Â Implemented Incremental Refresh & RLS for security.
- [ ] [ ]Â **List "Future Improvements"**
  - **Action:**Â Add a "Next Steps" section.
  - **Ideas:**Â "Implement Snowflake Streams for real-time data," "Add Python/Snowpark for Churn Prediction."

### **Final Repository Structure & Documentation**

- **Goal:**Â Make the repo clean, scannable, and professional.
- [ ] [ ]Â **Finalize Folder Structure**
  - Ensure root is clean. Move helper files toÂ `/docs`Â orÂ `/scripts`.
  - Structure:Plaintext
    ```jsx
    â”œâ”€â”€ dbt_project/          # The Transformation Logic
    â”œâ”€â”€ power_bi/             # The .pbip Project
    â”œâ”€â”€ docs/                 # Requirements, Runbook, Diagrams
    â”œâ”€â”€ scripts/              # Python seed generators
    â”œâ”€â”€ .github/              # CI/CD Workflows
    â”œâ”€â”€ README.md             # The Landing Page
    â”œâ”€â”€ REPORT.md             # The Business Analysis
    â””â”€â”€ PERFORMANCE.md        # The Optimization Logs
    ```
- [ ] [ ]Â **FinalizeÂ `README.md`Â (The Most Important File)**
  - **Header:**Â Project Title, One-line pitch, Tech Stack Badges.
  - **Visual:**Â Embed theÂ **Architecture Diagram**.
  - **Access:**Â Link to the hosted dbt Docs site.
  - **Impact:**Â Include the "Business Value" table (Time Saved, Revenue Opportunity).
- [ ] [ ]Â **FinalizeÂ `REPORT.md`**
  - **Content:**Â Executive Summary, 3 Key Insights (with screenshots), 3 Recommendations.

### **Portfolio Asset Creation**

- **Goal:**Â Create "Shareable" content for LinkedIn and Applications.
- [ ] [ ]Â **Record Demo Video (Loom)**
  - **Length:**Â 2-3 minutes max.
  - **Script:**Â Problem -> Architecture -> Demo of Dashboard -> One cool technical challenge you solved (e.g., Incremental logic).
  - **Action:**Â Add link to the top of README.
- [ ] [ ]Â **Create "One-Pager" PDF**
  - **Action:**Â Design a single page summary: "Retail Analytics Pipeline."
  - **Include:**Â Architecture Image, Dashboard Screenshot, "Results" bullets.
  - **Use:**Â Attach to job applications.
- [ ] [ ]Â **GitHub "Release"**
  - **Action:**Â On GitHub, click "Create a new release". Tag itÂ `v1.0-production`.
  - **Why:**Â Looks incredibly professional.

### Best Practices and Deliverables

### ğŸŒŸ Phase 17 Best Practices

- **Evidence over Claims:**Â Don't just say "I optimized the query." Show the screenshot of the Query Profile showing the reduction in "Bytes Scanned."
- **The "User" is King:**Â When describing iterations, always frame it as "User Request" or "Stakeholder Feedback."
  - *Example:*Â "Users reported the dashboard was slow, so I implemented Aggregation Tables."
- **Clean up your Git History:**Â If you have 50 commits named "fix", consider doing a "Squash and Merge" when merging your final feature branches to Main to keep the history clean.

### ğŸ“¦ Phase 17 Deliverables

1. **âœ…Â `PERFORMANCE.md`:**Â A log of technical optimizations and cost savings.
2. **âœ… Clean Repo:**Â Organized folders, no loose files.
3. **âœ…Â `README.md`:**Â The perfect landing page with Architecture Diagram embedded.
4. **âœ… Demo Video:**Â A visual walkthrough linked in the README.
5. **âœ…Â `v1.0`Â Release:**Â A formal tagged release on GitHub.
6. **Pin the Repo:**Â Go to your GitHub profile -> "Customize your pins" -> Select this project. It must be #1.
7. **Don't hide the "Ugly":**Â If you ran into a bug (e.g., "My incremental logic failed on day 3 because of a schema change"), document it in a "Challenges & Lessons Learned" section. This shows resilience.
8. **Visuals First:**Â In your README, never write a wall of text. Always alternate: Text -> Screenshot -> Text -> Code Snippet.

[Phase 8 Final Summary ](https://www.notion.so/Phase-8-Final-Summary-2cd1bb84f4a48103b35fce51a03eb090?pvs=21)

### ğŸ“‹ Project Summary and Deliverables

This project delivers a complete, end-to-end modern data analytics solution usingÂ **Azure Blob â†’ Snowflake â†’ dbt â†’ Power BI â†’ GitHub**.

**Key Things Produced**

- **Clean, reliable dataset**Â loaded from Azure Blob into Snowflake
- **Well-structured dbt project**Â with:
  - Reusable staging, intermediate, and mart models
  - Automated data cleaning
  - Data quality tests (unique, not null, accepted values)
  - Clear lineage showing how raw â†’ final analytics tables are built
- **Reusable data model**Â (star schema) ready for any analysis
- **Power BI semantic model**Â with:
  - Standardized KPIs
  - Reusable measures
  - Clean relationships + hierarchies
  - Optimized, easy-to-use model
- **Interactive Power BI Dashboard**Â including:
  - KPIs
  - Trends
  - Top/bottom analysis
  - Drill-through pages
  - Insights page & recommendations
- **Insights & Business Answers**Â to real business questions
- **Documented business impact**, such as:
  - Time saved by automating cleaning with dbt
  - Improved data quality
  - Faster reporting with semantic model
- **Fully documented project**Â in Notion + GitHub
  - Architecture diagram
  - Data model diagram
  - dbt lineage
  - Insights summary
  - README with explanation of each phase
- **Version-controlled code**Â using Git & GitHub
- **Final monitoring & stability checks**Â on Snowflake, dbt, Power BI

---

# ğŸ§©Â **One-Sentence Portfolio Summary**

> Built a complete analytics pipeline raw files to a production-ready dashboard, including automated dbt transformations, a reusable star schema, and an insight-driven Power BI report.

### Reuseable assets

**1.Â  Raw data in Azure Blob / Snowflake (ingestion pipelines)**

**2.Â  Staging models (stg\_\*) in dbt**

**3.Â  Dimension models (dim_customer, dim_date, dim_product, etc.)**

**4.Â  Common marts/intermediate models (int_orders, int_sessions, etc.)**

**5.Â  dbt tests, docs, exposures, sources, macros**

**6.Â  Semantic layer (dbt metrics or Power BI dataset relationships)**

**7.Â  Power BI dataset (shared dataset mode)**

**8.Â  Git repo + branch strategy**

**9.Â  CI/CD pipelines (dbt Cloud or GitHub Actions)**

**10.Â  Data contracts & catalog (if using Snowflake tags or Catalog)**
